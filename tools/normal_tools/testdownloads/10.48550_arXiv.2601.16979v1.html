<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs</title>
<!--Generated on Fri Jan 23 18:49:31 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on January 23, 2026.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2601.16979v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S1" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S1.SS1" title="In 1 Introduction â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Characterizing Critical Sharpness: Theory and Empirics</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.SS1" title="In 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.SS2" title="In 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Critical Sharpness: A Scalable Measure of Curvature</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.SS2.SSS0.Px1" title="In 2.2 Critical Sharpness: A Scalable Measure of Curvature â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Geometric Interpretation</span>:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.SS2.SSS0.Px2" title="In 2.2 Critical Sharpness: A Scalable Measure of Curvature â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title">Efficient estimation of critical learning rate:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.SS3" title="In 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>The Relationship between Critical Sharpness and Hessian Sharpness</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S3" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Critical Sharpness Dynamics at Scale</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S4" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>How much Pre-training data is needed to avoid Catastrophic forgetting?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S5" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S6" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion and Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S7" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Estimating Critical Learning Rate Using Forward Passes</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S7.SS0.SSS0.Px1" title="In 7 Estimating Critical Learning Rate Using Forward Passes â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title">Exponential Search:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S8" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Experimental Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Additional Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.SS1" title="In 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span>GPT Pre-training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.SS2" title="In 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span>OLMo checkpoint analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.SS3" title="In 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.3 </span>OLMo mid-training on a mixture consisting of DCLM and Math</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.SS4" title="In 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.4 </span>OLMo mid-training</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S10" title="In A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Theoretical Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S10.SS1" title="In 10 Theoretical Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1 </span>The Relationship between Directional and Hessian sharpness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S10.SS2" title="In 10 Theoretical Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2 </span>Stability Threshold for Optimizers with Weight Decay</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">1]Meta Superintelligence Labs
2]University of Maryland, College Park
<span class="ltx_ERROR undefined" id="p1.1.1">\contribution</span>[*]work done during an internship at Meta</p>
</div>
<h1 class="ltx_title ltx_title_document" lang="en">A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">Dayal Singh Kalra
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">Jean-Christophe Gagnon-Audet
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">Andrey Gromov
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">Ishita Mediratta
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">Kelvin Niu
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">Alexander H Miller
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname" lang="en">Michael Shvartsman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation" lang="en">[
</span>
<span class="ltx_contact ltx_role_affiliation" lang="en">[
</span>
<span class="ltx_contact ltx_role_email" lang="en"><a href="mailto:dayal@umd.edu">dayal@umd.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(<span class="ltx_text" id="id7.id1" lang="en">January 23, 2026</span>)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.6">Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks.
The most commonly studied measure, Hessian sharpness (<math alttext="\lambda_{\max}^{H}" class="ltx_Math" display="inline" id="id1.1.m1" intent=":literal"><semantics><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup><annotation encoding="application/x-tex">\lambda_{\max}^{H}</annotation></semantics></math>) â€”the largest eigenvalue of the loss Hessian â€”determines local training stability and interacts with the learning rate throughout training.
Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost.
We analyze <em class="ltx_emph ltx_font_italic" id="id6.6.1">critical sharpness</em> (<math alttext="\lambda_{c}" class="ltx_Math" display="inline" id="id2.2.m2" intent=":literal"><semantics><msub><mi>Î»</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\lambda_{c}</annotation></semantics></math>), a computationally efficient measure requiring fewer than <math alttext="10" class="ltx_Math" display="inline" id="id3.3.m3" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math> forward passes given the update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="id4.4.m4" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math>.
Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability.
Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to <math alttext="7" class="ltx_Math" display="inline" id="id5.5.m5" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math>B parameters, spanning both pre-training and mid-training of OLMo-2 models.
We further introduce <em class="ltx_emph ltx_font_italic" id="id6.6.2">relative critical sharpness</em> (<math alttext="\lambda_{c}^{1\to 2}" class="ltx_Math" display="inline" id="id6.6.m6" intent=":literal"><semantics><msubsup><mi>Î»</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup><annotation encoding="application/x-tex">\lambda_{c}^{1\to 2}</annotation></semantics></math>), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies.
Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\correspondence</span>
</div>
<section class="ltx_section" id="S1" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.2">Understanding the evolution of the loss landscape <math alttext="L(\bm{\theta})" class="ltx_Math" display="inline" id="S1.p1.1.m1" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta})</annotation></semantics></math> over the high-dimensional parameter space <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S1.p1.2.m2" intent=":literal"><semantics><mi>ğœ½</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation></semantics></math> is fundamental to analyzing the training dynamics of neural networks. The loss landscape describes how the objective function changes with model parameters, and its geometry directly influences optimization, generalization and stability throughout training <cite class="ltx_cite ltx_citemacro_citep">(Gilmer et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib15" title="">2022</a>; Kalra and Barkeshli, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib22" title="">2024</a>)</cite>. Intuitively, gradient-based optimization methods can swiftly navigate to the minima in smooth landscapes, while for rough landscapes, such methods can get trapped, hindering convergence or potentially converging to suboptimal solutions <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib27" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.4">The local geometry of the loss landscape is commonly examined through the eigenvalues and eigenvectors of its Hessian matrix <math alttext="H(\bm{\theta}):=\nabla_{\theta}^{2}L(\bm{\theta})" class="ltx_Math" display="inline" id="S1.p2.1.m1" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><msubsup><mo rspace="0.167em">âˆ‡</mo><mi>Î¸</mi><mn>2</mn></msubsup><mi>L</mi></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">H(\bm{\theta}):=\nabla_{\theta}^{2}L(\bm{\theta})</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib43" title="">2018</a>; Lewkowycz et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib26" title="">2020</a>; Cohen et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib6" title="">2021</a>)</cite>. These eigenvalues provide insights into the local curvature of the loss landscape â€”large eigenvalues correspond to sharp, steep directions, while small eigenvalues indicate flat, smooth regions.
The largest eigenvalue of the Hessian, which is often referred to as <em class="ltx_emph ltx_font_italic" id="S1.p2.4.1">Hessian sharpness</em> <math alttext="\lambda^{H}_{\max}" class="ltx_Math" display="inline" id="S1.p2.2.m2" intent=":literal"><semantics><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup><annotation encoding="application/x-tex">\lambda^{H}_{\max}</annotation></semantics></math>, quantifies the worst-case curvature of the landscape and is a fundamental metric in optimization. Its reciprocal, <em class="ltx_emph ltx_font_italic" id="S1.p2.4.2">flatness</em> <math alttext="1/\lambda_{\max}^{H}" class="ltx_Math" display="inline" id="S1.p2.3.m3" intent=":literal"><semantics><mrow><mn>1</mn><mo>/</mo><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup></mrow><annotation encoding="application/x-tex">1/\lambda_{\max}^{H}</annotation></semantics></math>, is a complementary measure used to describe the curvature.
In neural network optimization, Hessian sharpness relates to the local training stability <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib43" title="">2018</a>; Lewkowycz et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib26" title="">2020</a>)</cite>. For vanilla Gradient Descent (GD), if the learning rate exceeds the threshold <math alttext="\sim 2/{\lambda_{\max}^{H}}" class="ltx_Math" display="inline" id="S1.p2.4.m4" intent=":literal"><semantics><mrow><mi></mi><mo>âˆ¼</mo><mrow><mn>2</mn><mo>/</mo><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\sim 2/{\lambda_{\max}^{H}}</annotation></semantics></math>, loss increases as training â€˜catapultsâ€™ out of the local basin and eventually converges to a flatter region where the stability conditions are satisfied <cite class="ltx_cite ltx_citemacro_citep">(Lewkowycz et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib26" title="">2020</a>)</cite>.
This observation generalizes to more complex optimization problems, including mini-batch settings and adaptive optimizers, albeit governed by different notions of sharpness <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib43" title="">2018</a>; Agarwala and
Pennington, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib1" title="">2025</a>; Cohen et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib8" title="">2024</a>; Kalra and Barkeshli, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib22" title="">2024</a>)</cite>.</p>
</div>
<figure class="ltx_figure ltx_align_floatright" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Hessian sharpness exhibits progressive sharpening and Edge of Stability (EoS) under constant learning rate. The dashed lines corresponding to the learning rate mark the EoS threshold.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">As illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S1.F1" title="In 1 Introduction â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">1</span></a>, Hessian sharpness exhibits several robust trends throughout neural network training, particularly with constant learning rates.
Training typically begins with an early reduction in Hessian sharpness <cite class="ltx_cite ltx_citemacro_citep">(Kalra and Barkeshli, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib21" title="">2023</a>; Kalra et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib23" title="">2025</a>)</cite> followed by a continuous increase until it reaches the stability threshold <cite class="ltx_cite ltx_citemacro_citep">(Jastrzebski et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib20" title="">2020</a>; Cohen et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib6" title="">2021</a>)</cite>.
Once this threshold is reached, the training stabilizes through a self-stabilization mechanism <cite class="ltx_cite ltx_citemacro_citep">(Damian et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib9" title="">2023</a>; Cohen et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib7" title="">2025</a>)</cite> â€”instead of diverging as classical optimization would predict, Hessian sharpness begins to oscillate around this critical value. The continual increase in Hessian sharpness is referred to as <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">progressive sharpening</em> while the subsequent oscillations around the critical threshold is termed the <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">Edge of Stability</em> (EoS) <cite class="ltx_cite ltx_citemacro_citep">(Cohen et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib6" title="">2021</a>)</cite>.
For the more realistic setting of learning rate schedules involving warmup and decay, sharpness closely follows the learning rate schedule <cite class="ltx_cite ltx_citemacro_citep">(Gilmer et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib15" title="">2022</a>; Kalra and Barkeshli, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib21" title="">2023</a>; Cohen et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib6" title="">2021</a>)</cite>. Due to its particularly close relationship with learning rate, sharpness can be used as a diagnostic tool for identifying training instabilities.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Beyond training dynamics, sharpness of the final solutions is linked to generalization properties, motivated by the intuition that flatter minima generalize better <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib17" title="">1997</a>)</cite>. However, this relationship has been called into question <cite class="ltx_cite ltx_citemacro_citep">(Kaur et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib24" title="">2023</a>)</cite>, as empirical analyses show mixed results â€”while flatter solutions generalize better in some settings, they can hurt performance in others.
Sharpness also influences the early phase of training, with flatter initializations typically achieving better performance <cite class="ltx_cite ltx_citemacro_citep">(Dauphin and Schoenholz, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib12" title="">2019</a>; Kalra and Barkeshli, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib22" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Despite its value for examining model initialization, training dynamics, and generalization properties, analyzing sharpness at scale is challenging.
Computing Hessian sharpness relies on iterative eigenvalue solvers (e.g., Power iteration, Lanczos), which require repeated Hessian-vector products (HVPs). However, HVP computation via automatic differentiation is often incompatible with modern training efficiency tools; kernels like Flash Attention <cite class="ltx_cite ltx_citemacro_citep">(Dao et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib11" title="">2022</a>)</cite> typically lack second-derivative implementations required for double backpropagation. Furthermore, iterative solvers can require hundreds of iterations with potential convergence failures.
As a result, most existing studies are restricted to small-scale experiments (typically <math alttext="\sim 10" class="ltx_Math" display="inline" id="S1.p5.1.m1" intent=":literal"><semantics><mrow><mi></mi><mo>âˆ¼</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">\sim 10</annotation></semantics></math>M parameters), leaving open questions about how sharpness evolves in Large Language Models (LLMs) at scale, and how it relates to optimization and downstream performance. In this work, we address this challenge by analyzing <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">critical sharpness</em>, which leverages the relationship between curvature and training instability to serve as a computationally efficient proxy for the loss landscape curvature of LLMs.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contributions</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Our contributions are as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.6">We analyze critical sharpness, defined as <math alttext="\lambda_{c}=2/\eta_{c}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.1.m1" intent=":literal"><semantics><mrow><msub><mi>Î»</mi><mi>c</mi></msub><mo>=</mo><mrow><mn>2</mn><mo>/</mo><msub><mi>Î·</mi><mi>c</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\lambda_{c}=2/\eta_{c}</annotation></semantics></math>, where <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.2.m2" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math> is the <em class="ltx_emph ltx_font_italic" id="S1.I1.i1.p1.6.1">critical learning rate</em>â€”the smallest learning rate that causes the training loss to increase in the next training step. To estimate the critical learning rate <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.3.m3" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math>, we perform an efficient line search along the update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S1.I1.i1.p1.4.m4" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math> from training. This procedure only requires forward passes, making it fully compatible with modern large-scale distributed training infrastructure, while avoiding the convergence issues of iterative eigenvalue solvers.
In practice, we find that this procedure reliably estimates critical sharpness in only <math alttext="5" class="ltx_Math" display="inline" id="S1.I1.i1.p1.5.m5" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math>-<math alttext="6" class="ltx_Math" display="inline" id="S1.I1.i1.p1.6.m6" intent=":literal"><semantics><mn>6</mn><annotation encoding="application/x-tex">6</annotation></semantics></math> forward passes<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>except for the first iteration, which depends on the initial guess <math alttext="\eta_{0}" class="ltx_Math" display="inline" id="footnote1.m1" intent=":literal"><semantics><msub><mi>Î·</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\eta_{0}</annotation></semantics></math> for line search.</span></span></span>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We then examine the relationship between critical sharpness and Hessian sharpness. Under the quadratic loss approximation, we show that critical sharpness can be written as a weighted sum of Hessian eigenvalues and the two measures coincide when the gradient aligns with the largest eigenvector of the Hessian. We also generalize this result to adaptive optimizers.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We demonstrate that critical sharpness reliably captures well-documented Hessian sharpness phenomena, such as <em class="ltx_emph ltx_font_italic" id="S1.I1.i3.p1.1.1">progressive sharpening</em> and the <em class="ltx_emph ltx_font_italic" id="S1.I1.i3.p1.1.2">Edge of Stability</em> <cite class="ltx_cite ltx_citemacro_citep">(Cohen et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib6" title="">2021</a>)</cite>. This makes critical sharpness a practical and computationally efficient proxy for sharpness. Leveraging OLMo-2 checkpoints <cite class="ltx_cite ltx_citemacro_citep">(Walsh et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib41" title="">2025</a>)</cite> across pre-training and mid-training, we demonstrate progressive sharpening persists at realistic scales throughout training, including models with up to 7 billion parameters.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.3">We introduce a relative measure of critical sharpness <math alttext="\lambda_{c}^{1\to 2}" class="ltx_Math" display="inline" id="S1.I1.i4.p1.1.m1" intent=":literal"><semantics><msubsup><mi>Î»</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup><annotation encoding="application/x-tex">\lambda_{c}^{1\to 2}</annotation></semantics></math> to quantify the curvature of one loss relative to another, with application to analyzing the pre-training loss landscape during mid-training. By varying the mix ratio of pre-training and fine-tuning (e.g. math), we show that increasing the proportion of pre-training data lowers relative critical sharpness, helping the model stay near the â€œpre-trained basinâ€ during finetuning. We further demonstrate that downstream performance depends on basin retention: GSM8K <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib5" title="">2021</a>)</cite> (math) benefits from leaving the pre-trained basin (<math alttext="\eta&gt;2/\lambda_{c}^{1\to 2}" class="ltx_Math" display="inline" id="S1.I1.i4.p1.2.m2" intent=":literal"><semantics><mrow><mi>Î·</mi><mo>&gt;</mo><mrow><mn>2</mn><mo>/</mo><msubsup><mi>Î»</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">\eta&gt;2/\lambda_{c}^{1\to 2}</annotation></semantics></math>), while MMLU <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib16" title="">2021</a>)</cite> (generic reasoning) performs better when the basin is retained (<math alttext="\eta&lt;2/\lambda_{c}^{1\to 2}" class="ltx_Math" display="inline" id="S1.I1.i4.p1.3.m3" intent=":literal"><semantics><mrow><mi>Î·</mi><mo>&lt;</mo><mrow><mn>2</mn><mo>/</mo><msubsup><mi>Î»</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">\eta&lt;2/\lambda_{c}^{1\to 2}</annotation></semantics></math>). This enables us to prescribe data mixing strategies to balance performance on math and generic reasoning tasks, depending on the desired outcome.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_section" id="S2" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Characterizing Critical Sharpness: Theory and Empirics</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we characterize the dynamics of critical sharpness both theoretically and empirically. We first establish the theoretical relationship between critical sharpness and Hessian sharpness under a quadratic loss approximation, then validate these insights empirically on FCNs trained on CIFAR-10 using SGD across different batch sizes (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.F3" title="In 2.3 The Relationship between Critical Sharpness and Hessian Sharpness â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Setup</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.12">Consider a model with trainable parameters <math alttext="\bm{\theta}\in\mathbb{R}^{n}" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1" intent=":literal"><semantics><mrow><mi>ğœ½</mi><mo>âˆˆ</mo><msup><mi>â„</mi><mi>n</mi></msup></mrow><annotation encoding="application/x-tex">\bm{\theta}\in\mathbb{R}^{n}</annotation></semantics></math>, and let <math alttext="L(\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta})</annotation></semantics></math> denote the loss function with gradient <math alttext="g(\bm{\theta}):=\nabla_{\theta}L(\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3" intent=":literal"><semantics><mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><msub><mo rspace="0.167em">âˆ‡</mo><mi>Î¸</mi></msub><mi>L</mi></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">g(\bm{\theta}):=\nabla_{\theta}L(\bm{\theta})</annotation></semantics></math> and Hessian <math alttext="H(\bm{\theta}):=\nabla_{\theta}^{2}L(\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo>:=</mo><mrow><mrow><msubsup><mo rspace="0.167em">âˆ‡</mo><mi>Î¸</mi><mn>2</mn></msubsup><mi>L</mi></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">H(\bm{\theta}):=\nabla_{\theta}^{2}L(\bm{\theta})</annotation></semantics></math>. Let <math alttext="\{\lambda_{i}^{H}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>Î»</mi><mi>i</mi><mi>H</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{\lambda_{i}^{H}\}_{i=1}^{n}</annotation></semantics></math> and <math alttext="\{{u}_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{{u}_{i}\}_{i=1}^{n}</annotation></semantics></math> denote the eigenvalues and eigenvectors of the Hessian, respectively. The largest eigenvalue <math alttext="\lambda_{\max}^{H}=\max_{i}\lambda_{i}^{H}" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m7" intent=":literal"><semantics><mrow><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup><mo>=</mo><mrow><msub><mi>max</mi><mi>i</mi></msub><mo lspace="0.167em">â¡</mo><msubsup><mi>Î»</mi><mi>i</mi><mi>H</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\lambda_{\max}^{H}=\max_{i}\lambda_{i}^{H}</annotation></semantics></math>, termed <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.12.1">Hessian sharpness</em>, quantifies the worst-case local curvature. For adaptive optimizers with pre-conditioner <math alttext="P(\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m8" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\bm{\theta})</annotation></semantics></math> (e.g., Adam), the training dynamics is governed by the pre-conditioned Hessian <math alttext="P(\bm{\theta})^{-1/2}H(\bm{\theta})P(\bm{\theta})^{-1/2}" class="ltx_Math" display="inline" id="S2.SS1.p1.9.m9" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">P(\bm{\theta})^{-1/2}H(\bm{\theta})P(\bm{\theta})^{-1/2}</annotation></semantics></math>, with eigenvalues <math alttext="\{\lambda^{PH}_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S2.SS1.p1.10.m10" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>Î»</mi><mi>i</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{\lambda^{PH}_{i}\}_{i=1}^{n}</annotation></semantics></math> and eigenvectors <math alttext="\{{v}_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S2.SS1.p1.11.m11" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>v</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{{v}_{i}\}_{i=1}^{n}</annotation></semantics></math>.
We omit explicit <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S2.SS1.p1.12.m12" intent=":literal"><semantics><mi>ğœ½</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation></semantics></math> dependence when clear from context.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="180" id="S2.F2.g1" src="figures/landscape-edited-final.png" width="180"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.7.3.1" style="font-size:90%;">Figure 2</span>: </span><em class="ltx_emph ltx_font_italic" id="S2.F2.8.4" style="font-size:90%;">Comparison of different sharpness measures on an illustrative landscape featuring a sharp valley direction and a flat river direction</em><span class="ltx_text" id="S2.F2.4.2" style="font-size:90%;"> <cite class="ltx_cite ltx_citemacro_citep">(Wen et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib42" title="">2025</a>)</cite>. Hessian sharpness quantifies the curvature along the sharpest direction of the landscape (the valley). Directional sharpness measures the quadratic curvature along <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S2.F2.3.1.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math>, while critical sharpness, its empirical counterpart, quantifies how far one can step along <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S2.F2.4.2.m2" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math> before the loss increases.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Critical Sharpness: A Scalable Measure of Curvature</h3>
<div class="ltx_theorem ltx_theorem_definition" id="S2.Thmdefinition1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S2.Thmdefinition1.3.1.1">Definition 2.1</span></span><span class="ltx_text ltx_font_bold" id="S2.Thmdefinition1.4.2"> </span>(Critical learning rate <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S2.Thmdefinition1.1.m1" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math> and Sharpness <math alttext="\lambda_{c}" class="ltx_Math" display="inline" id="S2.Thmdefinition1.2.m2" intent=":literal"><semantics><msub><mi>Î»</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\lambda_{c}</annotation></semantics></math>)<span class="ltx_text ltx_font_bold" id="S2.Thmdefinition1.5.3">.</span>
</h6>
<div class="ltx_para" id="S2.Thmdefinition1.p1">
<p class="ltx_p" id="S2.Thmdefinition1.p1.1">Given an update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S2.Thmdefinition1.p1.1.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math>, we define <em class="ltx_emph ltx_font_italic" id="S2.Thmdefinition1.p1.1.1">critical learning rate</em> as the smallest learning rate that causes the loss to increase in the next training step:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx1">
<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\eta_{c}=\min_{\eta&gt;0}\{\eta\mid L(\bm{\theta}-\eta\Delta\bm{\theta})&gt;L(\bm{\theta})\}." class="ltx_Math" display="inline" id="S2.E1.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Î·</mi><mi>c</mi></msub><mo>=</mo><mrow><munder><mi>min</mi><mrow><mi>Î·</mi><mo>&gt;</mo><mn>0</mn></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">{</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo fence="true" rspace="0em">âˆ£</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo fence="true" lspace="0em">&gt;</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\eta_{c}=\min_{\eta&gt;0}\{\eta\mid L(\bm{\theta}-\eta\Delta\bm{\theta})&gt;L(\bm{\theta})\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.Thmdefinition1.p1.2">Correspondingly, we define <em class="ltx_emph ltx_font_italic" id="S2.Thmdefinition1.p1.2.1">critical sharpness</em> as the scaled reciprocal of the critical learning rate <math alttext="\lambda_{c}=2/\eta_{c}" class="ltx_Math" display="inline" id="S2.Thmdefinition1.p1.2.m1" intent=":literal"><semantics><mrow><msub><mi>Î»</mi><mi>c</mi></msub><mo>=</mo><mrow><mn>2</mn><mo>/</mo><msub><mi>Î·</mi><mi>c</mi></msub></mrow></mrow><annotation encoding="application/x-tex">\lambda_{c}=2/\eta_{c}</annotation></semantics></math>.</p>
</div>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S2.SS2.SSS0.Px1.1.1">Geometric Interpretation</span>:</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.1">Critical sharpness provides an intuitive, optimizer-aware measure of the local curvature of the loss landscape. Geometrically, it quantifies the â€œnatural length-scaleâ€ of the landscape by estimating how far one can move in the current update direction without leaving the local region. While Hessian sharpness quantifies the curvature along the steepest direction of the landscape, critical sharpness measures the curvature along the update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math>.
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.F2" title="In 2.1 Setup â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">2</span></a> compares critical sharpness with Hessian sharpness on an illustrative landscape.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Efficient estimation of critical learning rate:</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.2">To efficiently measure the critical learning rate, we build upon the line search method proposed by <cite class="ltx_cite ltx_citemacro_cite">Kalra and Barkeshli (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib22" title="">2024</a>)</cite>. Given the update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.1.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math> from training, we compute the critical learning rate <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.2.m2" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math> using a two-phase line search procedure:</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.6"><em class="ltx_emph ltx_font_italic" id="S2.I1.i1.p1.6.1">Exponential Search:</em> Starting from an initial guess <math alttext="\eta_{0}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.1.m1" intent=":literal"><semantics><msub><mi>Î·</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\eta_{0}</annotation></semantics></math>, we iteratively double (halve) <math alttext="\eta" class="ltx_Math" display="inline" id="S2.I1.i1.p1.2.m2" intent=":literal"><semantics><mi>Î·</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> until the loss <math alttext="L(\bm{\theta}-\eta\Delta\bm{\theta})" class="ltx_Math" display="inline" id="S2.I1.i1.p1.3.m3" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta\Delta\bm{\theta})</annotation></semantics></math> exceeds (or falls below) the current loss <math alttext="L(\bm{\theta})" class="ltx_Math" display="inline" id="S2.I1.i1.p1.4.m4" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta})</annotation></semantics></math>. This quickly identifies an interval <math alttext="[\eta_{\text{lower}},\eta_{\text{upper}}]" class="ltx_Math" display="inline" id="S2.I1.i1.p1.5.m5" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>,</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta_{\text{lower}},\eta_{\text{upper}}]</annotation></semantics></math> containing <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.6.m6" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.4"><em class="ltx_emph ltx_font_italic" id="S2.I1.i2.p1.4.1">Binary Search:</em> We then refine this interval using a binary search until the relative error <math alttext="|1-\eta_{\text{lower}}/\eta_{\text{upper}}|" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>/</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub></mrow></mrow><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|1-\eta_{\text{lower}}/\eta_{\text{upper}}|</annotation></semantics></math> falls below a threshold <math alttext="\epsilon" class="ltx_Math" display="inline" id="S2.I1.i2.p1.2.m2" intent=":literal"><semantics><mi>Ïµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>. For <math alttext="\epsilon=\frac{1}{2^{k}}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.3.m3" intent=":literal"><semantics><mrow><mi>Ïµ</mi><mo>=</mo><mfrac><mn>1</mn><msup><mn>2</mn><mi>k</mi></msup></mfrac></mrow><annotation encoding="application/x-tex">\epsilon=\frac{1}{2^{k}}</annotation></semantics></math>, the binary search converges in <math alttext="k" class="ltx_Math" display="inline" id="S2.I1.i2.p1.4.m4" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> steps.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.11">We approximate the critical learning rate using the mean of the final range <math alttext="\eta_{c}\approx\frac{1}{2}(\eta_{\text{lower}}+\eta_{\text{upper}})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.3.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mi>c</mi></msub><mo>â‰ˆ</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>+</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\eta_{c}\approx\frac{1}{2}(\eta_{\text{lower}}+\eta_{\text{upper}})</annotation></semantics></math>.
In practice, the exponential search typically requires only <math alttext="1" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.4.m2" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>â€“<math alttext="2" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.5.m3" intent=":literal"><semantics><mn>2</mn><annotation encoding="application/x-tex">2</annotation></semantics></math> iterations (except for the first use, which depends on the initial guess <math alttext="\eta_{0}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.6.m4" intent=":literal"><semantics><msub><mi>Î·</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\eta_{0}</annotation></semantics></math>), and setting <math alttext="\epsilon=1/16" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.7.m5" intent=":literal"><semantics><mrow><mi>Ïµ</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mn>16</mn></mrow></mrow><annotation encoding="application/x-tex">\epsilon=1/16</annotation></semantics></math> (i.e., <math alttext="k=4" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.8.m6" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">k=4</annotation></semantics></math>) provides a reliable and efficient estimate of the critical learning rate in approximately <math alttext="5" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.9.m7" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math>â€“<math alttext="6" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.10.m8" intent=":literal"><semantics><mn>6</mn><annotation encoding="application/x-tex">6</annotation></semantics></math> forward passes, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.F3" title="In 2.3 The Relationship between Critical Sharpness and Hessian Sharpness â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">3</span></a>.
Our approach is scalable, as it relies solely on forward passes to evaluate <math alttext="L(\bm{\theta}-\eta\Delta\bm{\theta})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.11.m9" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta\Delta\bm{\theta})</annotation></semantics></math> and leverages the same computational primitives as standard first-order distributed training, avoiding the challenges of Hessian-based methods.
In <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S7" title="7 Estimating Critical Learning Rate Using Forward Passes â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Section</span>Ëœ<span class="ltx_text ltx_ref_tag">7</span></a>, we provide the detailed algorithm and further discuss the design choices.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>The Relationship between Critical Sharpness and Hessian Sharpness</h3>
<figure class="ltx_figure" id="S2.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="567" id="S2.F3.sf1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S2.F3.sf1.3.2" style="font-size:90%;">Batch size = 50,000 (GD)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="567" id="S2.F3.sf2.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S2.F3.sf2.3.2" style="font-size:90%;">Batch size = 5000</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S2.F3.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="567" id="S2.F3.sf3.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.sf3.2.1.1" style="font-size:90%;">(c)</span> </span><span class="ltx_text" id="S2.F3.sf3.3.2" style="font-size:90%;">Batch size = 500</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.6.3.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.4.2" style="font-size:90%;">Comparison of different sharpness measures for MLPs trained on CIFAR-10 image classification task using SGD with learning rate <math alttext="\eta" class="ltx_Math" display="inline" id="S2.F3.3.1.m1" intent=":literal"><semantics><mi>Î·</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math>.
Both critical and directional sharpness exhibit progressive sharpening and Edge of Stability, albeit some deviations from Hessian sharpness.
The dashed line denotes the Edge of Stability threshold, given by <math alttext="2/\eta" class="ltx_Math" display="inline" id="S2.F3.4.2.m2" intent=":literal"><semantics><mrow><mn>2</mn><mo>/</mo><mi>Î·</mi></mrow><annotation encoding="application/x-tex">2/\eta</annotation></semantics></math>.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Before investigating the empirical behavior of critical sharpness, we establish its theoretical relationship with Hessian sharpness. This connection provides a principled foundation for understanding what critical sharpness captures.
To this end, we consider the quadratic approximation of the loss function along <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx2">
<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle L(\bm{\theta}-\eta\Delta\bm{\theta})\approx L(\bm{\theta})-\eta\Delta\bm{\theta}^{T}g(\bm{\theta})+\frac{1}{2}\eta^{2}\Delta\bm{\theta}^{T}H(\bm{\theta})\Delta\bm{\theta}." class="ltx_Math" display="inline" id="S2.Ex1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>â‰ˆ</mo><mrow><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğœ½</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>+</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>Î·</mi><mn>2</mn></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğœ½</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle L(\bm{\theta}-\eta\Delta\bm{\theta})\approx L(\bm{\theta})-\eta\Delta\bm{\theta}^{T}g(\bm{\theta})+\frac{1}{2}\eta^{2}\Delta\bm{\theta}^{T}H(\bm{\theta})\Delta\bm{\theta}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS3.p1.4">Within this approximation, the loss will increase if the learning rate <math alttext="\eta" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m1" intent=":literal"><semantics><mi>Î·</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> exceeds <math alttext="2/\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m2" intent=":literal"><semantics><mrow><mn>2</mn><mo>/</mo><msub><mi>Î»</mi><mtext>dir</mtext></msub></mrow><annotation encoding="application/x-tex">2/\lambda_{\text{dir}}</annotation></semantics></math>, where <math alttext="\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S2.SS3.p1.4.m3" intent=":literal"><semantics><msub><mi>Î»</mi><mtext>dir</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{dir}}</annotation></semantics></math> is the <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.4.1">directional sharpness</em> <cite class="ltx_cite ltx_citemacro_citep">(Pan and Li, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib34" title="">2022</a>; Roulet et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib37" title="">2024</a>)</cite>:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="S2.Thmdefinition2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S2.Thmdefinition2.2.1.1">Definition 2.2</span></span><span class="ltx_text ltx_font_bold" id="S2.Thmdefinition2.3.2"> </span>(Directional Sharpness <math alttext="\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S2.Thmdefinition2.1.m1" intent=":literal"><semantics><msub><mi>Î»</mi><mtext>dir</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{dir}}</annotation></semantics></math>)<span class="ltx_text ltx_font_bold" id="S2.Thmdefinition2.4.3">.</span>
</h6>
<div class="ltx_para" id="S2.Thmdefinition2.p1">
<p class="ltx_p" id="S2.Thmdefinition2.p1.1">The directional sharpness along the update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S2.Thmdefinition2.p1.1.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math> is:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx3">
<tbody id="S2.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\text{dir}}=\frac{\Delta\bm{\theta}^{T}H(\bm{\theta})\Delta\bm{\theta}}{\Delta\bm{\theta}^{T}g(\bm{\theta})}." class="ltx_Math" display="inline" id="S2.E2.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Î»</mi><mtext>dir</mtext></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğœ½</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğœ½</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\text{dir}}=\frac{\Delta\bm{\theta}^{T}H(\bm{\theta})\Delta\bm{\theta}}{\Delta\bm{\theta}^{T}g(\bm{\theta})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Directional sharpness <math alttext="\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S2.SS3.p2.1.m1" intent=":literal"><semantics><msub><mi>Î»</mi><mtext>dir</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{dir}}</annotation></semantics></math> serves as an analytically tractable approximation to the empirically measured critical sharpness. <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.F3" title="In 2.3 The Relationship between Critical Sharpness and Hessian Sharpness â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">3</span></a> shows that directional sharpness closely tracks critical sharpness throughout training.
This alignment suggests that the local quadratic approximation well captures the complex dynamics in this simplistic setting.
For Gradient Descent (GD), the directional sharpness can be expressed as a weighted sum of Hessian eigenvalues <cite class="ltx_cite ltx_citemacro_citep">(Roulet et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib37" title="">2024</a>)</cite>:</p>
</div>
<div class="ltx_theorem ltx_theorem_result" id="S2.Thmresult1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S2.Thmresult1.1.1.1">Result 2.1</span></span><span class="ltx_text ltx_font_bold" id="S2.Thmresult1.2.2"> </span>(Relationship between Directional and Hessian Sharpness for Gradient Descent)<span class="ltx_text ltx_font_bold" id="S2.Thmresult1.3.3">.</span>
</h6>
<div class="ltx_para" id="S2.Thmresult1.p1">
<p class="ltx_p" id="S2.Thmresult1.p1.3">For Gradient Descent (GD), the directional sharpness <math alttext="\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S2.Thmresult1.p1.1.m1" intent=":literal"><semantics><msub><mi>Î»</mi><mtext>dir</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{dir}}</annotation></semantics></math> can be expressed as a weighted sum of the Hessian eigenvalues <math alttext="\{\lambda_{i}^{H}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S2.Thmresult1.p1.2.m2" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>Î»</mi><mi>i</mi><mi>H</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{\lambda_{i}^{H}\}_{i=1}^{n}</annotation></semantics></math>, where the weights quantify the alignment of the gradient with Hessian eigendirections <math alttext="\{u_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S2.Thmresult1.p1.3.m3" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>u</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{u_{i}\}_{i=1}^{n}</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx4">
<tbody id="S2.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{H}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda_{\max}^{H}," class="ltx_Math" display="inline" id="S2.E3.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Î»</mi><mtext>dir</mtext></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>i</mi><mi>H</mi></msubsup></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>â‰¤</mo><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{H}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda_{\max}^{H},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.Thmresult1.p1.5">where <math alttext="c_{i}=\bm{g}^{T}\bm{u}_{i}" class="ltx_Math" display="inline" id="S2.Thmresult1.p1.4.m1" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><msup><mi>ğ’ˆ</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ’–</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">c_{i}=\bm{g}^{T}\bm{u}_{i}</annotation></semantics></math> is the projection of the gradient onto the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S2.Thmresult1.p1.5.m2" intent=":literal"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> eigenvector.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.2">It follows that if the gradient <math alttext="\bm{g}" class="ltx_Math" display="inline" id="S2.SS3.p3.1.m1" intent=":literal"><semantics><mi>ğ’ˆ</mi><annotation encoding="application/x-tex">\bm{g}</annotation></semantics></math> is perfectly aligned with the largest eigenvector, i.e., <math alttext="\bm{g}\propto\bm{u}_{\max}" class="ltx_Math" display="inline" id="S2.SS3.p3.2.m2" intent=":literal"><semantics><mrow><mi>ğ’ˆ</mi><mo>âˆ</mo><msub><mi>ğ’–</mi><mi>max</mi></msub></mrow><annotation encoding="application/x-tex">\bm{g}\propto\bm{u}_{\max}</annotation></semantics></math>, then directional sharpness coincides with Hessian sharpness. More generally, directional sharpness is always bounded above by Hessian sharpness. As a consequence, the gap between Hessian and directional sharpness quantifies the alignment of the gradient with the top eigendirection of the Hessian.
We now generalize this result to adaptive optimizers.</p>
</div>
<div class="ltx_theorem ltx_theorem_result" id="S2.Thmresult2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S2.Thmresult2.1.1.1">Result 2.2</span></span><span class="ltx_text ltx_font_bold" id="S2.Thmresult2.2.2"> </span>(Relationship between Directional and Hessian Sharpness for Adaptive optimizers)<span class="ltx_text ltx_font_bold" id="S2.Thmresult2.3.3">.</span>
</h6>
<div class="ltx_para" id="S2.Thmresult2.p1">
<p class="ltx_p" id="S2.Thmresult2.p1.5">For adaptive optimizers (e.g., Adam) with pre-conditioner <math alttext="P" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.1.m1" intent=":literal"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>, the directional sharpness <math alttext="\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.2.m2" intent=":literal"><semantics><msub><mi>Î»</mi><mtext>dir</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{dir}}</annotation></semantics></math> can be expressed as a weighted sum of the pre-conditioned Hessian eigenvalues <math alttext="\{\lambda^{PH}_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.3.m3" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>Î»</mi><mi>i</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{\lambda^{PH}_{i}\}_{i=1}^{n}</annotation></semantics></math>, where the weights quantify the alignment of the pre-conditioned gradient <math alttext="P^{-1/2}\bm{g}" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.4.m4" intent=":literal"><semantics><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğ’ˆ</mi></mrow><annotation encoding="application/x-tex">P^{-1/2}\bm{g}</annotation></semantics></math> with pre-conditioned Hessian eigendirections <math alttext="\{\bm{v}_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.5.m5" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>ğ’—</mi><mi>i</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{\bm{v}_{i}\}_{i=1}^{n}</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx5">
<tbody id="S2.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{PH}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda_{\max}^{PH}," class="ltx_Math" display="inline" id="S2.E4.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Î»</mi><mtext>dir</mtext></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>i</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>â‰¤</mo><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{PH}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda_{\max}^{PH},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.Thmresult2.p1.9">where <math alttext="c_{i}=P^{-1/2}\bm{g}^{T}\bm{v}_{i}" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.6.m1" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğ’ˆ</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ’—</mi><mi>i</mi></msub></mrow></mrow><annotation encoding="application/x-tex">c_{i}=P^{-1/2}\bm{g}^{T}\bm{v}_{i}</annotation></semantics></math> is the projection of the pre-conditioned gradient <math alttext="P^{-1/2}g(\bm{\theta})" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.7.m2" intent=":literal"><semantics><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P^{-1/2}g(\bm{\theta})</annotation></semantics></math> onto the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.8.m3" intent=":literal"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> eigenvector <math alttext="\bm{v}_{i}" class="ltx_Math" display="inline" id="S2.Thmresult2.p1.9.m4" intent=":literal"><semantics><msub><mi>ğ’—</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\bm{v}_{i}</annotation></semantics></math> of the pre-conditioned Hessian.</p>
</div>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Together, these results establish a connection between directional sharpness and Hessian sharpness, and indicate that the two measures diverge when the alignment between the gradient and the top eigendirection is small. In turn, Hessian and critical sharpness coincide when the loss surface is approximately locally quadratic, and the gradient is primarily along the top eigendirection.
We detail the derivations of the above results in <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S10.SS1" title="10.1 The Relationship between Directional and Hessian sharpness â€£ 10 Theoretical Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Section</span>Ëœ<span class="ltx_text ltx_ref_tag">10.1</span></a>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">We examine the differences between the three sharpness measures in <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.F3" title="In 2.3 The Relationship between Critical Sharpness and Hessian Sharpness â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">3</span></a>.
In the full-batch regime, Hessian sharpness exhibits progressive sharpening, eventually reaching the Edge of Stability (EoS) threshold. In contrast, both directional sharpness and critical sharpness remain nearly constant during the early stages of training, followed by an abrupt increase to the EoS threshold.
At smaller batch sizes, however, all three sharpness measures display a gradual increase from the onset of training. Notably, after crossing the EoS threshold, Hessian sharpness tends to oscillate above the threshold, whereas both critical sharpness and directional sharpness oscillate more closely around it.
Overall, critical sharpness exhibits progressive sharpening and Edge of Stability, albeit with some differences compared to Hessian sharpness discussed above. In the following section, we extend our analysis to more realistic, large-scale training settings.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Critical Sharpness Dynamics at Scale</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Modern large-scale models are typically trained using Adam with weight decay <cite class="ltx_cite ltx_citemacro_citep">(Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib30" title="">2019</a>)</cite>, which helps mitigate training instabilities <cite class="ltx_cite ltx_citemacro_citep">(Dâ€™Angelo et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib10" title="">2024</a>)</cite>. To analyze sharpness dynamics in this context, we first analyze the stability threshold for common optimizers with weight decay:</p>
</div>
<div class="ltx_theorem ltx_theorem_result" id="S3.Thmresult1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S3.Thmresult1.1.1.1">Result 3.1</span></span><span class="ltx_text ltx_font_bold" id="S3.Thmresult1.2.2"> </span>(Stability threshold for optimizers with weight decay)<span class="ltx_text ltx_font_bold" id="S3.Thmresult1.3.3">.</span>
</h6>
<div class="ltx_para" id="S3.Thmresult1.p1">
<p class="ltx_p" id="S3.Thmresult1.p1.1">For gradient descent and adaptive optimizers such as Adam, adding weight decay shifts the EoS threshold by a constant that depends on the decay strength <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.Thmresult1.p1.1.m1" intent=":literal"><semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx6">
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda^{H}_{\max}" class="ltx_Math" display="inline" id="S3.Ex2.m1" intent=":literal"><semantics><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup><annotation encoding="application/x-tex">\displaystyle\lambda^{H}_{\max}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{2}{\eta}-\gamma\quad" class="ltx_Math" display="inline" id="S3.Ex2.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>2</mn><mi>Î·</mi></mfrac></mstyle><mo>âˆ’</mo><mi>Î³</mi></mrow></mrow><mspace style="width:1em;" width="1em"></mspace></mrow><annotation encoding="application/x-tex">\displaystyle=\frac{2}{\eta}-\gamma\quad</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.Ex2.6.2.1.1">(GD)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda^{PH}_{\max}" class="ltx_Math" display="inline" id="S3.E5.m1" intent=":literal"><semantics><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup><annotation encoding="application/x-tex">\displaystyle\lambda^{PH}_{\max}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\left(\frac{2}{\eta}-\gamma\right)\left(\frac{1+\beta_{1}}{1-\beta_{1}}\right)\quad" class="ltx_Math" display="inline" id="S3.E5.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>2</mn><mi>Î·</mi></mfrac></mstyle><mo>âˆ’</mo><mi>Î³</mi></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow></mfrac></mstyle><mo>)</mo></mrow></mrow></mrow><mspace style="width:1em;" width="1em"></mspace></mrow><annotation encoding="application/x-tex">\displaystyle=\left(\frac{2}{\eta}-\gamma\right)\left(\frac{1+\beta_{1}}{1-\beta_{1}}\right)\quad</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E5.6.2.1.1">(Adam)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.Thmresult1.p1.4">where <math alttext="\eta" class="ltx_Math" display="inline" id="S3.Thmresult1.p1.2.m1" intent=":literal"><semantics><mi>Î·</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> is the learning rate, <math alttext="\beta_{1}" class="ltx_Math" display="inline" id="S3.Thmresult1.p1.3.m2" intent=":literal"><semantics><msub><mi>Î²</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_{1}</annotation></semantics></math> is Adamâ€™s momentum parameter, and <math alttext="\gamma" class="ltx_Math" display="inline" id="S3.Thmresult1.p1.4.m3" intent=":literal"><semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is the weight decay strength. We provide the detailed proofs in <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S10.SS2" title="10.2 Stability Threshold for Optimizers with Weight Decay â€£ 10 Theoretical Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Section</span>Ëœ<span class="ltx_text ltx_ref_tag">10.2</span></a>.</p>
</div>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We now analyze GPT-style Transformers pre-trained for next-token prediction on the FineWebEdu dataset <cite class="ltx_cite ltx_citemacro_citep">(Penedo et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib35" title="">2024</a>)</cite> using AdamW with Warmup-Stable-Decay (WSD) schedule <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib18" title="">2024</a>)</cite>.
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S3.F4" title="In 3 Critical Sharpness Dynamics at Scale â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">4</span></a> compares the dynamics of critical and pre-conditioned sharpness for three different learning rates. The pre-conditioned sharpness <math alttext="\lambda^{PH}_{\max}" class="ltx_Math" display="inline" id="S3.p2.1.m1" intent=":literal"><semantics><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup><annotation encoding="application/x-tex">\lambda^{PH}_{\max}</annotation></semantics></math> exhibits progressive sharpening while following the learning rate schedule throughout training â€”it is pushed down during the warmup phase, stays constant at the stability threshold during the stable phase, and increases again when the learning rate is decayed. Critical sharpness and directional sharpness follow similar trends throughout training â€”they exhibit clear progressive sharpening and EoS behavior, while oscillating below the EoS threshold during the stable phase and capturing the increase in pre-conditioned sharpness during the decay phase.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S3.F4.sf1.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S3.F4.sf2.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F4.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S3.F4.sf3.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.4.2.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.2.1" style="font-size:90%;">Dynamics of pre-conditioned, directional, and critical sharpness during GPT-style Transformer training on FineWebEdu with AdamW using a Warmup-Stable-Decay (WSD) schedule. Critical sharpness tracks pre-conditioned sharpness throughout training, making it an effective proxy. The colored dashed lines denote the theoretical learning rate threshold <math alttext="(2+2\beta_{1})/(1-\beta_{1})\eta" class="ltx_Math" display="inline" id="S3.F4.2.1.m1" intent=":literal"><semantics><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>2</mn><mo>+</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>/</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î·</mi></mrow><annotation encoding="application/x-tex">(2+2\beta_{1})/(1-\beta_{1})\eta</annotation></semantics></math> and the two black vertical lines mark the end of warmup and stable phases.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">While progressive sharpening has been consistently documented at small scales, especially in full-batch settings, its manifestation and relevance at large scales, particularly in the context of online LLM training, remain largely unexplored.
Having established that critical sharpness serves as an efficient and reliable proxy for progressive sharpening and Edge of Stability phenomena, we are now in the position to investigate whether progressive sharpening persists in large-scale LLM training.
To this end, we analyze the publicly available OLMo-2 <math alttext="7" class="ltx_Math" display="inline" id="S3.p3.1.m1" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math>B checkpoints <cite class="ltx_cite ltx_citemacro_citep">(Walsh et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib41" title="">2025</a>)</cite>. OLMo-2 provides checkpoints throughout both pre-training and mid-training stages, enabling us to study sharpness dynamics at scale without the computational cost of training such models from scratch.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.5">The OLMo-2 models are trained using a two-stage curriculum approach. During the pre-training stage (<math alttext="&gt;90\%" class="ltx_Math" display="inline" id="S3.p4.1.m1" intent=":literal"><semantics><mrow><mi></mi><mo>&gt;</mo><mrow><mn>90</mn><mo>%</mo></mrow></mrow><annotation encoding="application/x-tex">&gt;90\%</annotation></semantics></math> of compute), the model is trained for <math alttext="4" class="ltx_Math" display="inline" id="S3.p4.2.m2" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math>T tokens on the DCLM dataset <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib28" title="">2025</a>)</cite>, a diverse web-sourced corpus.
Following pre-training, the model undergoes a mid-training stage, where it is trained for <math alttext="50" class="ltx_Math" display="inline" id="S3.p4.3.m3" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math>B tokens on a curated mix.
This mix includes academic papers, Wikipedia articles, instruction-following data, StackExchange documents, and approximately <math alttext="50\%" class="ltx_Math" display="inline" id="S3.p4.4.m4" intent=":literal"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math> of the original pre-training dataset (DCLM).
During the pre-training stage, the model is trained using a learning rate schedule consisting of <math alttext="2{,}000" class="ltx_Math" display="inline" id="S3.p4.5.m5" intent=":literal"><semantics><mrow><mn>2</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">2{,}000</annotation></semantics></math> steps of warmup, followed by a cosine decay down to one-tenth of its peak value<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>For the OLMo-2 <math alttext="7" class="ltx_Math" display="inline" id="footnote2.m1" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math>B models, the cosine decay schedule is designed to reach one-tenth of the peak learning rate at <math alttext="5" class="ltx_Math" display="inline" id="footnote2.m2" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math>T tokens, but is truncated at <math alttext="4" class="ltx_Math" display="inline" id="footnote2.m3" intent=":literal"><semantics><mn>4</mn><annotation encoding="application/x-tex">4</annotation></semantics></math>T tokens.</span></span></span>. By comparison, during the mid-training stage, the learning rate is linearly decayed to zero.
As the learning rate is continuously decreased throughout training after the warmup stage, the EoS threshold increases, and consequently we expect sharpness to progressively increase for the remainder of training.
To confirm this, we examine how the critical sharpness evolves across pre-training and mid-training using the publicly available checkpoints (see <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S8" title="8 Experimental Details â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Section</span>Ëœ<span class="ltx_text ltx_ref_tag">8</span></a> for the details).
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S3.F5" title="In 3 Critical Sharpness Dynamics at Scale â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">5</span></a> shows that critical sharpness rapidly decreases during early training, but then continually increases (progressive sharpening) throughout both the pre-training and mid-training stages. This result provides the first empirical evidence of progressive sharpening at scale in practical LLM training settings.</p>
</div>
<figure class="ltx_figure" id="S3.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S3.F5.sf1.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S3.F5.sf2.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.4.2.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.2.1" style="font-size:90%;">Critical Sharpness of OLMo-2 <math alttext="7" class="ltx_Math" display="inline" id="S3.F5.2.1.m1" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math>B exhibits progressive sharpening throughout pre-training and mid-training. In the mid-training figure, the band around the mean trend shows the deviation across the three runs. </span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S4" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>How much Pre-training data is needed to avoid Catastrophic forgetting?</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To go beyond demonstrating pre-existing sharpness phenomena at scale, we next provide a new practical application of critical sharpness, focused on data mixing. We are motivated by the fact that during finetuning, neural networks are prone to <em class="ltx_emph ltx_font_italic" id="S4.p1.1.1">catastrophic forgetting</em>, where the model performance degrades on the pretraining dataset and benchmarks as the model adapts to the new task <cite class="ltx_cite ltx_citemacro_citep">(McCloskey and Cohen, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib32" title="">1989</a>; Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib31" title="">2025</a>; McLeish et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib33" title="">2025</a>)</cite>. To mitigate this, several strategies have been proposed <cite class="ltx_cite ltx_citemacro_citep">(De Lange et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib13" title="">2022</a>)</cite>, with mixing samples from the pre-training data being the most effective <cite class="ltx_cite ltx_citemacro_citep">(Robins, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib36" title="">1995</a>)</cite>. This practice is reflected in large-scale training: for example, <cite class="ltx_cite ltx_citemacro_cite">Walsh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib41" title="">2025</a>)</cite> use a mid-training mix consisting of approximately
<math alttext="50\%" class="ltx_Math" display="inline" id="S4.p1.1.m1" intent=":literal"><semantics><mrow><mn>50</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">50\%</annotation></semantics></math> pre-training data. Intuitively, we want to use as much as possible new data in fine-tuning, using as little pre-training rehearsal data as possible to retain base capabilities. However, it remains unclear what fraction of pre-training data is sufficient to effectively prevent catastrophic forgetting.
To this end, we leverage critical sharpness to systematically examine the effect of adding pre-training data to the training mix during OLMo mid-training and provide actionable guidance for selecting the pre-training fraction without exhaustive grid search.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">The goal of mid-training or finetuning is to improve the performance on specialized domains or tasks while preserving generic capabilities acquired during pre-training. Intuitively, this requires the model to adapt to the new task, while staying within the â€œpre-training basinâ€ â€”a region of the parameter space where the pre-training loss remains low. Leaving this basin would be marked by an increase in the pre-training loss, and the critical learning rate quantifies exactly how far we can step before this occurs.
To formalize this intuition, we define relative critical learning rate and sharpness, as follows:</p>
</div>
<div class="ltx_theorem ltx_theorem_definition" id="S4.Thmdefinition1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S4.Thmdefinition1.3.1.1">Definition 4.1</span></span><span class="ltx_text ltx_font_bold" id="S4.Thmdefinition1.4.2"> </span>(Relative Critical learning rate <math alttext="\eta_{c}^{1\to 2}" class="ltx_Math" display="inline" id="S4.Thmdefinition1.1.m1" intent=":literal"><semantics><msubsup><mi>Î·</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup><annotation encoding="application/x-tex">\eta_{c}^{1\to 2}</annotation></semantics></math> and Sharpness <math alttext="\lambda_{c}^{1\to 2}" class="ltx_Math" display="inline" id="S4.Thmdefinition1.2.m2" intent=":literal"><semantics><msubsup><mi>Î»</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup><annotation encoding="application/x-tex">\lambda_{c}^{1\to 2}</annotation></semantics></math>)<span class="ltx_text ltx_font_bold" id="S4.Thmdefinition1.5.3">.</span>
</h6>
<div class="ltx_para" id="S4.Thmdefinition1.p1">
<p class="ltx_p" id="S4.Thmdefinition1.p1.7">Consider a model with parameters <math alttext="\bm{\theta}" class="ltx_Math" display="inline" id="S4.Thmdefinition1.p1.1.m1" intent=":literal"><semantics><mi>ğœ½</mi><annotation encoding="application/x-tex">\bm{\theta}</annotation></semantics></math>, two loss functions <math alttext="L_{1}(\bm{\theta})" class="ltx_Math" display="inline" id="S4.Thmdefinition1.p1.2.m2" intent=":literal"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L_{1}(\bm{\theta})</annotation></semantics></math> and <math alttext="L_{2}(\bm{\theta})" class="ltx_Math" display="inline" id="S4.Thmdefinition1.p1.3.m3" intent=":literal"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L_{2}(\bm{\theta})</annotation></semantics></math>, and an update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S4.Thmdefinition1.p1.4.m4" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math> derived from <math alttext="L_{2}" class="ltx_Math" display="inline" id="S4.Thmdefinition1.p1.5.m5" intent=":literal"><semantics><msub><mi>L</mi><mn>2</mn></msub><annotation encoding="application/x-tex">L_{2}</annotation></semantics></math>. The <em class="ltx_emph ltx_font_italic" id="S4.Thmdefinition1.p1.7.1">relative critical learning rate</em> is the smallest learning rate for which taking a step in the direction <math alttext="\Delta\bm{\theta}_{2}" class="ltx_Math" display="inline" id="S4.Thmdefinition1.p1.6.m6" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}_{2}</annotation></semantics></math> increases the loss <math alttext="L_{1}(\bm{\theta})" class="ltx_Math" display="inline" id="S4.Thmdefinition1.p1.7.m7" intent=":literal"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L_{1}(\bm{\theta})</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx7">
<tbody id="S4.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\eta_{c}^{1\to 2}=\min_{\eta&gt;0}\{\eta\mid L_{1}(\bm{\theta}-\eta\Delta\bm{\theta}_{2})&gt;L_{1}(\bm{\theta})\}." class="ltx_Math" display="inline" id="S4.E6.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>Î·</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup><mo>=</mo><mrow><munder><mi>min</mi><mrow><mi>Î·</mi><mo>&gt;</mo><mn>0</mn></mrow></munder><mo>â¡</mo><mrow><mo stretchy="false">{</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo fence="true" rspace="0em">âˆ£</mo><mrow><msub><mi>L</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mn>2</mn></msub></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo fence="true" lspace="0em">&gt;</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>L</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\eta_{c}^{1\to 2}=\min_{\eta&gt;0}\{\eta\mid L_{1}(\bm{\theta}-\eta\Delta\bm{\theta}_{2})&gt;L_{1}(\bm{\theta})\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.Thmdefinition1.p1.8">The corresponding <em class="ltx_emph ltx_font_italic" id="S4.Thmdefinition1.p1.8.1">relative critical sharpness</em> is <math alttext="\lambda_{c}^{1\to 2}=2/\eta_{c}^{1\to 2}" class="ltx_Math" display="inline" id="S4.Thmdefinition1.p1.8.m1" intent=":literal"><semantics><mrow><msubsup><mi>Î»</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup><mo>=</mo><mrow><mn>2</mn><mo>/</mo><msubsup><mi>Î·</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">\lambda_{c}^{1\to 2}=2/\eta_{c}^{1\to 2}</annotation></semantics></math>.</p>
</div>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.2">This definition is general and applies whenever optimization on one objective may affect performance on another. The two losses can correspond to different tasks (e.g., pre-training vs. finetuning), different loss functions (e.g., next-token prediction vs. reinforcement learning objectives), or different data characteristics (e.g., short vs. long context). In this work, we focus on the finetuning setting, where <math alttext="L_{1}({\bm{\theta}})" class="ltx_Math" display="inline" id="S4.p3.1.m1" intent=":literal"><semantics><mrow><msub><mi>L</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L_{1}({\bm{\theta}})</annotation></semantics></math> is the pre-training loss on a general text corpus and <math alttext="L_{2}({\bm{\theta}})" class="ltx_Math" display="inline" id="S4.p3.2.m2" intent=":literal"><semantics><mrow><msub><mi>L</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L_{2}({\bm{\theta}})</annotation></semantics></math> is the finetuning loss on a specialized dataset such as math.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">To assess the impact of pre-training data fraction in the training mix during fine-training, we consider the OLMo-2 <math alttext="7" class="ltx_Math" display="inline" id="S4.p4.1.m1" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math>B pre-trained checkpoint as our starting point. We compute the update direction from a mixture composed of DCLM (pre-training data) and the math subset of Dolmino mix <cite class="ltx_cite ltx_citemacro_citep">(Walsh et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib41" title="">2025</a>)</cite>, and examine how the relative critical sharpness varies with the DCLM fraction in this mixture<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>When measuring the relative critical sharpness, we do not update the model parameters.</span></span></span>.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S4.F6" title="In 4 How much Pre-training data is needed to avoid Catastrophic forgetting? â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">6</span></a>(a) shows how the relative critical sharpness varies with the DCLM ratio in the training mix for several evaluation tasks. When the mix contains mostly math data (low DCLM ratio), the sharpness for DCLM is an order of magnitude higher than for other tasks, indicating that the pre-training loss landscape is particularly sharp and sensitive to finetuning updates in this regime. As more pre-training data is added, the relative critical sharpness for most tasks decreases, suggesting that the landscapes align.
On the other hand, when the DCLM ratio approaches one, the sharpness for downstream tasks such as Math, GSM8K, and MMLU increases, meaning that these tasks become the limiting factor for the maximum stable learning rate. Notably, there is an intermediate DCLM ratio (around 0.7) where the sharpness curves for different tasks intersect, representing a sweet spot that allows for the largest possible learning rate without being constrained by any single task.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="581" id="S4.F6.sf1.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="581" id="S4.F6.sf2.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="581" id="S4.F6.sf3.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">(a) Relative critical sharpness (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S4.Thmdefinition1" title="Definition 4.1 (Relative Critical learning rate ğœ‚_ğ‘^{1â†’2} and Sharpness ğœ†_ğ‘^{1â†’2}). â€£ 4 How much Pre-training data is needed to avoid Catastrophic forgetting? â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Definition</span>Ëœ<span class="ltx_text ltx_ref_tag">4.1</span></a>) for various evaluation tasks. The shaded region around the mean trends denotes the variation across batches. (b, c) GSM8K and MMLU accuracy as a function of pre-training (DCLM) mix ratio and learning rate. Red indicates a decrease in performance relative to the checkpoint, white indicates no change, and blue indicates an improvement. The black dashed line denotes the smallest critical learning rate among at the pre-trained checkpoint across tasks from (a) and white dashed line denotes the critical learning rate for the corresponding task.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.p6">
<p class="ltx_p" id="S4.p6.8">In <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S4.F6" title="In 4 How much Pre-training data is needed to avoid Catastrophic forgetting? â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">6</span></a>(b, c), we evaluate the impact of the pre-training mix by training the pre-trained checkpoint for <math alttext="1" class="ltx_Math" display="inline" id="S4.p6.1.m1" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>B tokens and measuring downstream accuracy on GSM8K and MMLU benchmarks. In these plots, red indicates a decrease in performance relative to the checkpoint, white indicates no change, and blue indicates an improvement.
We observe a natural trade-off: GSM8K accuracy is typically maximized outside the pre-training basin (i.e., with low DCLM ratio and high learning rate), but this comes at the expense of MMLU performance, which is best preserved within the pre-training basin. Therefore, if the sole objective is to improve on the finetuning task (math), then training primarily on it with a large learning rate is effective, though it may lead to forgetting on other tasks (MMLU). In contrast, if the goal is to improve on the finetuning task (math) while maintaining pre-training performance (MMLU), it is essential to include pre-training data in the mix, with a sweet spot emerging around a DCLM ratio of <math alttext="0.6" class="ltx_Math" display="inline" id="S4.p6.2.m2" intent=":literal"><semantics><mn>0.6</mn><annotation encoding="application/x-tex">0.6</annotation></semantics></math> and a learning rate of <math alttext="3e" class="ltx_Math" display="inline" id="S4.p6.3.m3" intent=":literal"><semantics><mrow><mn>3</mn><mo lspace="0em" rspace="0em">â€‹</mo><mi>e</mi></mrow><annotation encoding="application/x-tex">3e</annotation></semantics></math>-<math alttext="05" class="ltx_Math" display="inline" id="S4.p6.4.m4" intent=":literal"><semantics><mn>05</mn><annotation encoding="application/x-tex">05</annotation></semantics></math> in our experiment. Remarkably, this is close to the optimum of <math alttext="0.7" class="ltx_Math" display="inline" id="S4.p6.5.m5" intent=":literal"><semantics><mn>0.7</mn><annotation encoding="application/x-tex">0.7</annotation></semantics></math> suggested by the critical sharpness analysis. This result highlights the importance of balancing task-specific and general data in the training mix. Once the pre-trained loss <math alttext="L_{1}" class="ltx_Math" display="inline" id="S4.p6.6.m6" intent=":literal"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding="application/x-tex">L_{1}</annotation></semantics></math> increases (<math alttext="\eta&gt;2/\lambda_{c}^{1\to 2}" class="ltx_Math" display="inline" id="S4.p6.7.m7" intent=":literal"><semantics><mrow><mi>Î·</mi><mo>&gt;</mo><mrow><mn>2</mn><mo>/</mo><msubsup><mi>Î»</mi><mi>c</mi><mrow><mn>1</mn><mo stretchy="false">â†’</mo><mn>2</mn></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">\eta&gt;2/\lambda_{c}^{1\to 2}</annotation></semantics></math>), there is no guarantee that further updates will improve <math alttext="L_{1}" class="ltx_Math" display="inline" id="S4.p6.8.m8" intent=":literal"><semantics><msub><mi>L</mi><mn>1</mn></msub><annotation encoding="application/x-tex">L_{1}</annotation></semantics></math> again, unless the pre-train data is injected into the mix.</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.2">In <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.SS4" title="9.4 OLMo mid-training â€£ 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Section</span>Ëœ<span class="ltx_text ltx_ref_tag">9.4</span></a>, we extend the relative critical sharpness analysis to the datamix used in OLMo-2 mid-training. The results are consistent: we observe a sweet spot at a DCLM ratio of
<math alttext="0.6" class="ltx_Math" display="inline" id="S4.p7.1.m1" intent=":literal"><semantics><mn>0.6</mn><annotation encoding="application/x-tex">0.6</annotation></semantics></math>, close to the <math alttext="0.5" class="ltx_Math" display="inline" id="S4.p7.2.m2" intent=":literal"><semantics><mn>0.5</mn><annotation encoding="application/x-tex">0.5</annotation></semantics></math> ratio used in the original OLMo-2 training. We leave the validation of this prediction through downstream evaluation to future work.</p>
</div>
</section>
<section class="ltx_section" id="S5" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Works</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.2">In neural network training, if the learning rate exceeds the threshold <math alttext="\sim 2/\lambda^{H}_{\max}" class="ltx_Math" display="inline" id="S5.p1.1.m1" intent=":literal"><semantics><mrow><mi></mi><mo>âˆ¼</mo><mrow><mn>2</mn><mo>/</mo><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup></mrow></mrow><annotation encoding="application/x-tex">\sim 2/\lambda^{H}_{\max}</annotation></semantics></math>, it causes the loss to increase in the next training step <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib43" title="">2018</a>)</cite>. However, <cite class="ltx_cite ltx_citemacro_cite">Kalra and Barkeshli (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib21" title="">2023</a>)</cite> demonstrated that the empirical critical learning rate can be much higher than this theoretical threshold derived from convex analysis, with critical learning rate reaching up to <math alttext="40/\lambda^{H}_{\max}" class="ltx_Math" display="inline" id="S5.p1.2.m2" intent=":literal"><semantics><mrow><mn>40</mn><mo>/</mo><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup></mrow><annotation encoding="application/x-tex">40/\lambda^{H}_{\max}</annotation></semantics></math> when sharpness decreases significantly during training.
Our work extends theirs by showing that a similarly large critical learning rate can also arise when sharpness increases during training, due to contributions from other eigendirections (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.F3" title="In 2.3 The Relationship between Critical Sharpness and Hessian Sharpness â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">3</span></a>).
Beyond its role in characterizing training instabilities, Hessian sharpness also exhibits progressive sharpening during training. In particular, <cite class="ltx_cite ltx_citemacro_cite">Cohen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib6" title="">2021</a>)</cite> showed that sharpness increases when the learning rate is decayed, which we corroborate with additional analysis at large scales (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S3.F4" title="In 3 Critical Sharpness Dynamics at Scale â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figures</span>Ëœ<span class="ltx_text ltx_ref_tag">4</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S3.F5" title="Figure 5 â€£ 3 Critical Sharpness Dynamics at Scale â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">5</span></a>).</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.2">The works most closely related to ours are those by <cite class="ltx_cite ltx_citemacro_cite">Kalra and Barkeshli (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib22" title="">2024</a>)</cite> and <cite class="ltx_cite ltx_citemacro_cite">Roulet et al. (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib37" title="">2024</a>)</cite>. <cite class="ltx_cite ltx_citemacro_cite">Kalra and Barkeshli (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib22" title="">2024</a>)</cite> used critical learning rate to set the initial learning rate during warmup. By comparison, <cite class="ltx_cite ltx_citemacro_cite">Roulet et al. (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib37" title="">2024</a>)</cite> proposed setting the learning rate at â€˜edgeâ€™ throughout training, i.e., <math alttext="\eta_{t}=2/\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S5.p2.1.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mi>t</mi></msub><mo>=</mo><mrow><mn>2</mn><mo>/</mo><msub><mi>Î»</mi><mtext>dir</mtext></msub></mrow></mrow><annotation encoding="application/x-tex">\eta_{t}=2/\lambda_{\text{dir}}</annotation></semantics></math>, and show that it can outperform constant learning rate training, but not typical learning rate schedules, consisting of learning rate warmup followed by decay. <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib40" title="">2019</a>)</cite> also study setting the learning rate as <math alttext="\eta=1/\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S5.p2.2.m2" intent=":literal"><semantics><mrow><mi>Î·</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><msub><mi>Î»</mi><mtext>dir</mtext></msub></mrow></mrow><annotation encoding="application/x-tex">\eta=1/\lambda_{\text{dir}}</annotation></semantics></math>. In contrast, we use the critical learning rate to study the curvature dynamics of large-scale models and examine the effect of pre-training data on catastrophic forgetting.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Catastrophic forgetting is a long-standing problem in neural networks, first identified by <cite class="ltx_cite ltx_citemacro_cite">McCloskey and Cohen (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib32" title="">1989</a>)</cite>, where model performance degrades on previously learned tasks as it adapts to the new task. Approaches to mitigate catastrophic forgetting can be categorized into (i) regularization methods <cite class="ltx_cite ltx_citemacro_citep">(Kirkpatrick et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib25" title="">2017</a>; Ahn et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib2" title="">2019</a>)</cite>, (ii) ensembling and parameter isolation <cite class="ltx_cite ltx_citemacro_citep">(Rusu et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib38" title="">2016</a>; Aljundi et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib3" title="">2017</a>)</cite>, and (iii) rehearsal <cite class="ltx_cite ltx_citemacro_citep">(Robins, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib36" title="">1995</a>; Lopez-Paz and Ranzato, <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib29" title="">2017</a>)</cite>. Among these, rehearsal â€”mixing samples from the pre-training data â€”has become the most widely adopted strategy due to its simplicity and effectiveness. Recent works have demonstrated that catastrophic forgetting persists in LLM finetuning 
<cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib31" title="">2025</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib19" title="">2024</a>; Scialom et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib39" title="">2022</a>)</cite>. Particularly related to our work is that of <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib4" title="">2025</a>)</cite>, who show that if finetuning retains previously learned capabilities, the model remains in â€œmost-caseâ€ and â€œworst-caseâ€ basins.</p>
</div>
</section>
<section class="ltx_section" id="S6" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.2">In this work, we analyzed critical sharpness, a computationally efficient measure for studying the training dynamics of LLMs. Our results demonstrate that critical sharpness reliably captures key Hessian sharpness phenomena such as progressive sharpening and Edge of Stability, requiring fewer than <math alttext="10" class="ltx_Math" display="inline" id="S6.p1.1.m1" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math> forward passes, avoiding challenges associated with Hessian-based methods. Using this measure, we provided the first empirical evidence of progressive sharpening at the <math alttext="7" class="ltx_Math" display="inline" id="S6.p1.2.m2" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math>B parameter scale.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">We also introduced relative critical sharpness, which quantifies the curvature of one loss landscape along the update direction of another. Using this measure, we identified a sweet spot in the pre-training data fraction that balances specialization and retention during finetuning, enabling practitioners to evaluate data composition choices without extensive ablations. Beyond finetuning, relative critical sharpness provides a general framework for analyzing changes in the loss landscape due to distribution shifts, changes in loss functions, or modifications to the training data mixture.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">We believe critical sharpness can extend beyond the settings studied here.
More broadly, our results demonstrate that scalable curvature measures can provide actionable insights for large-scale training, from understanding optimization dynamics to informing data composition decisions.</p>
</div>
</section>
<section class="ltx_section" id="Sx1" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We would like to thank Tianyu He and Darshil Doshi for helpful discussions and detailed comments on the manuscript, and Sean McLeish, Konstantin Mishchenko, Aaron Defazio, Maissam Barkeshli, Benjamin Therien, and Jesse Dodge for helpful discussions.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwala and
Pennington (2025)</span>
<span class="ltx_bibblock">
Atish Agarwala and Jeffrey Pennington.

</span>
<span class="ltx_bibblock">High dimensional analysis reveals conservative sharpening and a
stochastic edge of stability, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.19261" title="">https://arxiv.org/abs/2404.19261</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahn et al. (2019)</span>
<span class="ltx_bibblock">
Hongjoon Ahn, Sungmin Cha, Donggyu Lee, and Taesup Moon.

</span>
<span class="ltx_bibblock">Uncertainty-based continual learning with adaptive regularization.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural
Information Processing Systems</em>, volume 32. Curran Associates, Inc., 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2019/file/2c3ddf4bf13852db711dd1901fb517fa-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aljundi et al. (2017)</span>
<span class="ltx_bibblock">
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars.

</span>
<span class="ltx_bibblock"> Expert Gate: Lifelong Learning with a Network of Experts .

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 7120â€“7129, Los Alamitos, CA, USA, July 2017. IEEE
Computer Society.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/CVPR.2017.753" title="">10.1109/CVPR.2017.753</a>.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.753" title="">https://doi.ieeecomputersociety.org/10.1109/CVPR.2017.753</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2025)</span>
<span class="ltx_bibblock">
Huanran Chen, Yinpeng Dong, Zeming Wei, Yao Huang, Yichi Zhang, Hang Su, and
Jun Zhu.

</span>
<span class="ltx_bibblock">Understanding pre-training and fine-tuning from loss landscape
perspectives, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2505.17646" title="">https://arxiv.org/abs/2505.17646</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
et al.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2110.14168</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen et al. (2021)</span>
<span class="ltx_bibblock">
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Gradient descent on neural networks typically occurs at the edge of
stability.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International Conference on Learning Representations</em>, 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=jh-rTtvkGeM" title="">https://openreview.net/forum?id=jh-rTtvkGeM</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen et al. (2025)</span>
<span class="ltx_bibblock">
Jeremy Cohen, Alex Damian, Ameet Talwalkar, J Zico Kolter, and Jason D. Lee.

</span>
<span class="ltx_bibblock">Understanding optimization in deep learning with central flows.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">The Thirteenth International Conference on Learning
Representations</em>, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=sIE2rI3ZPs" title="">https://openreview.net/forum?id=sIE2rI3ZPs</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen et al. (2024)</span>
<span class="ltx_bibblock">
Jeremy M. Cohen, Behrooz Ghorbani, Shankar Krishnan, Naman Agarwal, Sourabh
Medapati, Michal Badura, Daniel Suo, David Cardoze, Zachary Nado, George E.
Dahl, and Justin Gilmer.

</span>
<span class="ltx_bibblock">Adaptive gradient methods at the edge of stability, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.14484" title="">https://arxiv.org/abs/2207.14484</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Damian et al. (2023)</span>
<span class="ltx_bibblock">
Alex Damian, Eshaan Nichani, and Jason D. Lee.

</span>
<span class="ltx_bibblock">Self-stabilization: The implicit bias of gradient descent at the edge
of stability.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">The Eleventh International Conference on Learning
Representations</em>, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=nhKHA59gXz" title="">https://openreview.net/forum?id=nhKHA59gXz</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dâ€™Angelo et al. (2024)</span>
<span class="ltx_bibblock">
Francesco Dâ€™Angelo, Maksym Andriushchenko, Aditya Varre, and Nicolas
Flammarion.

</span>
<span class="ltx_bibblock">Why do we need weight decay in modern deep learning?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">The Thirty-eighth Annual Conference on Neural Information
Processing Systems</em>, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=YrAxxscKM2" title="">https://openreview.net/forum?id=YrAxxscKM2</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao et al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©.

</span>
<span class="ltx_bibblock">Flashattention: Fast and memory-efficient exact attention with
io-awareness.

</span>
<span class="ltx_bibblock">In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,
editors, <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Advances in Neural Information Processing Systems</em>, volume 35,
pages 16344â€“16359. Curran Associates, Inc., 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dauphin and Schoenholz (2019)</span>
<span class="ltx_bibblock">
Yann N Dauphin and Samuel Schoenholz.

</span>
<span class="ltx_bibblock">Metainit: Initializing learning by learning to initialize.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Advances in Neural
Information Processing Systems</em>, volume 32. Curran Associates, Inc., 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/876e8108f87eb61877c6263228b67256-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2019/file/876e8108f87eb61877c6263228b67256-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Lange et al. (2022)</span>
<span class="ltx_bibblock">
Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, AleÅ¡
Leonardis, Gregory Slabaugh, and Tinne Tuytelaars.

</span>
<span class="ltx_bibblock">A continual learning survey: Defying forgetting in classification
tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, 44(7):3366â€“3385, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/TPAMI.2021.3057446" title="">10.1109/TPAMI.2021.3057446</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elaydi (2005)</span>
<span class="ltx_bibblock">
S. Elaydi.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">An Introduction to Difference Equations</em>.

</span>
<span class="ltx_bibblock">Undergraduate Texts in Mathematics. Springer New York, 2005.

</span>
<span class="ltx_bibblock">ISBN 9780387230597.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://books.google.com/books?id=nPREsCQm_PYC" title="">https://books.google.com/books?id=nPREsCQm_PYC</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilmer et al. (2022)</span>
<span class="ltx_bibblock">
Justin Gilmer, Behrooz Ghorbani, Ankush Garg, Sneha Kudugunta, Behnam
Neyshabur, David Cardoze, George Edward Dahl, Zachary Nado, and Orhan Firat.

</span>
<span class="ltx_bibblock">A loss curvature perspective on training instabilities of deep
learning models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=OcKMT-36vUs" title="">https://openreview.net/forum?id=OcKMT-36vUs</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring massive multitask language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the International Conference on Learning
Representations (ICLR)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and JÃ¼rgen Schmidhuber.

</span>
<span class="ltx_bibblock">Flat minima.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Neural Computation</em>, 9:1â€“42, 1997.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:733161" title="">https://api.semanticscholar.org/CorpusID:733161</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2024)</span>
<span class="ltx_bibblock">
Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long,
Zhi Zheng, Yewei Fang, Yuxiang Huang, Xinrong Zhang, Zhen Leng Thai, Chongyi
Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, Ning Ding,
Chao Jia, Guoyang Zeng, dahai li, Zhiyuan Liu, and Maosong Sun.

</span>
<span class="ltx_bibblock">MiniCPM: Unveiling the potential of small language models with
scalable training strategies.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">First Conference on Language Modeling</em>, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=3X2L2TFr0f" title="">https://openreview.net/forum?id=3X2L2TFr0f</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Jianheng Huang, Leyang Cui, Ante Wang, Chengyi Yang, Xinting Liao, Linfeng
Song, Junfeng Yao, and Jinsong Su.

</span>
<span class="ltx_bibblock">Mitigating catastrophic forgetting in large language models with
self-synthesized rehearsal.

</span>
<span class="ltx_bibblock">In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pages 1416â€“1428,
Bangkok, Thailand, August 2024. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2024.acl-long.77" title="">10.18653/v1/2024.acl-long.77</a>.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.acl-long.77/" title="">https://aclanthology.org/2024.acl-long.77/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jastrzebski et al. (2020)</span>
<span class="ltx_bibblock">
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek
Tabor, Kyunghyun Cho*, and Krzysztof Geras*.

</span>
<span class="ltx_bibblock">The break-even point on optimization trajectories of deep neural
networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">International Conference on Learning Representations</em>, 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=r1g87C4KwB" title="">https://openreview.net/forum?id=r1g87C4KwB</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalra and Barkeshli (2023)</span>
<span class="ltx_bibblock">
Dayal Singh Kalra and Maissam Barkeshli.

</span>
<span class="ltx_bibblock">Phase diagram of early training dynamics in deep neural networks:
effect of the learning rate, depth, and width.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Thirty-seventh Conference on Neural Information Processing
Systems</em>, 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Al9yglQGKj" title="">https://openreview.net/forum?id=Al9yglQGKj</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalra and Barkeshli (2024)</span>
<span class="ltx_bibblock">
Dayal Singh Kalra and Maissam Barkeshli.

</span>
<span class="ltx_bibblock">Why warmup the learning rate? underlying mechanisms and improvements.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">The Thirty-eighth Annual Conference on Neural Information
Processing Systems</em>, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=NVl4SAmz5c" title="">https://openreview.net/forum?id=NVl4SAmz5c</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kalra et al. (2025)</span>
<span class="ltx_bibblock">
Dayal Singh Kalra, Tianyu He, and Maissam Barkeshli.

</span>
<span class="ltx_bibblock">Universal sharpness dynamics in neural network training: Fixed point
analysis, edge of stability, and route to chaos.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">The Thirteenth International Conference on Learning
Representations</em>, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=VZN0irKnl0" title="">https://openreview.net/forum?id=VZN0irKnl0</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaur et al. (2023)</span>
<span class="ltx_bibblock">
Simran Kaur, Jeremy Cohen, and Zachary Chase Lipton.

</span>
<span class="ltx_bibblock">On the maximum hessian eigenvalue and generalization.

</span>
<span class="ltx_bibblock">In Javier AntorÃ¡n, Arno Blaas, Fan Feng, Sahra Ghalebikesabi, Ian
Mason, Melanie F. Pradier, David Rohde, Francisco J. R. Ruiz, and Aaron
Schein, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings on "I Canâ€™t Believe Itâ€™s Not Better! -
Understanding Deep Learning Through Empirical Falsification" at NeurIPS 2022
Workshops</em>, volume 187 of <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">Proceedings of Machine Learning Research</em>,
pages 51â€“65. PMLR, 03 Dec 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v187/kaur23a.html" title="">https://proceedings.mlr.press/v187/kaur23a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kirkpatrick et al. (2017)</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
Raia Hadsell.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the National Academy of Sciences</em>, 114(13):3521â€“3526, 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1073/pnas.1611835114" title="">10.1073/pnas.1611835114</a>.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.pnas.org/doi/abs/10.1073/pnas.1611835114" title="">https://www.pnas.org/doi/abs/10.1073/pnas.1611835114</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewkowycz et al. (2020)</span>
<span class="ltx_bibblock">
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
Gur-Ari.

</span>
<span class="ltx_bibblock">The large learning rate phase of deep learning: the catapult
mechanism, 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2003.02218" title="">https://arxiv.org/abs/2003.02218</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2018)</span>
<span class="ltx_bibblock">
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Visualizing the loss landscape of neural nets.

</span>
<span class="ltx_bibblock">In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Advances in Neural Information Processing
Systems</em>, volume 31. Curran Associates, Inc., 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2025)</span>
<span class="ltx_bibblock">
Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre,
Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin,
Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin
Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna
Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej
Kilian, Hanlin Zhang, Rulin Shao, Sarah Pratt, Sunny Sanyal, Gabriel Ilharco,
Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Chandu,
Thao Nguyen, Igor Vasiljevic, Sham Kakade, Shuran Song, Sujay Sanghavi,
Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby,
Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca
Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis,
Yair Carmon, Achal Dave, Ludwig Schmidt, and Vaishaal Shankar.

</span>
<span class="ltx_bibblock">Datacomp-lm: In search of the next generation of training sets for
language models, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.11794" title="">https://arxiv.org/abs/2406.11794</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lopez-Paz and Ranzato (2017)</span>
<span class="ltx_bibblock">
David Lopez-Paz and Marc' Aurelio Ranzato.

</span>
<span class="ltx_bibblock">Gradient episodic memory for continual learning.

</span>
<span class="ltx_bibblock">In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in Neural
Information Processing Systems</em>, volume 30. Curran Associates, Inc., 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">International Conference on Learning Representations</em>, 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="">https://openreview.net/forum?id=Bkg6RiCqY7</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2025)</span>
<span class="ltx_bibblock">
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang.

</span>
<span class="ltx_bibblock">An empirical study of catastrophic forgetting in large language
models during continual fine-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">IEEE Transactions on Audio, Speech and Language Processing</em>,
33:3776â€“3786, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1109/TASLPRO.2025.3606231" title="">10.1109/TASLPRO.2025.3606231</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCloskey and Cohen (1989)</span>
<span class="ltx_bibblock">
Michael McCloskey and Neal J. Cohen.

</span>
<span class="ltx_bibblock">Catastrophic interference in connectionist networks: The sequential
learning problem.

</span>
<span class="ltx_bibblock">volume 24 of <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Psychology of Learning and Motivation</em>, pages
109â€“165. Academic Press, 1989.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/https://doi.org/10.1016/S0079-7421(08)60536-8" title="">https://doi.org/10.1016/S0079-7421(08)60536-8</a>.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0079742108605368" title="">https://www.sciencedirect.com/science/article/pii/S0079742108605368</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McLeish et al. (2025)</span>
<span class="ltx_bibblock">
Sean McLeish, Ang Li, John Kirchenbauer, Dayal Singh Kalra, Brian R.
Bartoldson, Bhavya Kailkhura, Avi Schwarzschild, Jonas Geiping, Tom
Goldstein, and Micah Goldblum.

</span>
<span class="ltx_bibblock">Teaching pretrained language models to think deeper with retrofitted
recurrence, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2511.07384" title="">https://arxiv.org/abs/2511.07384</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan and Li (2022)</span>
<span class="ltx_bibblock">
Yan Pan and Yuanzhi Li.

</span>
<span class="ltx_bibblock">Toward understanding why adam converges faster than SGD for
transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">OPT 2022: Optimization for Machine Learning (NeurIPS 2022
Workshop)</em>, 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Sf1NlV2r6PO" title="">https://openreview.net/forum?id=Sf1NlV2r6PO</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et al. (2024)</span>
<span class="ltx_bibblock">
Guilherme Penedo, Hynek KydlÃ­Äek, Loubna Ben allal, Anton Lozhkov,
Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf.

</span>
<span class="ltx_bibblock">The fineweb datasets: Decanting the web for the finest text data at
scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">The Thirty-eight Conference on Neural Information Processing
Systems Datasets and Benchmarks Track</em>, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=n6SCkn2QaG" title="">https://openreview.net/forum?id=n6SCkn2QaG</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robins (1995)</span>
<span class="ltx_bibblock">
Anthony Robins.

</span>
<span class="ltx_bibblock">Catastrophic forgetting, rehearsal and pseudorehearsal.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Connection Science</em>, 7(2):123â€“146, 1995.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.1080/09540099550039318" title="">10.1080/09540099550039318</a>.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1080/09540099550039318" title="">https://doi.org/10.1080/09540099550039318</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roulet et al. (2024)</span>
<span class="ltx_bibblock">
Vincent Roulet, Atish Agarwala, Jean-Bastien Grill, Grzegorz Michal Swirszcz,
Mathieu Blondel, and Fabian Pedregosa.

</span>
<span class="ltx_bibblock">Stepping on the edge: Curvature aware learning rate tuners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">The Thirty-eighth Annual Conference on Neural Information
Processing Systems</em>, 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=SEflLHIhhJ" title="">https://openreview.net/forum?id=SEflLHIhhJ</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusu et al. (2016)</span>
<span class="ltx_bibblock">
Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.

</span>
<span class="ltx_bibblock">Progressive neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:1606.04671</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scialom et al. (2022)</span>
<span class="ltx_bibblock">
Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan.

</span>
<span class="ltx_bibblock">Fine-tuned language models are continual learners.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors,
<em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing</em>, pages 6107â€“6122, Abu Dhabi, United Arab Emirates,
December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock"><a class="ltx_ref" href="https:/doi.org/10.18653/v1/2022.emnlp-main.410" title="">10.18653/v1/2022.emnlp-main.410</a>.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-main.410/" title="">https://aclanthology.org/2022.emnlp-main.410/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2019)</span>
<span class="ltx_bibblock">
Sharan Vaswani, Aaron Mishkin, Issam Laradji, Mark Schmidt, Gauthier Gidel, and
Simon Lacoste-Julien.

</span>
<span class="ltx_bibblock">Painless stochastic gradient: Interpolation, line-search, and
convergence rates.

</span>
<span class="ltx_bibblock">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in Neural
Information Processing Systems</em>, volume 32. Curran Associates, Inc., 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2019/file/2557911c1bf75c2b643afb4ecbfc8ec2-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2019/file/2557911c1bf75c2b643afb4ecbfc8ec2-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Walsh et al. (2025)</span>
<span class="ltx_bibblock">
Evan Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita
Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin
Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman,
Christopher Clark, Pradeep Dasigi, Nouha Dziri, Allyson Ettinger, Michal
Guerquin, David Heineman, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya
Malik, William Merrill, Lester James Validad Miranda, Jacob Morrison, Tyler
Murray, Crystal Nam, Jake Poznanski, Valentina Pyatkin, Aman Rangapur,
Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael
Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh
Hajishirzi.

</span>
<span class="ltx_bibblock">2 OLMo 2 furious (COLMâ€™s version).

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Second Conference on Language Modeling</em>, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=2ezugTT9kU" title="">https://openreview.net/forum?id=2ezugTT9kU</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wen et al. (2025)</span>
<span class="ltx_bibblock">
Kaiyue Wen, Zhiyuan Li, Jason S. Wang, David Leo Wright Hall, Percy Liang, and
Tengyu Ma.

</span>
<span class="ltx_bibblock">Understanding warmup-stable-decay learning rates: A river valley loss
landscape view.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">The Thirteenth International Conference on Learning
Representations</em>, 2025.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=m51BgoqvbP" title="">https://openreview.net/forum?id=m51BgoqvbP</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2018)</span>
<span class="ltx_bibblock">
Lei Wu, Chao Ma, and Weinan E.

</span>
<span class="ltx_bibblock">How sgd selects the global minima in over-parameterized learning: A
dynamical stability perspective.

</span>
<span class="ltx_bibblock">In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Advances in Neural Information Processing
Systems</em>, volume 31. Curran Associates, Inc., 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p3">
<span class="ltx_ERROR undefined" id="p3.1" lang="en">\beginappendix</span>
</div>
<section class="ltx_section" id="S7" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Estimating Critical Learning Rate Using Forward Passes</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.3">This section provides additional details on how to measure the critical learning rate using only forward passes. We generalize the line search method proposed by <cite class="ltx_cite ltx_citemacro_cite">Kalra and Barkeshli (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib22" title="">2024</a>)</cite> to accommodate a generic initial guess <math alttext="\eta_{0}" class="ltx_Math" display="inline" id="S7.p1.1.m1" intent=":literal"><semantics><msub><mi>Î·</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\eta_{0}</annotation></semantics></math> and modify the exit condition for the binary search to get a better estimate of the critical sharpness.
Given the update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="S7.p1.2.m2" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math> from training, we compute the critical learning rate <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S7.p1.3.m3" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math> using a two-phase line search procedure:</p>
</div>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Exponential Search:</h4>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p1.7">Starting from an initial guess <math alttext="\eta_{0}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p1.1.m1" intent=":literal"><semantics><msub><mi>Î·</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\eta_{0}</annotation></semantics></math>, we iteratively double (half) the learning rate <math alttext="\eta" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p1.2.m2" intent=":literal"><semantics><mi>Î·</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> until the loss <math alttext="L(\bm{\theta}-\eta\Delta\bm{\theta})" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p1.3.m3" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta\Delta\bm{\theta})</annotation></semantics></math> exceeds (or falls below) the current loss <math alttext="L(\bm{\theta})" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p1.4.m4" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta})</annotation></semantics></math>. This quickly identifies an interval <math alttext="[\eta_{\text{lower}},\eta_{\text{upper}}]" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p1.5.m5" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>,</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta_{\text{lower}},\eta_{\text{upper}}]</annotation></semantics></math> containing <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p1.6.m6" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math>, with <math alttext="\eta_{\text{upper}}=2\eta_{\text{lower}}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p1.7.m7" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo>=</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub></mrow></mrow><annotation encoding="application/x-tex">\eta_{\text{upper}}=2\eta_{\text{lower}}</annotation></semantics></math>. Notably, each iteration requires only a single forward pass to evaluate the loss. The full algorithm is provided in <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#alg1" title="In Exponential Search: â€£ 7 Estimating Critical Learning Rate Using Forward Passes â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Algorithm</span>Ëœ<span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p2.8">The number of exponential iterations depends on how close the initial guess <math alttext="\eta_{0}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p2.1.m1" intent=":literal"><semantics><msub><mi>Î·</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\eta_{0}</annotation></semantics></math> is to the true critical learning rate <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p2.2.m2" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math>. After the first iteration, we update the initial guess to our current estimate of <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p2.3.m3" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math>, i.e., <math alttext="\eta_{0}=\eta_{c}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p2.4.m4" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mn>0</mn></msub><mo>=</mo><msub><mi>Î·</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">\eta_{0}=\eta_{c}</annotation></semantics></math>. In the subsequent steps, only <math alttext="1-2" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p2.5.m5" intent=":literal"><semantics><mrow><mn>1</mn><mo>âˆ’</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">1-2</annotation></semantics></math> exponential steps are needed, since <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p2.6.m6" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math> tends to remain stable. However, if the training exhibits a large instability, causing the landscape to drastically change, more steps may be necessary.
To prevent indefinite iterations in such edge cases, we cap the exponential search to at most <math alttext="40" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p2.7.m7" intent=":literal"><semantics><mn>40</mn><annotation encoding="application/x-tex">40</annotation></semantics></math> iterations, which corresponds to a increasing or decreasing the learning rate by a factor of <math alttext="10^{12}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p2.8.m8" intent=":literal"><semantics><msup><mn>10</mn><mn>12</mn></msup><annotation encoding="application/x-tex">10^{12}</annotation></semantics></math> relative to the initial guess.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Exponential Search</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">1:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg1.l1.2">Input:</span> Update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="alg1.l1.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math>, Initial loss <math alttext="L(\bm{\theta})" class="ltx_Math" display="inline" id="alg1.l1.m2" intent=":literal"><semantics><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta})</annotation></semantics></math>, initial guess <math alttext="\eta_{0}" class="ltx_Math" display="inline" id="alg1.l1.m3" intent=":literal"><semantics><msub><mi>Î·</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\eta_{0}</annotation></semantics></math>, maximum iterations <math alttext="N" class="ltx_Math" display="inline" id="alg1.l1.m4" intent=":literal"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">2:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg1.l2.2">Output:</span> Interval <math alttext="[\eta_{\text{lower}},\eta_{\text{upper}}]" class="ltx_Math" display="inline" id="alg1.l2.m1" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>,</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta_{\text{lower}},\eta_{\text{upper}}]</annotation></semantics></math> containing the critical learning rate <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="alg1.l2.m2" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">3:</span></span>â€‚<math alttext="\eta\leftarrow\eta_{0}" class="ltx_Math" display="inline" id="alg1.l3.m1" intent=":literal"><semantics><mrow><mi>Î·</mi><mo stretchy="false">â†</mo><msub><mi>Î·</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\eta\leftarrow\eta_{0}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">4:</span></span>â€‚<math alttext="i\leftarrow 1" class="ltx_Math" display="inline" id="alg1.l4.m1" intent=":literal"><semantics><mrow><mi>i</mi><mo stretchy="false">â†</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">i\leftarrow 1</annotation></semantics></math> <span class="ltx_text" id="alg1.l4.2" style="float:right;">// Iteration
</span>
</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">5:</span></span>â€‚<math alttext="L(\bm{\theta}-\eta\Delta\bm{\theta})\leftarrow\text{ComputeLoss}(\eta,\Delta\bm{\theta})" class="ltx_Math" display="inline" id="alg1.l5.m1" intent=":literal"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">â†</mo><mrow><mtext>ComputeLoss</mtext><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>Î·</mi><mo>,</mo><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta\Delta\bm{\theta})\leftarrow\text{ComputeLoss}(\eta,\Delta\bm{\theta})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">6:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg1.l6.2">if</span> <math alttext="L(\bm{\theta}-\eta\Delta\bm{\theta})&lt;L(\bm{\theta})" class="ltx_Math" display="inline" id="alg1.l6.m1" intent=":literal"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta\Delta\bm{\theta})&lt;L(\bm{\theta})</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l6.3">then</span>
</div>
<div class="ltx_listingline" id="alg1.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l7.1.1.1" style="font-size:80%;">7:</span></span>â€ƒâ€‚<math alttext="\text{dir}\leftarrow+1" class="ltx_Math" display="inline" id="alg1.l7.m1" intent=":literal"><semantics><mrow><mtext>dir</mtext><mo stretchy="false">â†</mo><mrow><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\text{dir}\leftarrow+1</annotation></semantics></math> <span class="ltx_text" id="alg1.l7.2" style="float:right;">// Increase learning rate
</span>
</div>
<div class="ltx_listingline" id="alg1.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l8.1.1.1" style="font-size:80%;">8:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg1.l8.2">else</span>
</div>
<div class="ltx_listingline" id="alg1.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l9.1.1.1" style="font-size:80%;">9:</span></span>â€ƒâ€‚<math alttext="\text{dir}\leftarrow-1" class="ltx_Math" display="inline" id="alg1.l9.m1" intent=":literal"><semantics><mrow><mtext>dir</mtext><mo stretchy="false">â†</mo><mrow><mo>âˆ’</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\text{dir}\leftarrow-1</annotation></semantics></math> <span class="ltx_text" id="alg1.l9.2" style="float:right;">// Decrease learning rate
</span>
</div>
<div class="ltx_listingline" id="alg1.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l10.1.1.1" style="font-size:80%;">10:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg1.l10.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l10.3">if</span>
</div>
<div class="ltx_listingline" id="alg1.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l11.1.1.1" style="font-size:80%;">11:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg1.l11.2">while</span> <math alttext="i&lt;N_{\max}" class="ltx_Math" display="inline" id="alg1.l11.m1" intent=":literal"><semantics><mrow><mi>i</mi><mo>&lt;</mo><msub><mi>N</mi><mi>max</mi></msub></mrow><annotation encoding="application/x-tex">i&lt;N_{\max}</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l11.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l12.1.1.1" style="font-size:80%;">12:</span></span>â€ƒâ€‚<math alttext="\eta\leftarrow\eta\times 2^{\text{dir}}" class="ltx_Math" display="inline" id="alg1.l12.m1" intent=":literal"><semantics><mrow><mi>Î·</mi><mo stretchy="false">â†</mo><mrow><mi>Î·</mi><mo lspace="0.222em" rspace="0.222em">Ã—</mo><msup><mn>2</mn><mtext>dir</mtext></msup></mrow></mrow><annotation encoding="application/x-tex">\eta\leftarrow\eta\times 2^{\text{dir}}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l13.1.1.1" style="font-size:80%;">13:</span></span>â€ƒâ€‚<math alttext="L(\bm{\theta}-\eta\Delta\bm{\theta})\leftarrow\text{ComputeLoss}(\eta,\Delta\bm{\theta})" class="ltx_Math" display="inline" id="alg1.l13.m1" intent=":literal"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">â†</mo><mrow><mtext>ComputeLoss</mtext><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>Î·</mi><mo>,</mo><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta\Delta\bm{\theta})\leftarrow\text{ComputeLoss}(\eta,\Delta\bm{\theta})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l14.1.1.1" style="font-size:80%;">14:</span></span>â€ƒâ€‚<math alttext="i\leftarrow i+1" class="ltx_Math" display="inline" id="alg1.l14.m1" intent=":literal"><semantics><mrow><mi>i</mi><mo stretchy="false">â†</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">i\leftarrow i+1</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l15">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l15.1.1.1" style="font-size:80%;">15:</span></span>â€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg1.l15.2">if</span> <math alttext="\text{dir}=+1" class="ltx_Math" display="inline" id="alg1.l15.m1" intent=":literal"><semantics><mrow><mtext>dir</mtext><mo>=</mo><mrow><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\text{dir}=+1</annotation></semantics></math>  and <math alttext="L(\bm{\theta}-\eta\Delta\bm{\theta})&gt;L(\bm{\theta})" class="ltx_Math" display="inline" id="alg1.l15.m2" intent=":literal"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta\Delta\bm{\theta})&gt;L(\bm{\theta})</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l15.3">then</span>
</div>
<div class="ltx_listingline" id="alg1.l16">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l16.1.1.1" style="font-size:80%;">16:</span></span>â€ƒâ€ƒâ€‚<math alttext="\eta_{\text{lower}}\leftarrow\eta/2" class="ltx_Math" display="inline" id="alg1.l16.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo stretchy="false">â†</mo><mrow><mi>Î·</mi><mo>/</mo><mn>2</mn></mrow></mrow><annotation encoding="application/x-tex">\eta_{\text{lower}}\leftarrow\eta/2</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l17">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l17.1.1.1" style="font-size:80%;">17:</span></span>â€ƒâ€ƒâ€‚<math alttext="\eta_{\text{upper}}\leftarrow\eta" class="ltx_Math" display="inline" id="alg1.l17.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">â†</mo><mi>Î·</mi></mrow><annotation encoding="application/x-tex">\eta_{\text{upper}}\leftarrow\eta</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l18">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l18.1.1.1" style="font-size:80%;">18:</span></span>â€ƒâ€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg1.l18.2">return</span> <math alttext="[\eta_{\text{lower}},\eta_{\text{upper}}]" class="ltx_Math" display="inline" id="alg1.l18.m1" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>,</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta_{\text{lower}},\eta_{\text{upper}}]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l19">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l19.1.1.1" style="font-size:80%;">19:</span></span>â€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg1.l19.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l19.3">if</span>
</div>
<div class="ltx_listingline" id="alg1.l20">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l20.1.1.1" style="font-size:80%;">20:</span></span>â€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg1.l20.2">if</span> <math alttext="\text{dir}=-1" class="ltx_Math" display="inline" id="alg1.l20.m1" intent=":literal"><semantics><mrow><mtext>dir</mtext><mo>=</mo><mrow><mo>âˆ’</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">\text{dir}=-1</annotation></semantics></math>  and <math alttext="L(\bm{\theta}-\eta\Delta\bm{\theta})&lt;L(\bm{\theta})" class="ltx_Math" display="inline" id="alg1.l20.m2" intent=":literal"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>&lt;</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta\Delta\bm{\theta})&lt;L(\bm{\theta})</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l20.3">then</span>
</div>
<div class="ltx_listingline" id="alg1.l21">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l21.1.1.1" style="font-size:80%;">21:</span></span>â€ƒâ€ƒâ€‚<math alttext="\eta_{\text{lower}}\leftarrow\eta" class="ltx_Math" display="inline" id="alg1.l21.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo stretchy="false">â†</mo><mi>Î·</mi></mrow><annotation encoding="application/x-tex">\eta_{\text{lower}}\leftarrow\eta</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l22">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l22.1.1.1" style="font-size:80%;">22:</span></span>â€ƒâ€ƒâ€‚<math alttext="\eta_{\text{upper}}\leftarrow 2\eta" class="ltx_Math" display="inline" id="alg1.l22.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">â†</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î·</mi></mrow></mrow><annotation encoding="application/x-tex">\eta_{\text{upper}}\leftarrow 2\eta</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l23">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l23.1.1.1" style="font-size:80%;">23:</span></span>â€ƒâ€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg1.l23.2">return</span> <math alttext="[\eta_{\text{lower}},\eta_{\text{upper}}]" class="ltx_Math" display="inline" id="alg1.l23.m1" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>,</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta_{\text{lower}},\eta_{\text{upper}}]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l24">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l24.1.1.1" style="font-size:80%;">24:</span></span>â€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg1.l24.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l24.3">if</span>
</div>
<div class="ltx_listingline" id="alg1.l25">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l25.1.1.1" style="font-size:80%;">25:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg1.l25.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l25.3">while</span>
</div>
<div class="ltx_listingline" id="alg1.l26">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l26.1.1.1" style="font-size:80%;">26:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg1.l26.2">return</span> <math alttext="[\eta,\eta]" class="ltx_Math" display="inline" id="alg1.l26.m1" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><mi>Î·</mi><mo>,</mo><mi>Î·</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta,\eta]</annotation></semantics></math>
</div>
</div>
</figure>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p3.7"><span class="ltx_text ltx_font_bold" id="S7.SS0.SSS0.Px1.p3.7.1">Binary Search:</span> We then refine this interval using a binary search. At each iterate, we evaluate the loss at the midpoint <math alttext="\eta_{\text{mid}}=\frac{1}{2}(\eta_{\text{lower}}+\eta_{\text{upper}})" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p3.1.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>mid</mtext></msub><mo>=</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>+</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\eta_{\text{mid}}=\frac{1}{2}(\eta_{\text{lower}}+\eta_{\text{upper}})</annotation></semantics></math> and shorten the interval accordingly. This process is continued until the relative error <math alttext="|1-\eta_{\text{lower}}/\eta_{\text{upper}}|" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p3.2.m2" intent=":literal"><semantics><mrow><mo stretchy="false">|</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>/</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub></mrow></mrow><mo stretchy="false">|</mo></mrow><annotation encoding="application/x-tex">|1-\eta_{\text{lower}}/\eta_{\text{upper}}|</annotation></semantics></math> falls below a specified threshold <math alttext="\epsilon" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p3.3.m3" intent=":literal"><semantics><mi>Ïµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>. For <math alttext="\epsilon=\frac{1}{2^{k}}" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p3.4.m4" intent=":literal"><semantics><mrow><mi>Ïµ</mi><mo>=</mo><mfrac><mn>1</mn><msup><mn>2</mn><mi>k</mi></msup></mfrac></mrow><annotation encoding="application/x-tex">\epsilon=\frac{1}{2^{k}}</annotation></semantics></math>, the binary search converges in <math alttext="k" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p3.5.m5" intent=":literal"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> steps. In practice, we find that setting <math alttext="\epsilon=1/16" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p3.6.m6" intent=":literal"><semantics><mrow><mi>Ïµ</mi><mo>=</mo><mrow><mn>1</mn><mo>/</mo><mn>16</mn></mrow></mrow><annotation encoding="application/x-tex">\epsilon=1/16</annotation></semantics></math> (i.e., <math alttext="k=4" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p3.7.m7" intent=":literal"><semantics><mrow><mi>k</mi><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">k=4</annotation></semantics></math>) provides a reliable and efficient estimate of the critical learning rate. The full algorithm is detailed in <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#alg2" title="In Exponential Search: â€£ 7 Estimating Critical Learning Rate Using Forward Passes â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Algorithm</span>Ëœ<span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg2.2.1.1">Algorithm 2</span> </span> Binary Search</figcaption>
<div class="ltx_listing ltx_listing" id="alg2.3">
<div class="ltx_listingline" id="alg2.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l1.1.1.1" style="font-size:80%;">1:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg2.l1.2">Input:</span> Update direction <math alttext="\Delta\bm{\theta}" class="ltx_Math" display="inline" id="alg2.l1.m1" intent=":literal"><semantics><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}</annotation></semantics></math>, Initial loss <math alttext="L_{\text{before}}" class="ltx_Math" display="inline" id="alg2.l1.m2" intent=":literal"><semantics><msub><mi>L</mi><mtext>before</mtext></msub><annotation encoding="application/x-tex">L_{\text{before}}</annotation></semantics></math>, initial interval <math alttext="[\eta_{\text{lower}},\eta_{\text{upper}}]" class="ltx_Math" display="inline" id="alg2.l1.m3" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>,</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta_{\text{lower}},\eta_{\text{upper}}]</annotation></semantics></math>, tolerance <math alttext="\epsilon" class="ltx_Math" display="inline" id="alg2.l1.m4" intent=":literal"><semantics><mi>Ïµ</mi><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l2.1.1.1" style="font-size:80%;">2:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg2.l2.2">Output:</span> Interval <math alttext="[\eta_{\text{lower}},\eta_{\text{upper}}]" class="ltx_Math" display="inline" id="alg2.l2.m1" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>,</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta_{\text{lower}},\eta_{\text{upper}}]</annotation></semantics></math> containing the critical learning rate <math alttext="\eta_{c}" class="ltx_Math" display="inline" id="alg2.l2.m2" intent=":literal"><semantics><msub><mi>Î·</mi><mi>c</mi></msub><annotation encoding="application/x-tex">\eta_{c}</annotation></semantics></math> s.t. <math alttext="\left|1-\frac{\eta_{\text{lower}}}{\eta_{\text{upper}}}\right|&lt;\epsilon" class="ltx_Math" display="inline" id="alg2.l2.m3" intent=":literal"><semantics><mrow><mrow><mo>|</mo><mrow><mn>1</mn><mo>âˆ’</mo><mfrac><msub><mi>Î·</mi><mtext>lower</mtext></msub><msub><mi>Î·</mi><mtext>upper</mtext></msub></mfrac></mrow><mo>|</mo></mrow><mo>&lt;</mo><mi>Ïµ</mi></mrow><annotation encoding="application/x-tex">\left|1-\frac{\eta_{\text{lower}}}{\eta_{\text{upper}}}\right|&lt;\epsilon</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l3.1.1.1" style="font-size:80%;">3:</span></span>â€‚<math alttext="\text{i}\leftarrow 0" class="ltx_Math" display="inline" id="alg2.l3.m1" intent=":literal"><semantics><mrow><mtext>i</mtext><mo stretchy="false">â†</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\text{i}\leftarrow 0</annotation></semantics></math> <span class="ltx_text" id="alg2.l3.2" style="float:right;">// iteration
</span>
</div>
<div class="ltx_listingline" id="alg2.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l4.1.1.1" style="font-size:80%;">4:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg2.l4.2">while</span> <math alttext="\left|1-\eta_{\text{lower}}/\eta_{\text{upper}}\right|&gt;\epsilon" class="ltx_Math" display="inline" id="alg2.l4.m1" intent=":literal"><semantics><mrow><mrow><mo>|</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>/</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub></mrow></mrow><mo>|</mo></mrow><mo>&gt;</mo><mi>Ïµ</mi></mrow><annotation encoding="application/x-tex">\left|1-\eta_{\text{lower}}/\eta_{\text{upper}}\right|&gt;\epsilon</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg2.l4.3">do</span>
</div>
<div class="ltx_listingline" id="alg2.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l5.1.1.1" style="font-size:80%;">5:</span></span>â€ƒâ€‚<math alttext="\eta_{\text{mid}}\leftarrow\frac{1}{2}(\eta_{\text{lower}}+\eta_{\text{upper}})" class="ltx_Math" display="inline" id="alg2.l5.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>mid</mtext></msub><mo stretchy="false">â†</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>+</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\eta_{\text{mid}}\leftarrow\frac{1}{2}(\eta_{\text{lower}}+\eta_{\text{upper}})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l6.1.1.1" style="font-size:80%;">6:</span></span>â€ƒâ€‚<math alttext="L(\bm{\theta}-\eta_{\text{mid}}\Delta\bm{\theta})\leftarrow\text{ComputeLoss}(\eta_{\text{mid}},\Delta\bm{\theta})" class="ltx_Math" display="inline" id="alg2.l6.m1" intent=":literal"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><msub><mi>Î·</mi><mtext>mid</mtext></msub><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">â†</mo><mrow><mtext>ComputeLoss</mtext><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><msub><mi>Î·</mi><mtext>mid</mtext></msub><mo>,</mo><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta_{\text{mid}}\Delta\bm{\theta})\leftarrow\text{ComputeLoss}(\eta_{\text{mid}},\Delta\bm{\theta})</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l7.1.1.1" style="font-size:80%;">7:</span></span>â€ƒâ€‚<math alttext="i\leftarrow i+1" class="ltx_Math" display="inline" id="alg2.l7.m1" intent=":literal"><semantics><mrow><mi>i</mi><mo stretchy="false">â†</mo><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></mrow><annotation encoding="application/x-tex">i\leftarrow i+1</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l8">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l8.1.1.1" style="font-size:80%;">8:</span></span>â€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg2.l8.2">if</span> <math alttext="L(\bm{\theta}-\eta_{\text{mid}}\Delta\bm{\theta})&gt;L(\bm{\theta})" class="ltx_Math" display="inline" id="alg2.l8.m1" intent=":literal"><semantics><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>ğœ½</mi><mo>âˆ’</mo><mrow><msub><mi>Î·</mi><mtext>mid</mtext></msub><mo lspace="0em" rspace="0em">â€‹</mo><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow><mo>&gt;</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">L(\bm{\theta}-\eta_{\text{mid}}\Delta\bm{\theta})&gt;L(\bm{\theta})</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg2.l8.3">then</span>
</div>
<div class="ltx_listingline" id="alg2.l9">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l9.1.1.1" style="font-size:80%;">9:</span></span>â€ƒâ€ƒâ€‚<math alttext="\eta_{\text{upper}}\leftarrow\eta_{\text{mid}}" class="ltx_Math" display="inline" id="alg2.l9.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">â†</mo><msub><mi>Î·</mi><mtext>mid</mtext></msub></mrow><annotation encoding="application/x-tex">\eta_{\text{upper}}\leftarrow\eta_{\text{mid}}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l10">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l10.1.1.1" style="font-size:80%;">10:</span></span>â€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg2.l10.2">else</span>
</div>
<div class="ltx_listingline" id="alg2.l11">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l11.1.1.1" style="font-size:80%;">11:</span></span>â€ƒâ€ƒâ€‚<math alttext="\eta_{\text{lower}}\leftarrow\eta_{\text{mid}}" class="ltx_Math" display="inline" id="alg2.l11.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo stretchy="false">â†</mo><msub><mi>Î·</mi><mtext>mid</mtext></msub></mrow><annotation encoding="application/x-tex">\eta_{\text{lower}}\leftarrow\eta_{\text{mid}}</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l12">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l12.1.1.1" style="font-size:80%;">12:</span></span>â€ƒâ€‚<span class="ltx_text ltx_font_bold" id="alg2.l12.2">end</span> <span class="ltx_text ltx_font_bold" id="alg2.l12.3">if</span>
</div>
<div class="ltx_listingline" id="alg2.l13">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l13.1.1.1" style="font-size:80%;">13:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg2.l13.2">end</span> <span class="ltx_text ltx_font_bold" id="alg2.l13.3">while</span>
</div>
<div class="ltx_listingline" id="alg2.l14">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l14.1.1.1" style="font-size:80%;">14:</span></span>â€‚<span class="ltx_text ltx_font_bold" id="alg2.l14.2">return</span> <math alttext="[\eta_{\text{lower}},\eta_{\text{upper}}]" class="ltx_Math" display="inline" id="alg2.l14.m1" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>,</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\eta_{\text{lower}},\eta_{\text{upper}}]</annotation></semantics></math>
</div>
</div>
</figure>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p4">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p4.2">Finally, we approximate the critical learning rate using the mean of the final range <math alttext="\eta_{c}\approx\frac{1}{2}(\eta_{\text{lower}}+\eta_{\text{upper}})" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p4.1.m1" intent=":literal"><semantics><mrow><msub><mi>Î·</mi><mi>c</mi></msub><mo>â‰ˆ</mo><mrow><mfrac><mn>1</mn><mn>2</mn></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><msub><mi>Î·</mi><mtext>lower</mtext></msub><mo>+</mo><msub><mi>Î·</mi><mtext>upper</mtext></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\eta_{c}\approx\frac{1}{2}(\eta_{\text{lower}}+\eta_{\text{upper}})</annotation></semantics></math>. Overall, this procedure reliably computes the critical learning rate in <math alttext="5-6" class="ltx_Math" display="inline" id="S7.SS0.SSS0.Px1.p4.2.m2" intent=":literal"><semantics><mrow><mn>5</mn><mo>âˆ’</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">5-6</annotation></semantics></math> forward passes.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Experimental Details</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.7"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2601.16979v1#S1.F1" title="In 1 Introduction â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text ltx_font_bold" id="S8.p1.7.1">:</span> We considered GPT-style Pre-LN Transformers consisting of <math alttext="12" class="ltx_Math" display="inline" id="S8.p1.1.m1" intent=":literal"><semantics><mn>12</mn><annotation encoding="application/x-tex">12</annotation></semantics></math> layers, with an embedding dimension of <math alttext="n_{\text{embd}}=768" class="ltx_Math" display="inline" id="S8.p1.2.m2" intent=":literal"><semantics><mrow><msub><mi>n</mi><mtext>embd</mtext></msub><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">n_{\text{embd}}=768</annotation></semantics></math>. The model in total has <math alttext="\sim 100" class="ltx_Math" display="inline" id="S8.p1.3.m3" intent=":literal"><semantics><mrow><mi></mi><mo>âˆ¼</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">\sim 100</annotation></semantics></math>M parameters. We train the model on the <math alttext="10" class="ltx_Math" display="inline" id="S8.p1.4.m4" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math>B token subset of FineWebEdu dataset with <math alttext="\sim 1" class="ltx_Math" display="inline" id="S8.p1.5.m5" intent=":literal"><semantics><mrow><mi></mi><mo>âˆ¼</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sim 1</annotation></semantics></math>M tokens per step using AdamW with constant learning rate and hyperparameters <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S8.p1.6.m6" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\beta_{1}=0.9</annotation></semantics></math> and <math alttext="\beta_{2}=0.95" class="ltx_Math" display="inline" id="S8.p1.7.m7" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding="application/x-tex">\beta_{2}=0.95</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.4"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2601.16979v1#S2.F3" title="In 2.3 The Relationship between Critical Sharpness and Hessian Sharpness â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">3</span></a>: We considered Fully Connected Networks (FCNs) consisting of four layers, with width <math alttext="512" class="ltx_Math" display="inline" id="S8.p2.1.m1" intent=":literal"><semantics><mn>512</mn><annotation encoding="application/x-tex">512</annotation></semantics></math> and GeLU activation function. The models were trained on the CIFAR-10 image classification task using SGD with a constant learning rate of <math alttext="\eta=" class="ltx_Math" display="inline" id="S8.p2.2.m2" intent=":literal"><semantics><mrow><mi>Î·</mi><mo>=</mo><mi></mi></mrow><annotation encoding="application/x-tex">\eta=</annotation></semantics></math>3e-02. We experimented with three different batch sizes: <math alttext="B\in[500,5000,50000]" class="ltx_Math" display="inline" id="S8.p2.3.m3" intent=":literal"><semantics><mrow><mi>B</mi><mo>âˆˆ</mo><mrow><mo stretchy="false">[</mo><mn>500</mn><mo>,</mo><mn>5000</mn><mo>,</mo><mn>50000</mn><mo stretchy="false">]</mo></mrow></mrow><annotation encoding="application/x-tex">B\in[500,5000,50000]</annotation></semantics></math>, with a batch size of <math alttext="50,000" class="ltx_Math" display="inline" id="S8.p2.4.m4" intent=":literal"><semantics><mrow><mn>50</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">50,000</annotation></semantics></math> corresponds to full-batch gradient descent.</p>
</div>
<div class="ltx_para" id="S8.p3">
<p class="ltx_p" id="S8.p3.7"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2601.16979v1#S3.F4" title="In 3 Critical Sharpness Dynamics at Scale â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">4</span></a>: We considered GPT-style Pre-LN Transformers consisting of <math alttext="12" class="ltx_Math" display="inline" id="S8.p3.1.m1" intent=":literal"><semantics><mn>12</mn><annotation encoding="application/x-tex">12</annotation></semantics></math> layers, with an embedding dimension of <math alttext="n_{\text{embd}}=768" class="ltx_Math" display="inline" id="S8.p3.2.m2" intent=":literal"><semantics><mrow><msub><mi>n</mi><mtext>embd</mtext></msub><mo>=</mo><mn>768</mn></mrow><annotation encoding="application/x-tex">n_{\text{embd}}=768</annotation></semantics></math>. The model in total has <math alttext="\sim 100" class="ltx_Math" display="inline" id="S8.p3.3.m3" intent=":literal"><semantics><mrow><mi></mi><mo>âˆ¼</mo><mn>100</mn></mrow><annotation encoding="application/x-tex">\sim 100</annotation></semantics></math>M parameters. We train the model on the <math alttext="10" class="ltx_Math" display="inline" id="S8.p3.4.m4" intent=":literal"><semantics><mn>10</mn><annotation encoding="application/x-tex">10</annotation></semantics></math>B token subset of FineWebEdu dataset with <math alttext="\sim 1" class="ltx_Math" display="inline" id="S8.p3.5.m5" intent=":literal"><semantics><mrow><mi></mi><mo>âˆ¼</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\sim 1</annotation></semantics></math>M tokens per step using AdamW and hyperparameters <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S8.p3.6.m6" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\beta_{1}=0.9</annotation></semantics></math> and <math alttext="\beta_{2}=0.95" class="ltx_Math" display="inline" id="S8.p3.7.m7" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mn>2</mn></msub><mo>=</mo><mn>0.95</mn></mrow><annotation encoding="application/x-tex">\beta_{2}=0.95</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S8.p4">
<p class="ltx_p" id="S8.p4.5"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2601.16979v1#S3.F5" title="In 3 Critical Sharpness Dynamics at Scale â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text ltx_font_bold" id="S8.p4.5.1">:</span> We evaluate the publicly available OLMo-2 7B checkpoint at both the pre-training and mid-training stages. For each checkpoint, we estimate the critical learning rate using the AdamW optimizer with a batch size of <math alttext="16" class="ltx_Math" display="inline" id="S8.p4.1.m1" intent=":literal"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation></semantics></math> and hyperparameters <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S8.p4.2.m2" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\beta_{1}=0.9</annotation></semantics></math> and <math alttext="\beta_{2}=0.99" class="ltx_Math" display="inline" id="S8.p4.3.m3" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow><annotation encoding="application/x-tex">\beta_{2}=0.99</annotation></semantics></math>. Since we do not have access to OLMoâ€™s original optimizer states, we first accumulate Adamâ€™s first and second moments over <math alttext="100" class="ltx_Math" display="inline" id="S8.p4.4.m4" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> steps. After this warm-up period, we measure the critical learning rate for the next <math alttext="100" class="ltx_Math" display="inline" id="S8.p4.5.m5" intent=":literal"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math> steps and report the average value across these steps. For the pre-training checkpoints, we evaluate the model using the DCLM dataset, whereas the mid-training checkpoints are evaluated using the Dolmino mix dataset <cite class="ltx_cite ltx_citemacro_citep">(Walsh et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib41" title="">2025</a>)</cite>.
Importantly, model parameters are not updated during this experiment.</p>
</div>
<div class="ltx_para" id="S8.p5">
<p class="ltx_p" id="S8.p5.6"><a class="ltx_ref ltx_font_bold" href="https://arxiv.org/html/2601.16979v1#S4.F6" title="In 4 How much Pre-training data is needed to avoid Catastrophic forgetting? â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">6</span></a><span class="ltx_text ltx_font_bold" id="S8.p5.6.1">:</span> We consider the last OLMo-2 <math alttext="7" class="ltx_Math" display="inline" id="S8.p5.1.m1" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math>B as our starting point and further train on a mixture composed of DCLM and and the math subset of Dolmino mix <cite class="ltx_cite ltx_citemacro_citep">(Walsh et al., <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib41" title="">2025</a>)</cite>. In <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S4.F6" title="In 4 How much Pre-training data is needed to avoid Catastrophic forgetting? â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">6</span></a>(a), we evaluate the relative critical sharpness for several evaluation tasks across DCLM ratios. (b, c) We further train the pre-trained checkpoint on the train mix for <math alttext="16,000" class="ltx_Math" display="inline" id="S8.p5.2.m2" intent=":literal"><semantics><mrow><mn>16</mn><mo>,</mo><mn>000</mn></mrow><annotation encoding="application/x-tex">16,000</annotation></semantics></math> steps (<math alttext="1" class="ltx_Math" display="inline" id="S8.p5.3.m3" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>B tokens) using AdamW with a constant learning rate <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We consider a constant learning rate because we are only training the model for <math alttext="1" class="ltx_Math" display="inline" id="footnote4.m1" intent=":literal"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>B tokens, as compared to the OLMo mid-training for <math alttext="50" class="ltx_Math" display="inline" id="footnote4.m2" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math>B tokens.</span></span></span>, batch size of <math alttext="16" class="ltx_Math" display="inline" id="S8.p5.4.m4" intent=":literal"><semantics><mn>16</mn><annotation encoding="application/x-tex">16</annotation></semantics></math> and hyperparameters <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="S8.p5.5.m5" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn></mrow><annotation encoding="application/x-tex">\beta_{1}=0.9</annotation></semantics></math> and <math alttext="\beta_{2}=0.99" class="ltx_Math" display="inline" id="S8.p5.6.m6" intent=":literal"><semantics><mrow><msub><mi>Î²</mi><mn>2</mn></msub><mo>=</mo><mn>0.99</mn></mrow><annotation encoding="application/x-tex">\beta_{2}=0.99</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S8.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S8.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S8.F7.sf1.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S8.F7.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S8.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S8.F7.sf2.g1" src="x14.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S8.F7.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S8.F7.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S8.F7.sf3.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S8.F7.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S8.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S8.F7.3.2" style="font-size:90%;">Dynamics of pre-conditioned, directional, and critical sharpness during GPT-style Transformer training on FineWebEdu with AdamW. Critical sharpness tracks pre-conditioned sharpness throughout training, making it an effective proxy.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S9" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Additional Results</h2>
<section class="ltx_subsection" id="S9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>GPT Pre-training</h3>
<div class="ltx_para" id="S9.SS1.p1">
<p class="ltx_p" id="S9.SS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S8.F7" title="In 8 Experimental Details â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">7</span></a> presents the same results as <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S3.F4" title="In 3 Critical Sharpness Dynamics at Scale â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">4</span></a>, but directly compares different sharpness values at a fixed learning rate.</p>
</div>
<figure class="ltx_figure" id="S9.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S9.F8.sf1.g1" src="x16.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S9.F8.sf2.g1" src="x17.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F8.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="565" id="S9.F8.sf3.g1" src="x18.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F8.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S9.F8.3.2" style="font-size:90%;">Dynamics of pre-conditioned, directional, and critical sharpness during GPT-style Transformer training on FineWebEdu using AdamW with a constant learning rate.</span></figcaption>
</figure>
<div class="ltx_para" id="S9.SS1.p2">
<p class="ltx_p" id="S9.SS1.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.F8" title="In 9.1 GPT Pre-training â€£ 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">8</span></a> compares the training trajectories of pre-conditioned sharpness, directional sharpness and critical sharpness for GPT pre-training using AdamW with a fixed learning rate schedule. The pre-conditioned sharpness exhibits continually increases (progressive sharpening) until it reaches the stability threshold and oscillates around it (Edge of Stability). Both directional and critical sharpness also exhibit progressive sharpening and EoS behavior, while oscillating below the EoS threshold. The oscillation below the threshold is expected from <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S2.Thmresult1" title="Result 2.1 (Relationship between Directional and Hessian Sharpness for Gradient Descent). â€£ 2.3 The Relationship between Critical Sharpness and Hessian Sharpness â€£ 2 Characterizing Critical Sharpness: Theory and Empirics â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Result</span>Ëœ<span class="ltx_text ltx_ref_tag">2.1</span></a>, as critical sharpness includes contributions from other eigendirections as well.</p>
</div>
<figure class="ltx_figure" id="S9.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="581" id="S9.F9.sf1.g1" src="x19.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F9.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="581" id="S9.F9.sf2.g1" src="x20.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F9.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="S9.F9.3.2" style="font-size:90%;">GSM8K and MMLU accuracy as a function of pre-training (DCLM) mix ratio and learning rate. Red indicates a decrease in performance relative to the checkpoint, white indicates no change, and blue indicates an improvement.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S9.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>OLMo checkpoint analysis</h3>
<div class="ltx_para" id="S9.SS2.p1">
<p class="ltx_p" id="S9.SS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.F10" title="In 9.2 OLMo checkpoint analysis â€£ 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">10</span></a> shows the downstream performance of OLMo-2 7B checkpoints during both pre-training (left column) and mid-training (right column). We observe that MMLU shows consistent improvement throughout both phases, indicating steady improvement. GSM8K also improves during pre-training, but exhibits a dramatic improvement at a single checkpoint (<math alttext="5" class="ltx_Math" display="inline" id="S9.SS2.p1.1.m1" intent=":literal"><semantics><mn>5</mn><annotation encoding="application/x-tex">5</annotation></semantics></math>B tokens) during mid-training, where its accuracy jumps from 25% to 58%. As GSM8K exhibits such a quick improvement within a short training window, we specifically consider it as the candidate for further improvement during mid-training. This allows us to observe significant improvements without requiring extensive training.
In contrast, Hellaswag and OpenBookQA exhibit rapid early improvements during pre-training, but do not show appreciable gains during mid-training.</p>
</div>
<figure class="ltx_figure" id="S9.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F10.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S9.F10.sf1.g1" src="x21.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F10.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S9.F10.sf2.g1" src="x22.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F10.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S9.F10.sf3.g1" src="x23.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F10.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S9.F10.sf4.g1" src="x24.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.sf4.2.1.1" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F10.sf5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S9.F10.sf5.g1" src="x25.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.sf5.2.1.1" style="font-size:90%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F10.sf6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S9.F10.sf6.g1" src="x26.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.sf6.2.1.1" style="font-size:90%;">(f)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F10.sf7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S9.F10.sf7.g1" src="x27.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.sf7.2.1.1" style="font-size:90%;">(g)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S9.F10.sf8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="484" id="S9.F10.sf8.g1" src="x28.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.sf8.2.1.1" style="font-size:90%;">(h)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F10.4.2.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="S9.F10.2.1" style="font-size:90%;">Downstream Performance of OLMo-2 <math alttext="7" class="ltx_Math" display="inline" id="S9.F10.2.1.m1" intent=":literal"><semantics><mn>7</mn><annotation encoding="application/x-tex">7</annotation></semantics></math>B checkpoints during pre-training (left column) and mid-training (right column).</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S9.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.3 </span>OLMo mid-training on a mixture consisting of DCLM and Math</h3>
<div class="ltx_para" id="S9.SS3.p1">
<p class="ltx_p" id="S9.SS3.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.F9" title="In 9.1 GPT Pre-training â€£ 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">9</span></a> shows the same results as <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S4.F6" title="In 4 How much Pre-training data is needed to avoid Catastrophic forgetting? â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">6</span></a>(b, c), but shows them as a heatmap with accuracy values.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4 </span>OLMo mid-training</h3>
<figure class="ltx_table" id="S9.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S9.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S9.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S9.T1.2.1.1.1" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_italic" id="S9.T1.2.1.1.1.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S9.T1.2.1.1.2" style="padding-top:2pt;padding-bottom:2pt;"><span class="ltx_text ltx_font_italic" id="S9.T1.2.1.1.2.1">Tokens (B)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S9.T1.2.1.1.3" style="padding-top:2pt;padding-bottom:2pt;">Source %</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S9.T1.2.1.1.4" style="padding-top:2pt;padding-bottom:2pt;">Mix %</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S9.T1.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S9.T1.2.2.1.1" style="padding-top:2pt;padding-bottom:2pt;">Filtered DCLM</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S9.T1.2.2.1.2" style="padding-top:2pt;padding-bottom:2pt;">752</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.2.2.1.3" style="padding-top:2pt;padding-bottom:2pt;">3.23</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S9.T1.2.2.1.4" style="padding-top:2pt;padding-bottom:2pt;">47.2</td>
</tr>
<tr class="ltx_tr" id="S9.T1.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S9.T1.2.3.2.1" style="padding-top:2pt;padding-bottom:2pt;">Decontam. FLAN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S9.T1.2.3.2.2" style="padding-top:2pt;padding-bottom:2pt;">17.0</th>
<td class="ltx_td ltx_align_center" id="S9.T1.2.3.2.3" style="padding-top:2pt;padding-bottom:2pt;">50.0</td>
<td class="ltx_td ltx_align_center" id="S9.T1.2.3.2.4" style="padding-top:2pt;padding-bottom:2pt;">16.6</td>
</tr>
<tr class="ltx_tr" id="S9.T1.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S9.T1.2.4.3.1" style="padding-top:2pt;padding-bottom:2pt;">StackExchange Q&amp;A</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S9.T1.2.4.3.2" style="padding-top:2pt;padding-bottom:2pt;">1.26</th>
<td class="ltx_td ltx_align_center" id="S9.T1.2.4.3.3" style="padding-top:2pt;padding-bottom:2pt;">100</td>
<td class="ltx_td ltx_align_center" id="S9.T1.2.4.3.4" style="padding-top:2pt;padding-bottom:2pt;">2.45</td>
</tr>
<tr class="ltx_tr" id="S9.T1.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S9.T1.2.5.4.1" style="padding-top:2pt;padding-bottom:2pt;">peS2o</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S9.T1.2.5.4.2" style="padding-top:2pt;padding-bottom:2pt;">58.6</th>
<td class="ltx_td ltx_align_center" id="S9.T1.2.5.4.3" style="padding-top:2pt;padding-bottom:2pt;">5.15</td>
<td class="ltx_td ltx_align_center" id="S9.T1.2.5.4.4" style="padding-top:2pt;padding-bottom:2pt;">5.85</td>
</tr>
<tr class="ltx_tr" id="S9.T1.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S9.T1.2.6.5.1" style="padding-top:2pt;padding-bottom:2pt;">Wikipedia/Wikibooks</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S9.T1.2.6.5.2" style="padding-top:2pt;padding-bottom:2pt;">3.7</th>
<td class="ltx_td ltx_align_center" id="S9.T1.2.6.5.3" style="padding-top:2pt;padding-bottom:2pt;">100</td>
<td class="ltx_td ltx_align_center" id="S9.T1.2.6.5.4" style="padding-top:2pt;padding-bottom:2pt;">7.11</td>
</tr>
<tr class="ltx_tr" id="S9.T1.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S9.T1.2.7.6.1" style="padding-top:2pt;padding-bottom:2pt;">Dolmino Math</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S9.T1.2.7.6.2" style="padding-top:2pt;padding-bottom:2pt;">10.7</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S9.T1.2.7.6.3" style="padding-top:2pt;padding-bottom:2pt;">100</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S9.T1.2.7.6.4" style="padding-top:2pt;padding-bottom:2pt;">20.8</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S9.T1.3.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S9.T1.4.2" style="font-size:90%;">Summary of Dolmino mix dataset used for mid-training OLMo-2 models.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S9.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="232" id="S9.F11.g1" src="x29.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S9.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="S9.F11.3.2" style="font-size:90%;">Relative critical sharpness for various subsets of the Dolmino mix dataset. The shaded region around the mean trends denotes the variation across batches. </span></figcaption>
</figure>
<div class="ltx_para" id="S9.SS4.p1">
<p class="ltx_p" id="S9.SS4.p1.4">Next, we consider OLMo mid-training with the full Dolmino mix dataset. <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.T1" title="In 9.4 OLMo mid-training â€£ 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Table</span>Ëœ<span class="ltx_text ltx_ref_tag">1</span></a> summarizes the composition of the <math alttext="50" class="ltx_Math" display="inline" id="S9.SS4.p1.1.m1" intent=":literal"><semantics><mn>50</mn><annotation encoding="application/x-tex">50</annotation></semantics></math>B token Dolmino mix subset. To investigate the effect of the pre-training dataset, we keep the relative mix percentages of all components fixed except for the DCLM ratio, which we vary.
<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S9.F11" title="In 9.4 OLMo mid-training â€£ 9 Additional Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Figure</span>Ëœ<span class="ltx_text ltx_ref_tag">11</span></a> presents the relative critical sharpness for different components of the Dolmino mix dataset. Consistent with our findings in <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S4" title="4 How much Pre-training data is needed to avoid Catastrophic forgetting? â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">Section</span>Ëœ<span class="ltx_text ltx_ref_tag">4</span></a>, we observe a sweet spot at a DCLM ratio of
<math alttext="0.6" class="ltx_Math" display="inline" id="S9.SS4.p1.2.m2" intent=":literal"><semantics><mn>0.6</mn><annotation encoding="application/x-tex">0.6</annotation></semantics></math>. At this ratio, the relative critical sharpness of DCLM decreases, and the sharpness curves for different tasks intersect. This intersection represents an optimal point that allows for the largest possible learning rate without being limited by any single task. Based on this analysis, we predict that using a DCLM ratio of <math alttext="0.6" class="ltx_Math" display="inline" id="S9.SS4.p1.3.m3" intent=":literal"><semantics><mn>0.6</mn><annotation encoding="application/x-tex">0.6</annotation></semantics></math>, which is close to the <math alttext="0.47" class="ltx_Math" display="inline" id="S9.SS4.p1.4.m4" intent=":literal"><semantics><mn>0.47</mn><annotation encoding="application/x-tex">0.47</annotation></semantics></math> DCLM ratio used in the original paper.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S10" lang="en">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span>Theoretical Results</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">In this section, we provide the proofs for the theoretical results presented in the main text.</p>
</div>
<section class="ltx_subsection" id="S10.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1 </span>The Relationship between Directional and Hessian sharpness</h3>
<div class="ltx_theorem ltx_theorem_result" id="S10.Thmresult1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S10.Thmresult1.1.1.1">Result 10.1</span></span><span class="ltx_text ltx_font_bold" id="S10.Thmresult1.2.2"> </span>(Relationship between Directional and Hessian Sharpness for Gradient Descent)<span class="ltx_text ltx_font_bold" id="S10.Thmresult1.3.3">.</span>
</h6>
<div class="ltx_para" id="S10.Thmresult1.p1">
<p class="ltx_p" id="S10.Thmresult1.p1.3">For Gradient Descent (GD), the directional sharpness <math alttext="\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S10.Thmresult1.p1.1.m1" intent=":literal"><semantics><msub><mi>Î»</mi><mtext>dir</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{dir}}</annotation></semantics></math> can be expressed as a weighted sum of the Hessian eigenvalues <math alttext="\{\lambda_{i}^{H}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S10.Thmresult1.p1.2.m2" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>Î»</mi><mi>i</mi><mi>H</mi></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{\lambda_{i}^{H}\}_{i=1}^{n}</annotation></semantics></math>, where the weights quantify the alignment of the gradient with Hessian eigendirections <math alttext="\{u_{i}(\bm{\theta})\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S10.Thmresult1.p1.3.m3" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mrow><msub><mi>u</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{u_{i}(\bm{\theta})\}_{i=1}^{n}</annotation></semantics></math>.</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx8">
<tbody id="S10.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{H}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda_{\max}^{H}" class="ltx_Math" display="inline" id="S10.Ex3.m1" intent=":literal"><semantics><mrow><msub><mi>Î»</mi><mtext>dir</mtext></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>i</mi><mi>H</mi></msubsup></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>â‰¤</mo><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{H}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda_{\max}^{H}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult1.p1.5">where <math alttext="c_{i}=g(\bm{\theta})^{T}u_{i}(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult1.p1.4.m1" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>u</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">c_{i}=g(\bm{\theta})^{T}u_{i}(\bm{\theta})</annotation></semantics></math> is the projection of the gradient onto the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S10.Thmresult1.p1.5.m2" intent=":literal"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> eigenvector.</p>
</div>
<div class="ltx_para" id="S10.Thmresult1.p2">
<p class="ltx_p" id="S10.Thmresult1.p2.1"><span class="ltx_text ltx_font_italic" id="S10.Thmresult1.p2.1.1">Proof:</span> For gradient descent <math alttext="\Delta\bm{\theta}=g(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult1.p2.1.m1" intent=":literal"><semantics><mrow><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mo>=</mo><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}=g(\bm{\theta})</annotation></semantics></math>, the directional sharpness is given by:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx9">
<tbody id="S10.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\text{dir}}=\frac{g(\bm{\theta})^{T}H(\bm{\theta})g(\bm{\theta})}{g(\bm{\theta})^{T}g(\bm{\theta})}." class="ltx_Math" display="inline" id="S10.Ex4.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Î»</mi><mtext>dir</mtext></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\text{dir}}=\frac{g(\bm{\theta})^{T}H(\bm{\theta})g(\bm{\theta})}{g(\bm{\theta})^{T}g(\bm{\theta})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult1.p2.2">Next, writing <math alttext="g(\bm{\theta})=\sum_{i=1}^{n}c_{i}u_{i}(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult1.p2.2.m1" intent=":literal"><semantics><mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msub><mi>c</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>u</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">g(\bm{\theta})=\sum_{i=1}^{n}c_{i}u_{i}(\bm{\theta})</annotation></semantics></math>, we have the required result:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx10">
<tbody id="S10.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{H}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda^{H}_{\max}" class="ltx_Math" display="inline" id="S10.Ex5.m1" intent=":literal"><semantics><mrow><msub><mi>Î»</mi><mtext>dir</mtext></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>i</mi><mi>H</mi></msubsup></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>â‰¤</mo><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{H}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda^{H}_{\max}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_theorem ltx_theorem_result" id="S10.Thmresult2">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S10.Thmresult2.1.1.1">Result 10.2</span></span><span class="ltx_text ltx_font_bold" id="S10.Thmresult2.2.2"> </span>(Relationship between Directional and Hessian Sharpness for Adaptive optimizers)<span class="ltx_text ltx_font_bold" id="S10.Thmresult2.3.3">.</span>
</h6>
<div class="ltx_para" id="S10.Thmresult2.p1">
<p class="ltx_p" id="S10.Thmresult2.p1.5">For adaptive optimizers (such as Adam) with pre-conditioner <math alttext="P(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.1.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P(\bm{\theta})</annotation></semantics></math>, the directional sharpness <math alttext="\lambda_{\text{dir}}" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.2.m2" intent=":literal"><semantics><msub><mi>Î»</mi><mtext>dir</mtext></msub><annotation encoding="application/x-tex">\lambda_{\text{dir}}</annotation></semantics></math> can be expressed as a weighted sum of the pre-conditioned Hessian eigenvalues <math alttext="\{\lambda^{PH}_{i}\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.3.m3" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msubsup><mi>Î»</mi><mi>i</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{\lambda^{PH}_{i}\}_{i=1}^{n}</annotation></semantics></math>, where the weights quantify the alignment of the pre-conditioned gradient <math alttext="P^{-1/2}g(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.4.m4" intent=":literal"><semantics><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P^{-1/2}g(\bm{\theta})</annotation></semantics></math> with pre-conditioned Hessian eigendirections <math alttext="\{v_{i}(\bm{\theta})\}_{i=1}^{n}" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.5.m5" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo stretchy="false">}</mo></mrow><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><annotation encoding="application/x-tex">\{v_{i}(\bm{\theta})\}_{i=1}^{n}</annotation></semantics></math>.</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx11">
<tbody id="S10.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{PH}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda_{\max}^{PH}" class="ltx_Math" display="inline" id="S10.Ex6.m1" intent=":literal"><semantics><mrow><msub><mi>Î»</mi><mtext>dir</mtext></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>i</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>c</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>â‰¤</mo><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\text{dir}}=\frac{\sum_{i=1}^{n}c_{i}^{2}\lambda_{i}^{PH}}{\sum_{i=1}^{n}c_{i}^{2}}\leq\lambda_{\max}^{PH}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult2.p1.9">where <math alttext="c_{i}=P^{-1/2}g(\bm{\theta})^{T}v_{i}(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.6.m1" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>v</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">c_{i}=P^{-1/2}g(\bm{\theta})^{T}v_{i}(\bm{\theta})</annotation></semantics></math> is the projection of the gradient <math alttext="g(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.7.m2" intent=":literal"><semantics><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">g(\bm{\theta})</annotation></semantics></math> onto the <math alttext="i^{th}" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.8.m3" intent=":literal"><semantics><msup><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>h</mi></mrow></msup><annotation encoding="application/x-tex">i^{th}</annotation></semantics></math> eigenvector <math alttext="v_{i}(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult2.p1.9.m4" intent=":literal"><semantics><mrow><msub><mi>v</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">v_{i}(\bm{\theta})</annotation></semantics></math> of the pre-conditioned Hessian.</p>
</div>
<div class="ltx_para" id="S10.Thmresult2.p2">
<p class="ltx_p" id="S10.Thmresult2.p2.1"><span class="ltx_text ltx_font_italic" id="S10.Thmresult2.p2.1.1">Proof</span>. For adaptive optimizers <math alttext="\Delta\bm{\theta}=P^{-1}g(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult2.p2.1.m1" intent=":literal"><semantics><mrow><mrow><mi mathvariant="normal">Î”</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mo>=</mo><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\Delta\bm{\theta}=P^{-1}g(\bm{\theta})</annotation></semantics></math>, the directional sharpness is given by:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx12">
<tbody id="S10.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\text{dir}}=\frac{g(\bm{\theta})^{T}P^{-1}H(\bm{\theta})P^{-1}g(\bm{\theta})}{g(\bm{\theta})^{T}P^{-1}g(\bm{\theta})}." class="ltx_Math" display="inline" id="S10.Ex7.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Î»</mi><mtext>dir</mtext></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mrow><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\text{dir}}=\frac{g(\bm{\theta})^{T}P^{-1}H(\bm{\theta})P^{-1}g(\bm{\theta})}{g(\bm{\theta})^{T}P^{-1}g(\bm{\theta})}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult2.p2.5">Next, define <math alttext="\phi:=P^{1/2}\bm{\theta}" class="ltx_Math" display="inline" id="S10.Thmresult2.p2.2.m1" intent=":literal"><semantics><mrow><mi>Ï•</mi><mo>:=</mo><mrow><msup><mi>P</mi><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow></mrow><annotation encoding="application/x-tex">\phi:=P^{1/2}\bm{\theta}</annotation></semantics></math>, then the pre-conditioned gradient and Hessian are <math alttext="P^{-1/2}g(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult2.p2.3.m2" intent=":literal"><semantics><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">P^{-1/2}g(\bm{\theta})</annotation></semantics></math> and <math alttext="P^{-1/2}H(\bm{\theta})P^{-1/2}" class="ltx_Math" display="inline" id="S10.Thmresult2.p2.4.m3" intent=":literal"><semantics><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup></mrow><annotation encoding="application/x-tex">P^{-1/2}H(\bm{\theta})P^{-1/2}</annotation></semantics></math>. Finally, writing the pre-conditioned gradient as <math alttext="P^{-1/2}g(\bm{\theta})=\sum_{i=1}^{n}\alpha_{i}v_{i}(\bm{\theta})" class="ltx_Math" display="inline" id="S10.Thmresult2.p2.5.m4" intent=":literal"><semantics><mrow><mrow><msup><mi>P</mi><mrow><mo>âˆ’</mo><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>g</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo rspace="0.111em">=</mo><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msub><mi>Î±</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>v</mi><mi>i</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">P^{-1/2}g(\bm{\theta})=\sum_{i=1}^{n}\alpha_{i}v_{i}(\bm{\theta})</annotation></semantics></math>, we have the required result:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx13">
<tbody id="S10.Ex8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{dir}=\frac{\sum_{i=1}^{n}\alpha_{i}^{2}\lambda_{i}^{PH}}{\sum_{i=1}^{n}\alpha_{i}^{2}}\leq\lambda^{PH}_{\max}." class="ltx_Math" display="inline" id="S10.Ex8.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>Î»</mi><mrow><mi>d</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>i</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>r</mi></mrow></msub><mo>=</mo><mstyle displaystyle="true"><mfrac><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><msubsup><mi>Î±</mi><mi>i</mi><mn>2</mn></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>i</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow></mrow><mrow><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msubsup><mi>Î±</mi><mi>i</mi><mn>2</mn></msubsup></mrow></mfrac></mstyle><mo>â‰¤</mo><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{dir}=\frac{\sum_{i=1}^{n}\alpha_{i}^{2}\lambda_{i}^{PH}}{\sum_{i=1}^{n}\alpha_{i}^{2}}\leq\lambda^{PH}_{\max}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
</section>
<section class="ltx_subsection" id="S10.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2 </span>Stability Threshold for Optimizers with Weight Decay</h3>
<div class="ltx_para" id="S10.SS2.p1">
<p class="ltx_p" id="S10.SS2.p1.1">In this section, we derive stability thresholds for common optimizers with Weight Decay (WD) with strength <math alttext="\gamma" class="ltx_Math" display="inline" id="S10.SS2.p1.1.m1" intent=":literal"><semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> using quadratic loss function defined as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx14">
<tbody id="S10.Ex9"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle L(\bm{\theta})=\frac{1}{2}\bm{\theta}^{T}H\bm{\theta}+\bm{g}^{T}\bm{\theta}+c." class="ltx_Math" display="inline" id="S10.Ex9.m1" intent=":literal"><semantics><mrow><mrow><mrow><mi>L</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mi>ğœ½</mi><mo stretchy="false">)</mo></mrow></mrow><mo>=</mo><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mn>2</mn></mfrac></mstyle><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğœ½</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mo>+</mo><mrow><msup><mi>ğ’ˆ</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mo>+</mo><mi>c</mi></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle L(\bm{\theta})=\frac{1}{2}\bm{\theta}^{T}H\bm{\theta}+\bm{g}^{T}\bm{\theta}+c.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_theorem ltx_theorem_result" id="S10.Thmresult3">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S10.Thmresult3.1.1.1">Result 10.3</span></span><span class="ltx_text ltx_font_bold" id="S10.Thmresult3.2.2"> </span>(Stability threshold for GD with WD)<span class="ltx_text ltx_font_bold" id="S10.Thmresult3.3.3">.</span>
</h6>
<div class="ltx_para" id="S10.Thmresult3.p1">
<p class="ltx_p" id="S10.Thmresult3.p1.1">For gradient descent, adding weight decay shifts the EoS threshold by the decay strength <math alttext="\gamma" class="ltx_Math" display="inline" id="S10.Thmresult3.p1.1.m1" intent=":literal"><semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx15">
<tbody id="S10.Ex10"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda^{H}_{\max}=\frac{2}{\eta}-\gamma" class="ltx_Math" display="inline" id="S10.Ex10.m1" intent=":literal"><semantics><mrow><msubsup><mi>Î»</mi><mi>max</mi><mi>H</mi></msubsup><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>2</mn><mi>Î·</mi></mfrac></mstyle><mo>âˆ’</mo><mi>Î³</mi></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\lambda^{H}_{\max}=\frac{2}{\eta}-\gamma</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult3.p1.3">where <math alttext="\eta" class="ltx_Math" display="inline" id="S10.Thmresult3.p1.2.m1" intent=":literal"><semantics><mi>Î·</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> is the learning rate and <math alttext="\gamma" class="ltx_Math" display="inline" id="S10.Thmresult3.p1.3.m2" intent=":literal"><semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is the weight decay strength.</p>
</div>
<div class="ltx_para" id="S10.Thmresult3.p2">
<p class="ltx_p" id="S10.Thmresult3.p2.6">Proof.
The GD updates with weight decay are given by</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx16">
<tbody id="S10.Ex11"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{\theta}_{t+1}" class="ltx_Math" display="inline" id="S10.Ex11.m1" intent=":literal"><semantics><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\displaystyle\bm{\theta}_{t+1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(1-\eta\gamma)\bm{\theta}_{t}-\eta\left(H+\bm{g}\right)." class="ltx_Math" display="inline" id="S10.Ex11.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>(</mo><mrow><mi>H</mi><mo>+</mo><mi>ğ’ˆ</mi></mrow><mo>)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=(1-\eta\gamma)\bm{\theta}_{t}-\eta\left(H+\bm{g}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult3.p2.1">The GD dynamics projected along the top eigenvector <math alttext="\bm{u}" class="ltx_Math" display="inline" id="S10.Thmresult3.p2.1.m1" intent=":literal"><semantics><mi>ğ’–</mi><annotation encoding="application/x-tex">\bm{u}</annotation></semantics></math> is given by:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx17">
<tbody id="S10.Ex12"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{u}^{T}\bm{\theta}_{t+1}" class="ltx_Math" display="inline" id="S10.Ex12.m1" intent=":literal"><semantics><mrow><msup><mi>ğ’–</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\displaystyle\bm{u}^{T}\bm{\theta}_{t+1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\bm{u}^{T}(1-\eta\gamma-\eta H)\bm{\theta}_{t}-\eta\bm{u}^{T}\bm{g}," class="ltx_Math" display="inline" id="S10.Ex12.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><msup><mi>ğ’–</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğ’–</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğ’ˆ</mi></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle=\bm{u}^{T}(1-\eta\gamma-\eta H)\bm{\theta}_{t}-\eta\bm{u}^{T}\bm{g},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S10.Ex13"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{u}^{T}\bm{\theta}_{t+1}" class="ltx_Math" display="inline" id="S10.Ex13.m1" intent=":literal"><semantics><mrow><msup><mi>ğ’–</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\displaystyle\bm{u}^{T}\bm{\theta}_{t+1}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=(1-\eta\gamma-\eta\lambda)\bm{u}^{T}\bm{\theta}_{t}-\eta\bm{u}^{T}\bm{g}." class="ltx_Math" display="inline" id="S10.Ex13.m2" intent=":literal"><semantics><mrow><mrow><mi></mi><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î»</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğ’–</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğ’–</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğ’ˆ</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle=(1-\eta\gamma-\eta\lambda)\bm{u}^{T}\bm{\theta}_{t}-\eta\bm{u}^{T}\bm{g}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult3.p2.2">Furthermore, define <math alttext="q=\bm{u}^{T}\bm{\theta}+\frac{1}{\lambda+\gamma}\bm{u}^{T}\bm{g}" class="ltx_Math" display="inline" id="S10.Thmresult3.p2.2.m1" intent=":literal"><semantics><mrow><mi>q</mi><mo>=</mo><mrow><mrow><msup><mi>ğ’–</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğœ½</mi></mrow><mo>+</mo><mrow><mfrac><mn>1</mn><mrow><mi>Î»</mi><mo>+</mo><mi>Î³</mi></mrow></mfrac><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>ğ’–</mi><mi>T</mi></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğ’ˆ</mi></mrow></mrow></mrow><annotation encoding="application/x-tex">q=\bm{u}^{T}\bm{\theta}+\frac{1}{\lambda+\gamma}\bm{u}^{T}\bm{g}</annotation></semantics></math>, then</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx18">
<tbody id="S10.Ex14"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle q_{t+1}=(1-\eta\gamma-\eta\lambda)q_{t}." class="ltx_Math" display="inline" id="S10.Ex14.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>q</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î»</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>q</mi><mi>t</mi></msub></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle q_{t+1}=(1-\eta\gamma-\eta\lambda)q_{t}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult3.p2.5">The sequence <math alttext="\{q_{t}\}_{t=0}^{T}" class="ltx_Math" display="inline" id="S10.Thmresult3.p2.3.m1" intent=":literal"><semantics><msubsup><mrow><mo stretchy="false">{</mo><msub><mi>q</mi><mi>t</mi></msub><mo stretchy="false">}</mo></mrow><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mi>T</mi></msubsup><annotation encoding="application/x-tex">\{q_{t}\}_{t=0}^{T}</annotation></semantics></math> diverges if <math alttext="|1-\eta(\lambda+\gamma)|&gt;1" class="ltx_Math" display="inline" id="S10.Thmresult3.p2.4.m2" intent=":literal"><semantics><mrow><mrow><mo stretchy="false">|</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mi>Î»</mi><mo>+</mo><mi>Î³</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo stretchy="false">|</mo></mrow><mo>&gt;</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">|1-\eta(\lambda+\gamma)|&gt;1</annotation></semantics></math>, which is equivalent to <math alttext="\lambda&gt;\frac{2}{\eta}-\gamma" class="ltx_Math" display="inline" id="S10.Thmresult3.p2.5.m3" intent=":literal"><semantics><mrow><mi>Î»</mi><mo>&gt;</mo><mrow><mfrac><mn>2</mn><mi>Î·</mi></mfrac><mo>âˆ’</mo><mi>Î³</mi></mrow></mrow><annotation encoding="application/x-tex">\lambda&gt;\frac{2}{\eta}-\gamma</annotation></semantics></math>.</p>
</div>
</div>
<div class="ltx_para" id="S10.SS2.p2">
<p class="ltx_p" id="S10.SS2.p2.1">To prove results for Adam, we need the following result from <cite class="ltx_cite ltx_citemacro_cite">Elaydi (<a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#bib.bib14" title="">2005</a>)</cite>.</p>
</div>
<div class="ltx_theorem ltx_theorem_lemma" id="S10.Thmtheorem1">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S10.Thmtheorem1.1.1.1">Lemma 10.1</span></span><span class="ltx_text ltx_font_bold" id="S10.Thmtheorem1.2.2"> </span>(Stability Condition for Difference Equations)<span class="ltx_text ltx_font_bold" id="S10.Thmtheorem1.3.3">.</span>
</h6>
<div class="ltx_para" id="S10.Thmtheorem1.p1">
<p class="ltx_p" id="S10.Thmtheorem1.p1.1"><span class="ltx_text ltx_font_italic" id="S10.Thmtheorem1.p1.1.1">Consider non-homogeneous difference equations of the type:</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx19">
<tbody id="S10.Ex15"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle q_{t+1}+p_{1}q_{t}+p_{2}q_{t-1}-c=0." class="ltx_Math" display="inline" id="S10.Ex15.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><msub><mi>q</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mrow><msub><mi>p</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>q</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><msub><mi>p</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>q</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>âˆ’</mo><mi>c</mi></mrow><mo>=</mo><mn>0</mn></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle q_{t+1}+p_{1}q_{t}+p_{2}q_{t-1}-c=0.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmtheorem1.p1.2"><span class="ltx_text ltx_font_italic" id="S10.Thmtheorem1.p1.2.1">The solutions of the above equation are asymptotically stable iff:</span></p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx20">
<tbody id="S10.Ex16"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle 1+p_{1}+p_{2}&gt;0,\qquad" class="ltx_Math" display="inline" id="S10.Ex16.m1" intent=":literal"><semantics><mrow><mrow><mrow><mn>1</mn><mo>+</mo><msub><mi>p</mi><mn>1</mn></msub><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle 1+p_{1}+p_{2}&gt;0,\qquad</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle 1-p_{1}+p_{2}&gt;0,\qquad" class="ltx_Math" display="inline" id="S10.Ex16.m2" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>p</mi><mn>1</mn></msub></mrow><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle 1-p_{1}+p_{2}&gt;0,\qquad</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle 1-p_{2}&gt;0" class="ltx_Math" display="inline" id="S10.Ex16.m3" intent=":literal"><semantics><mrow><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\displaystyle 1-p_{2}&gt;0</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</div>
<div class="ltx_theorem ltx_theorem_result" id="S10.Thmresult4">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span class="ltx_text ltx_font_bold" id="S10.Thmresult4.1.1.1">Result 10.4</span></span><span class="ltx_text ltx_font_bold" id="S10.Thmresult4.2.2"> </span>(Stability threshold for AdamW)<span class="ltx_text ltx_font_bold" id="S10.Thmresult4.3.3">.</span>
</h6>
<div class="ltx_para" id="S10.Thmresult4.p1">
<p class="ltx_p" id="S10.Thmresult4.p1.1">For Adam, adding weight decay shifts the EoS threshold a constant that depends on the decay strength <math alttext="\gamma" class="ltx_Math" display="inline" id="S10.Thmresult4.p1.1.m1" intent=":literal"><semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx21">
<tbody id="S10.Ex17"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda^{PH}_{\max}=\frac{2+2\beta_{1}}{\eta(1-\beta_{1})}-\gamma" class="ltx_Math" display="inline" id="S10.Ex17.m1" intent=":literal"><semantics><mrow><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mn>2</mn><mo>+</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow></mrow><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>âˆ’</mo><mi>Î³</mi></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\lambda^{PH}_{\max}=\frac{2+2\beta_{1}}{\eta(1-\beta_{1})}-\gamma</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p1.3">where <math alttext="\eta" class="ltx_Math" display="inline" id="S10.Thmresult4.p1.2.m1" intent=":literal"><semantics><mi>Î·</mi><annotation encoding="application/x-tex">\eta</annotation></semantics></math> is the learning rate and <math alttext="\gamma" class="ltx_Math" display="inline" id="S10.Thmresult4.p1.3.m2" intent=":literal"><semantics><mi>Î³</mi><annotation encoding="application/x-tex">\gamma</annotation></semantics></math> is the weight decay strength.</p>
</div>
<div class="ltx_para" id="S10.Thmresult4.p2">
<p class="ltx_p" id="S10.Thmresult4.p2.3">Proof. The update equations of AdamW can be written as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx22">
<tbody id="S10.Ex18"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{m}_{t+1}=\beta_{1}\bm{m}_{t}+(1-\beta_{1})\nabla_{\theta}L" class="ltx_Math" display="inline" id="S10.Ex18.m1" intent=":literal"><semantics><mrow><msub><mi>ğ’</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ’</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0.167em" rspace="0em">â€‹</mo><mrow><msub><mo rspace="0.167em">âˆ‡</mo><mi>Î¸</mi></msub><mi>L</mi></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\bm{m}_{t+1}=\beta_{1}\bm{m}_{t}+(1-\beta_{1})\nabla_{\theta}L</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S10.Ex19"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{\theta}_{t+1}=(1-\eta\gamma)\bm{\theta}_{t}-\eta P^{-1}_{t+1}\bm{m}_{t+1}," class="ltx_Math" display="inline" id="S10.Ex19.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>P</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ’</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{\theta}_{t+1}=(1-\eta\gamma)\bm{\theta}_{t}-\eta P^{-1}_{t+1}\bm{m}_{t+1},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p2.2">where <math alttext="P_{t+1}=(1-\beta^{t+1}_{1})\left[\text{diag}(\sqrt{\bm{v}_{t+1}})+\epsilon\bm{I}\right]" class="ltx_Math" display="inline" id="S10.Thmresult4.p2.1.m1" intent=":literal"><semantics><mrow><msub><mi>P</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msubsup><mi>Î²</mi><mn>1</mn><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msubsup></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>[</mo><mrow><mrow><mtext>diag</mtext><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><msqrt><msub><mi>ğ’—</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></msqrt><mo stretchy="false">)</mo></mrow></mrow><mo>+</mo><mrow><mi>Ïµ</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğ‘°</mi></mrow></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">P_{t+1}=(1-\beta^{t+1}_{1})\left[\text{diag}(\sqrt{\bm{v}_{t+1}})+\epsilon\bm{I}\right]</annotation></semantics></math> is Adamâ€™s pre-conditioner with <math alttext="\bm{v}_{t+1}=\beta_{2}\bm{v}_{t}+(1-\beta_{2})(\nabla_{\theta}L)^{2}" class="ltx_Math" display="inline" id="S10.Thmresult4.p2.2.m2" intent=":literal"><semantics><mrow><msub><mi>ğ’—</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><msub><mi>Î²</mi><mn>2</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ’—</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>2</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msup><mrow><mo stretchy="false">(</mo><mrow><msub><mo rspace="0.167em">âˆ‡</mo><mi>Î¸</mi></msub><mi>L</mi></mrow><mo stretchy="false">)</mo></mrow><mn>2</mn></msup></mrow></mrow></mrow><annotation encoding="application/x-tex">\bm{v}_{t+1}=\beta_{2}\bm{v}_{t}+(1-\beta_{2})(\nabla_{\theta}L)^{2}</annotation></semantics></math> is the moving average of the squared gradients.</p>
</div>
<div class="ltx_para" id="S10.Thmresult4.p3">
<p class="ltx_p" id="S10.Thmresult4.p3.1">Next, we multiply the parameter update equation by <math alttext="P_{t}" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.1.m1" intent=":literal"><semantics><msub><mi>P</mi><mi>t</mi></msub><annotation encoding="application/x-tex">P_{t}</annotation></semantics></math> and rearrange the terms to obtain:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx23">
<tbody id="S10.Ex20"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle P_{t+1}\bm{\theta}_{t+1}=(1-\eta\gamma)P_{t+1}\bm{\theta}_{t}-\eta\bm{m}_{t+1}" class="ltx_Math" display="inline" id="S10.Ex20.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>P</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>=</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>P</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ’</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle P_{t+1}\bm{\theta}_{t+1}=(1-\eta\gamma)P_{t+1}\bm{\theta}_{t}-\eta\bm{m}_{t+1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S10.Ex21"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\bm{m}_{t+1}=\frac{1}{\eta}P_{t+1}\left[(1-\eta\gamma)\bm{\theta}_{t}-\bm{\theta}_{t+1}\right]." class="ltx_Math" display="inline" id="S10.Ex21.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ğ’</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>Î·</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>P</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>[</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{m}_{t+1}=\frac{1}{\eta}P_{t+1}\left[(1-\eta\gamma)\bm{\theta}_{t}-\bm{\theta}_{t+1}\right].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.2">Next, shift the time index back by one to get <math alttext="\bm{m_{t}}" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.2.m1" intent=":literal"><semantics><msub><mi>ğ’</mi><mi>ğ’•</mi></msub><annotation encoding="application/x-tex">\bm{m_{t}}</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx24">
<tbody id="S10.Ex22"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{m}_{t}=\frac{1}{\eta}P_{t}\left[(1-\eta\gamma)\bm{\theta}_{t-1}-\bm{\theta}_{t}\right]." class="ltx_Math" display="inline" id="S10.Ex22.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ğ’</mi><mi>t</mi></msub><mo>=</mo><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>Î·</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>P</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>[</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub></mrow><mo>âˆ’</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>]</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{m}_{t}=\frac{1}{\eta}P_{t}\left[(1-\eta\gamma)\bm{\theta}_{t-1}-\bm{\theta}_{t}\right].</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.10">Inserting this in the momentum update equation:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx25">
<tbody id="S10.Ex23"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{m}_{t+1}=\beta_{1}\bm{m}_{t}+(1-\beta_{1})(H\bm{\theta}_{t}+\bm{g})," class="ltx_Math" display="inline" id="S10.Ex23.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ğ’</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ’</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>+</mo><mi>ğ’ˆ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{m}_{t+1}=\beta_{1}\bm{m}_{t}+(1-\beta_{1})(H\bm{\theta}_{t}+\bm{g}),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.11">we obtain:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx26">
<tbody id="S10.Ex24"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\frac{1}{\eta}P_{t+1}\left[(1-\eta\gamma)\bm{\theta}_{t}-\bm{\theta}_{t+1}\right]=\beta_{1}\frac{1}{\eta}P_{t}\left[(1-\eta\gamma)\bm{\theta}_{t-1}-\bm{\theta}_{t}\right]+(1-\beta_{1})(H\bm{\theta}_{t}+\bm{g})." class="ltx_Math" display="inline" id="S10.Ex24.m1" intent=":literal"><semantics><mrow><mrow><mrow><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>Î·</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>P</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>[</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>]</mo></mrow></mrow><mo>=</mo><mrow><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mstyle displaystyle="true"><mfrac><mn>1</mn><mi>Î·</mi></mfrac></mstyle><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>P</mi><mi>t</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>[</mo><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub></mrow><mo>âˆ’</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>]</mo></mrow></mrow><mo>+</mo><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>+</mo><mi>ğ’ˆ</mi></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\frac{1}{\eta}P_{t+1}\left[(1-\eta\gamma)\bm{\theta}_{t}-\bm{\theta}_{t+1}\right]=\beta_{1}\frac{1}{\eta}P_{t}\left[(1-\eta\gamma)\bm{\theta}_{t-1}-\bm{\theta}_{t}\right]+(1-\beta_{1})(H\bm{\theta}_{t}+\bm{g}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.4">Next, we assume that the pre-conditioner is changing slowly and as a result <math alttext="P=P_{t+1}\approx P_{t}" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.3.m1" intent=":literal"><semantics><mrow><mi>P</mi><mo>=</mo><msub><mi>P</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>â‰ˆ</mo><msub><mi>P</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">P=P_{t+1}\approx P_{t}</annotation></semantics></math>. On multiplying the above equation with <math alttext="\eta P_{t+1}^{-1}" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.4.m2" intent=":literal"><semantics><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>P</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">\eta P_{t+1}^{-1}</annotation></semantics></math>, we get:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx27">
<tbody id="S10.Ex25"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle(1-\eta\gamma)\bm{\theta}_{t}-\bm{\theta}_{t+1}=\beta_{1}(1-\eta\gamma)\bm{\theta}_{t-1}-\beta_{1}\bm{\theta}_{t}+\eta(1-\beta_{1})P^{-1}H\bm{\theta}_{t}+\eta(1-\beta_{1})P^{-1}\bm{g}." class="ltx_Math" display="inline" id="S10.Ex25.m1" intent=":literal"><semantics><mrow><mrow><mrow><mrow><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><mo>=</mo><mrow><mrow><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub></mrow><mo>âˆ’</mo><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow></mrow><mo>+</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>+</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğ’ˆ</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle(1-\eta\gamma)\bm{\theta}_{t}-\bm{\theta}_{t+1}=\beta_{1}(1-\eta\gamma)\bm{\theta}_{t-1}-\beta_{1}\bm{\theta}_{t}+\eta(1-\beta_{1})P^{-1}H\bm{\theta}_{t}+\eta(1-\beta_{1})P^{-1}\bm{g}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.6">Next, we rearrange the terms to group <math alttext="\bm{\theta}_{t+1},\bm{\theta}_{t}" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.5.m1" intent=":literal"><semantics><mrow><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\bm{\theta}_{t+1},\bm{\theta}_{t}</annotation></semantics></math> and <math alttext="\bm{\theta}_{t-1}" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.6.m2" intent=":literal"><semantics><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\bm{\theta}_{t-1}</annotation></semantics></math>:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx28">
<tbody id="S10.Ex26"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\bm{\theta}_{t+1}=\left[(1-\eta\gamma+\beta_{1}-\eta(1-\beta_{1})P^{-1}H)\right]\bm{\theta}_{t}-\beta_{1}(1-\eta\gamma)\bm{\theta}_{t-1}-\eta(1-\beta_{1})P^{-1}\bm{g}." class="ltx_Math" display="inline" id="S10.Ex26.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mrow><mo>[</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo>+</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğ’ˆ</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\bm{\theta}_{t+1}=\left[(1-\eta\gamma+\beta_{1}-\eta(1-\beta_{1})P^{-1}H)\right]\bm{\theta}_{t}-\beta_{1}(1-\eta\gamma)\bm{\theta}_{t-1}-\eta(1-\beta_{1})P^{-1}\bm{g}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.8">Next, we multiply on the right with the top eigenvector of the pre-conditioned Hessian <math alttext="\bm{v}_{\max}" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.7.m1" intent=":literal"><semantics><msub><mi>ğ’—</mi><mi>max</mi></msub><annotation encoding="application/x-tex">\bm{v}_{\max}</annotation></semantics></math> and define <math alttext="q_{t}=\bm{v}_{\max}^{T}\bm{\theta_{t}}" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.8.m2" intent=":literal"><semantics><mrow><msub><mi>q</mi><mi>t</mi></msub><mo>=</mo><mrow><msubsup><mi>ğ’—</mi><mi>max</mi><mi>T</mi></msubsup><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğœ½</mi><mi>ğ’•</mi></msub></mrow></mrow><annotation encoding="application/x-tex">q_{t}=\bm{v}_{\max}^{T}\bm{\theta_{t}}</annotation></semantics></math> to obtain:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx29">
<tbody id="S10.Ex27"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle q_{t+1}=\left[(1-\eta\gamma+\beta_{1}-\eta(1-\beta_{1})\lambda_{\max}^{PH})\right]q_{t}-\beta_{1}(1-\eta\gamma)q_{t-1}-\eta(1-\beta_{1})\bm{v}_{\max}P^{-1}\bm{g}." class="ltx_Math" display="inline" id="S10.Ex27.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>q</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mrow><mrow><mo>[</mo><mrow><mo stretchy="false">(</mo><mrow><mrow><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo>+</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow></mrow><mo stretchy="false">)</mo></mrow><mo>]</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>q</mi><mi>t</mi></msub></mrow><mo>âˆ’</mo><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>q</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>ğ’—</mi><mi>max</mi></msub><mo lspace="0em" rspace="0em">â€‹</mo><msup><mi>P</mi><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><mo lspace="0em" rspace="0em">â€‹</mo><mi>ğ’ˆ</mi></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle q_{t+1}=\left[(1-\eta\gamma+\beta_{1}-\eta(1-\beta_{1})\lambda_{\max}^{PH})\right]q_{t}-\beta_{1}(1-\eta\gamma)q_{t-1}-\eta(1-\beta_{1})\bm{v}_{\max}P^{-1}\bm{g}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.12">Comparing the equation with Lemma <a class="ltx_ref" href="https://arxiv.org/html/2601.16979v1#S10.Thmtheorem1" title="Lemma 10.1 (Stability Condition for Difference Equations). â€£ 10.2 Stability Threshold for Optimizers with Weight Decay â€£ 10 Theoretical Results â€£ A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs"><span class="ltx_text ltx_ref_tag">10.1</span></a>, we have:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx30">
<tbody id="S10.Ex28"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle p_{1}=-\left[1-\eta\gamma+\beta_{1}-\eta(1-\beta_{1})\lambda_{\max}^{PH}\right]" class="ltx_Math" display="inline" id="S10.Ex28.m1" intent=":literal"><semantics><mrow><msub><mi>p</mi><mn>1</mn></msub><mo>=</mo><mrow><mo>âˆ’</mo><mrow><mo>[</mo><mrow><mrow><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo>+</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow></mrow><mo>]</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle p_{1}=-\left[1-\eta\gamma+\beta_{1}-\eta(1-\beta_{1})\lambda_{\max}^{PH}\right]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S10.Ex29"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle p_{2}=\beta_{1}(1-\eta\gamma)." class="ltx_Math" display="inline" id="S10.Ex29.m1" intent=":literal"><semantics><mrow><mrow><msub><mi>p</mi><mn>2</mn></msub><mo>=</mo><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle p_{2}=\beta_{1}(1-\eta\gamma).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.9">Plugging the above expressions into the first stability condition <math alttext="1-p_{1}+p_{2}&gt;0" class="ltx_Math" display="inline" id="S10.Thmresult4.p3.9.m1" intent=":literal"><semantics><mrow><mrow><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>p</mi><mn>1</mn></msub></mrow><mo>+</mo><msub><mi>p</mi><mn>2</mn></msub></mrow><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">1-p_{1}+p_{2}&gt;0</annotation></semantics></math>, we have:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx31">
<tbody id="S10.Ex30"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle 1+\left[1-\eta\gamma+\beta_{1}-\eta(1-\beta_{1})\lambda_{\max}^{PH}\right]+\beta_{1}(1-\eta\gamma)&gt;0," class="ltx_Math" display="inline" id="S10.Ex30.m1" intent=":literal"><semantics><mrow><mrow><mrow><mn>1</mn><mo>+</mo><mrow><mo>[</mo><mrow><mrow><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo>+</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup></mrow></mrow><mo>]</mo></mrow><mo>+</mo><mrow><msub><mi>Î²</mi><mn>1</mn></msub><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>Î³</mi></mrow></mrow><mo stretchy="false">)</mo></mrow></mrow></mrow><mo>&gt;</mo><mn>0</mn></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">\displaystyle 1+\left[1-\eta\gamma+\beta_{1}-\eta(1-\beta_{1})\lambda_{\max}^{PH}\right]+\beta_{1}(1-\eta\gamma)&gt;0,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.13">which yields the desired result:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S10.EGx32">
<tbody id="S10.Ex31"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\lambda_{\max}^{PH}&lt;\frac{2+2\beta_{1}}{\eta(1-\beta_{1})}-\gamma\left(\frac{1+\beta_{1}}{1-\beta_{1}}\right)=\left(\frac{2}{\eta}-\gamma\right)\left(\frac{1+\beta_{1}}{1-\beta_{1}}\right)." class="ltx_Math" display="inline" id="S10.Ex31.m1" intent=":literal"><semantics><mrow><mrow><msubsup><mi>Î»</mi><mi>max</mi><mrow><mi>P</mi><mo lspace="0em" rspace="0em">â€‹</mo><mi>H</mi></mrow></msubsup><mo>&lt;</mo><mrow><mstyle displaystyle="true"><mfrac><mrow><mn>2</mn><mo>+</mo><mrow><mn>2</mn><mo lspace="0em" rspace="0em">â€‹</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow></mrow><mrow><mi>Î·</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo stretchy="false">(</mo><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mo stretchy="false">)</mo></mrow></mrow></mfrac></mstyle><mo>âˆ’</mo><mrow><mi>Î³</mi><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow></mfrac></mstyle><mo>)</mo></mrow></mrow></mrow><mo>=</mo><mrow><mrow><mo>(</mo><mrow><mstyle displaystyle="true"><mfrac><mn>2</mn><mi>Î·</mi></mfrac></mstyle><mo>âˆ’</mo><mi>Î³</mi></mrow><mo>)</mo></mrow><mo lspace="0em" rspace="0em">â€‹</mo><mrow><mo>(</mo><mstyle displaystyle="true"><mfrac><mrow><mn>1</mn><mo>+</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î²</mi><mn>1</mn></msub></mrow></mfrac></mstyle><mo>)</mo></mrow></mrow></mrow><mo lspace="0em">.</mo></mrow><annotation encoding="application/x-tex">\displaystyle\lambda_{\max}^{PH}&lt;\frac{2+2\beta_{1}}{\eta(1-\beta_{1})}-\gamma\left(\frac{1+\beta_{1}}{1-\beta_{1}}\right)=\left(\frac{2}{\eta}-\gamma\right)\left(\frac{1+\beta_{1}}{1-\beta_{1}}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S10.Thmresult4.p3.14">The other two stability conditions are trivially satisfied.</p>
</div>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jan 23 18:49:31 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
