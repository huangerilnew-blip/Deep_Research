<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AnyView: Synthesizing Any Novel View in Dynamic Scenes</title>
<!--Generated on Sun Jan 18 21:06:13 2026 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="/static/browse/0.3.4/css/arxiv-html-papers-20250916.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2601.16982v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS1" title="In 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Video Generative Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2" title="In 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Dynamic View Synthesis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS1" title="In 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Problem Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2" title="In 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS3" title="In 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS4" title="In 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1" title="In 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS2" title="In 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3" title="In 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS4" title="In 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S5" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A1" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:144%;">A</span> </span><span class="ltx_text" style="font-size:144%;">Uncertainty Analysis</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:144%;">B</span> </span><span class="ltx_text" style="font-size:144%;">Additional Qualitative Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:144%;">C</span> </span><span class="ltx_text" style="font-size:144%;">Training Datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.SS1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Kubric-5D</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:144%;">D</span> </span><span class="ltx_text" style="font-size:144%;">Evaluation Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:144%;">E</span> </span><span class="ltx_text" style="font-size:144%;">Baselines</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">AnyView: Synthesizing Any Novel View in Dynamic Scenes</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_tabular ltx_align_middle" id="id12.12.12">
<span class="ltx_tbody">
<span class="ltx_tr" id="id4.4.4.4">
<span class="ltx_td ltx_align_center" id="id4.4.4.4.4">Basile Van Hoorick<sup class="ltx_sup" id="id4.4.4.4.4.1"><span class="ltx_text ltx_font_italic" id="id4.4.4.4.4.1.1">1</span></sup> ‚ÄÉDian Chen<sup class="ltx_sup" id="id4.4.4.4.4.2"><span class="ltx_text ltx_font_italic" id="id4.4.4.4.4.2.1">1</span></sup> ‚ÄÉShun Iwase<sup class="ltx_sup" id="id4.4.4.4.4.3"><span class="ltx_text ltx_font_italic" id="id4.4.4.4.4.3.1">1</span></sup> ‚ÄÉPavel Tokmakov<sup class="ltx_sup" id="id4.4.4.4.4.4"><span class="ltx_text ltx_font_italic" id="id4.4.4.4.4.4.1">1</span></sup></span></span>
<span class="ltx_tr" id="id8.8.8.8">
<span class="ltx_td ltx_align_center" id="id8.8.8.8.4">Muhammad Zubair Irshad<sup class="ltx_sup" id="id8.8.8.8.4.1"><span class="ltx_text ltx_font_italic" id="id8.8.8.8.4.1.1">1</span></sup> ‚ÄÉIgor Vasiljevic<sup class="ltx_sup" id="id8.8.8.8.4.2"><span class="ltx_text ltx_font_italic" id="id8.8.8.8.4.2.1">1</span></sup> ‚ÄÉSwati Gupta<sup class="ltx_sup" id="id8.8.8.8.4.3"><span class="ltx_text ltx_font_italic" id="id8.8.8.8.4.3.1">1</span></sup> ‚ÄÉFangzhou Cheng<sup class="ltx_sup" id="id8.8.8.8.4.4"><span class="ltx_text ltx_font_italic" id="id8.8.8.8.4.4.1">1,2</span></sup></span></span>
<span class="ltx_tr" id="id10.10.10.10">
<span class="ltx_td ltx_align_center" id="id10.10.10.10.2">Sergey Zakharov<sup class="ltx_sup" id="id10.10.10.10.2.1"><span class="ltx_text ltx_font_italic" id="id10.10.10.10.2.1.1">1</span></sup> ‚ÄÉVitor Campagnolo Guizilini<sup class="ltx_sup" id="id10.10.10.10.2.2"><span class="ltx_text ltx_font_italic" id="id10.10.10.10.2.2.1">1</span></sup></span></span>
<span class="ltx_tr" id="id12.12.12.12">
<span class="ltx_td ltx_align_center" id="id12.12.12.12.2"><sup class="ltx_sup" id="id12.12.12.12.2.2"><span class="ltx_text ltx_font_italic" id="id12.12.12.12.2.2.1" style="font-size:83%;">1</span></sup><span class="ltx_text" id="id12.12.12.12.2.1" style="font-size:83%;">Toyota Research Institute ‚ÄÇ‚ÄÑ<sup class="ltx_sup" id="id12.12.12.12.2.1.1"><span class="ltx_text ltx_font_italic" id="id12.12.12.12.2.1.1.1">2</span></sup>Amazon Web Services</span></span></span>
<span class="ltx_tr" id="id12.12.12.13.1">
<span class="ltx_td ltx_align_center" id="id12.12.12.13.1.1"><a class="ltx_ref ltx_href" href="https://tri-ml.github.io/AnyView/" title="">tri-ml.github.io/AnyView</a></span></span>
</span>
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id15.id1">Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments.
In this work, we introduce <span class="ltx_text ltx_font_bold" id="id15.id1.1">AnyView</span>, a diffusion-based video generation framework for <em class="ltx_emph ltx_font_italic" id="id15.id1.2">dynamic view synthesis</em> with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories.
We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose AnyViewBench, a challenging new benchmark tailored towards <em class="ltx_emph ltx_font_italic" id="id15.id1.3">extreme</em> dynamic view synthesis in diverse real-world scenarios.
In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints,
while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from <em class="ltx_emph ltx_font_italic" id="id15.id1.4">any</em> viewpoint.</p>
</div>
<div class="ltx_logical-block" id="id14">
<div class="ltx_para" id="id14.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="446" id="id13.g1" src="x1.png" width="951"/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.6.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.7.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S0.F1.7.2.1">Enabling consistent extreme monocular dynamic view synthesis:</span>
We introduce <em class="ltx_emph ltx_font_italic" id="S0.F1.7.2.2">AnyView</em>, a diffusion framework that can generate videos of dynamic scenes from <em class="ltx_emph ltx_font_italic" id="S0.F1.7.2.3">any</em> chosen perspective,
conditioned on a single input video.
Our model operates end-to-end, without explicit scene reconstruction or expensive test-time optimization techniques.
Existing methods tend to fail to extrapolate, largely copying the input view.
More recent baselines can recover the overall structure in some cases (1st, 2nd rows), but fail when the camera trajectories become more complex (3rd row).
Meanwhile, our method preserves scene geometry, appearance, and dynamics,
despite working with drastically different target poses and highly ‚Äúincomplete‚Äù visual observations.
<span class="ltx_text" id="S0.F1.7.2.4" style="font-size:89%;">(D) indicates a baseline that relies on reprojected point clouds from estimated depth maps.</span>
</span></figcaption>
</figure>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Generating a new video from an arbitrary camera perspective while the scene is in motion is a highly ambitious and fundamentally under-constrained task.
A single input view only depicts a fraction of the world; the rest is occluded, transient, or simply unknown.
New moving objects may enter the scene at any moment, and unobserved regions might be dynamic themselves, further introducing uncertainty into the generative process.
Exact 4D reconstruction from such signals is therefore impractical in the general case.
For many downstream uses of 4D video representations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib66" title="Dreamitate: real-world visuomotor policy learning via video generation">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib67" title="Drivedreamer: towards real-world-drive world models for autonomous driving">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib37" title="Ctrl-world: a controllable generative world model for robot manipulation">19</a>]</cite>, ‚Äî such as robotics, world models, simulation, telepresence, VR/AR, autonomous driving ‚Äî what matters is not an exact correspondence with ground truth, but rather whether the resulting representation is realistic, temporally stable, and self-consistent across large viewpoint changes.
A common problem with learned visuomotor policies, for example, is that they often suffer from brittleness under shifting camera poses¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib49" title="RoVi-aug: robot and viewpoint augmentation for cross-embodiment robot learning">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib50" title="View-invariant policy learning via zero-shot novel view synthesis">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib51" title="Learning view-invariant world models for visual robotic manipulation">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib52" title="Mobi-pi: mobilizing your robot learning policy">62</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Humans routinely engage this problem in a way that is both rooted in intuition and very useful in practice: as we observe the physical world, we mentally ‚Äúre-project‚Äù scenes, inferring likely layouts, object shapes, scene completions, and plausible dynamics from limited information¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib56" title="The neural mechanisms of perceptual filling-in">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib57" title="The importance of amodal completion in everyday perception">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib58" title="Beyond the edges of a view: boundary extension in human scene-selective visual cortex">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib59" title="Untangling invariant object recognition">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib60" title="Object permanence in 3/12-and 4/12-month-old infants.">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib61" title="The reviewing of object files: object-specific integration of information">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib62" title="Mental rotation of three-dimensional objects">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib63" title="Spatial memory: how egocentric and allocentric combine">7</a>]</cite>.
This is not simply a low-level reconstruction capability: it is a powerful prior over shapes, semantics, materials, and motion that yields predictions that are largely viewpoint-invariant.
The goal of this paper is to take a step towards solving that objective: we target perceptually realistic 4D video synthesis under extreme camera trajectories and displacements.
To that end, we endow video generative models with the same inductive bias: to produce reasonable scene completions, based on a single input video,
that respect scene geometry, physics, and object permanence, even when there is little overlap with the conditioning view.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Most existing <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">dynamic view synthesis</em> (DVS) approaches and benchmarks are not built for this regime¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib26" title="Dynamic novel-view synthesis: a reality check">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib4" title="Shape of motion: 4d reconstruction from a single video">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib53" title="Dynamic view synthesis from dynamic monocular video">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib54" title="4d gaussian splatting for real-time dynamic scene rendering">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib55" title="Vivid4D: improving 4d reconstruction from monocular video by video inpainting">25</a>]</cite>, as they typically operate in <em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">narrow</em> settings: the input and target cameras are spatially nearby, looking in similar directions, and thus methods are designed to maximize pixel metrics under limited motion, ignoring the rest of the scene.
In particular, most current state of the art DVS methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib43" title="Dynamic view synthesis as an inverse problem">66</a>]</cite> rely on explicit 3D reconstructions (i.e., depth reprojection + image inpainting), costly test-time optimization and finetuning techniques, and support a limited set of camera trajectories.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To move away from this simplified setting, we first present <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">AnyView</span>,
a novel diffusion-based DVS architecture for high-fidelity video-to-video synthesis under dramatic camera trajectory changes, capable of producing perceptually plausible and semantically consistent videos from arbitrary novel viewpoints.
Our framework is purposefully light on explicit inductive biases: camera parameters are provided via dense ray-space conditioning, allowing us to support any model (including non-pinhole), and the network learns to synthesize unobserved content implicitly, guided by large-scale, diverse training data. To reach this level of implicit 4D understanding, we leverage existing video foundation models as a source of rich internet-scale 2D appearance and motion priors, and augment them by incorporating multi-view geometry and camera controllability, learned using 12 multi-domain 3D and 4D datasets.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Secondly, due to the aforementioned shortcomings of existing evaluation procedures, we assemble <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">AnyViewBench</span>,
a novel benchmark that formalizes and standardizes the <em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">extreme</em> DVS task across various domains (driving, robotics, and human activity), camera rigs (ego-centric and exo-centric), and camera motion patterns (fixed, linear, or complex, sometimes with changing intrinsics).
Each scene provides at least two time-synchronized views, enabling rigorous metric evaluations with ground truth videos without resorting to proxy setups.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="180" id="S2.F2.g1" src="x2.png" width="813"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.8.2.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.2.1" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S2.F2.2.1.1">The AnyView architecture.</span>
For both the clean input and noisy target videos, we concatenate pixels (RGB values) and camera information (Pl√ºcker vectors) belonging to the <span class="ltx_text ltx_font_italic" id="S2.F2.2.1.2">same</span> viewpoint along the <span class="ltx_text ltx_font_italic" id="S2.F2.2.1.3">channel</span> dimension, after independently encoding each modality into latent embeddings.
We then stack these two multimodal videos along the <span class="ltx_text ltx_font_italic" id="S2.F2.2.1.4">sequence dimension</span>, for a total of <math alttext="2\cdot t\cdot h\cdot w" class="ltx_Math" display="inline" id="S2.F2.2.1.m1" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>t</mi><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>h</mi><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">2\cdot t\cdot h\cdot w</annotation></semantics></math> tokens, which are fed into the diffusion transformer to iteratively denoise the target video.
</span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Video Generative Models</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In recent years, significant advances have been made in video generation, leading to the development of increasingly capable generative models.
Stability AI‚Äôs SVD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib30" title="Stable video diffusion: scaling latent video diffusion models to large datasets">6</a>]</cite> pioneered video diffusion by adding temporal layers to a pre-trained image diffusion network¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib34" title="High-resolution image synthesis with latent diffusion models">44</a>]</cite>, allowing coherent short video clip generation from single images or text prompts.
CogVideoX¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib32" title="CogVideoX: text-to-video diffusion models with an expert transformer">64</a>]</cite> introduced a 3D Variational Autoencoder (VAE) to compress videos across spatial and temporal dimensions, enhancing both compression rate and video fidelity. NVIDIA‚Äôs Cosmos¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib28" title="Cosmos world foundation model platform for physical ai">1</a>]</cite> introduced a suite of models with strong long-range temporal consistency and flexible conditioning signals (text, image and video input).
Wan¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib29" title="Wan: open and advanced large-scale video generative models">52</a>]</cite> is a novel mixture of experts-based video generation architecture, and provides a suite of video world models that excel at prompt following and photorealistic generation.
However, none of these architectures were originally designed with camera conditioning in mind, focusing instead on future frame forecasting in the single ‚Äì or more recently multi-camera¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib33" title="World simulation with video foundation models for physical ai">35</a>]</cite> ‚Äì setting.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Dynamic View Synthesis</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Dynamic view synthesis is the task of generating novel renderings from arbitrary viewpoints and timesteps given a monocular video of a dynamic scene.
A number of works have combined video generation with explicit geometric conditioning to improve geometric 3D consistency and control¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib69" title="Cami2v: camera-controlled image-to-video diffusion model">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib70" title="Cameractrl: enabling camera control for text-to-video generation">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib71" title="Sv3d: novel multi-view synthesis and 3d generation from a single image using latent video diffusion">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib72" title="Cat4d: create anything in 4d with multi-view video diffusion models">58</a>]</cite>.
Shape of Motion¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib4" title="Shape of motion: 4d reconstruction from a single video">53</a>]</cite> addresses monocular dynamic reconstruction by representing scene motion through a compact set of SE(3) motion bases, enabling soft segmentation into multiple rigidly moving parts using monocular depth and long-range 2D tracks. It fuses monocular depth and long-range 2D tracks to obtain a globally consistent dynamic 3D representation.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">While explicit modeling approaches can achieve relatively high accuracy, they are computationally expensive and brittle.
GCD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite> proposed to address dynamic view synthesis as an implicit problem, by re-purposing internet-scale video diffusion models via camera conditioning. This implicit formulation provides the greatest flexibility and robustness, but requires ground truth multi-view video data for training.
ReCamMaster¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib27" title="ReCamMaster: camera-controlled generative rendering from a single video">3</a>]</cite> advanced this research direction by utilizing a more powerful video generation model and a more realistic simulator to generate training data, whereas Trajectory Attention¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a>]</cite> augments video diffusion models with a trajectory-aware attention mechanism, improving fine-grained camera motion control and temporal consistency.
AC3D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib68" title="Ac3d: analyzing and improving 3d camera control in video diffusion transformers">2</a>]</cite> analyzes how video diffusion models internally represent 3D camera motion, adding ControlNet-style conditioning to improve controllability.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Other methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a>]</cite> have taken a hybrid approach by first lifting the input video in 3D via monocular depth estimation, reprojecting the resulting point cloud to the target camera pose, and then treating dynamic view synthesis as an inpainting problem. Among these methods, CogNVS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a>]</cite> further introduces test-time optimization to improve rendering accuracy at the cost of inference speed, while StreetCrafter¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib35" title="StreetCrafter: street view synthesis with controllable video diffusion models">61</a>]</cite> focuses on autonomous driving scene generation, utilizing LiDAR renderings as the control signal. Very recently, InverseDVS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib8" title="Dynamic view synthesis as an inverse problem">65</a>]</cite> has proposed a training-free approach that reformulates inpainting as structured latent manipulation in the noise initialization phase of a video diffusion model.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">While the shift towards explicit scene reconstruction and test-time optimization has led to high-quality dynamic view synthesis in the <span class="ltx_text ltx_font_italic" id="S2.SS2.p4.1.1">narrow</span> setting, where camera motion is limited to neighboring and highly overlapping regions, we experimentally demonstrate that these methods do not generalize to the more challenging <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.2">extreme</em> setting. In contrast, data-driven, implicit approaches are in principle capable of dynamic view synthesis from any viewpoint, but are in practice limited by the availability of diverse training data. In this work, we address this limitation by (1) combining a wide body of publicly available datasets to train AnyView ‚Äî the first model capable of synthesizing arbitrary novel views in dynamic, real-world scenes; and (2) proposing a new benchmark, AnyViewBench, to properly evaluate dynamic view synthesis performance in this new setting.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="310" id="S3.F3.g1" src="x3.png" width="809"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.9.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.10.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S3.F3.10.2.1">Overview of our training data mixture.</span>
We train and evaluate AnyView on both single-view and multi-view videos from four domains: <em class="ltx_emph ltx_font_italic" id="S3.F3.10.2.2">3D</em>, <em class="ltx_emph ltx_font_italic" id="S3.F3.10.2.3">Driving</em>, <em class="ltx_emph ltx_font_italic" id="S3.F3.10.2.4">Robotics</em>, and <em class="ltx_emph ltx_font_italic" id="S3.F3.10.2.5">Other</em> (see Section <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS3" title="3.3 Datasets ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3.3</span></a>).
During training, we perform weighted sampling to ensure each domain is seen equally often, <em class="ltx_emph ltx_font_italic" id="S3.F3.10.2.6">i.e</em>.<span class="ltx_text" id="S3.F3.10.2.7"></span> comprises 25% of the batch.
</span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Statement</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.14">The goal of dynamic view synthesis (DVS) is to create an output video <math alttext="\boldsymbol{V}_{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1" intent=":literal"><semantics><msub><mi>ùëΩ</mi><mi>y</mi></msub><annotation encoding="application/x-tex">\boldsymbol{V}_{y}</annotation></semantics></math> of an underlying scene as depicted from a chosen virtual viewpoint <math alttext="c_{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2" intent=":literal"><semantics><msub><mi>c</mi><mi>y</mi></msub><annotation encoding="application/x-tex">c_{y}</annotation></semantics></math>, given an input video <math alttext="\boldsymbol{V}_{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3" intent=":literal"><semantics><msub><mi>ùëΩ</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\boldsymbol{V}_{x}</annotation></semantics></math> recorded by a camera with known poses <math alttext="c_{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4" intent=":literal"><semantics><msub><mi>c</mi><mi>x</mi></msub><annotation encoding="application/x-tex">c_{x}</annotation></semantics></math> and intrinsics <math alttext="i_{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5" intent=":literal"><semantics><msub><mi>i</mi><mi>x</mi></msub><annotation encoding="application/x-tex">i_{x}</annotation></semantics></math> over time.
Specifically, we define the input (observed) RGB video as <math alttext="\boldsymbol{V}_{x}\in\mathbb{R}^{T\times H\times W\times 3}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6" intent=":literal"><semantics><mrow><msub><mi>ùëΩ</mi><mi>x</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>W</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{V}_{x}\in\mathbb{R}^{T\times H\times W\times 3}</annotation></semantics></math>, the target (unobserved) RGB video as <math alttext="\boldsymbol{V}_{y}\in\mathbb{R}^{T\times H\times W\times 3}" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7" intent=":literal"><semantics><mrow><msub><mi>ùëΩ</mi><mi>y</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>W</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{V}_{y}\in\mathbb{R}^{T\times H\times W\times 3}</annotation></semantics></math>, the input camera trajectory as <math alttext="c_{x}\in\mathbb{R}^{T\times 4\times 4}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>x</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>4</mn><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">c_{x}\in\mathbb{R}^{T\times 4\times 4}</annotation></semantics></math> with intrinsics <math alttext="i_{x}\in\mathbb{R}^{T\times 3\times 3}" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9" intent=":literal"><semantics><mrow><msub><mi>i</mi><mi>x</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">i_{x}\in\mathbb{R}^{T\times 3\times 3}</annotation></semantics></math>, and the target camera trajectory as <math alttext="c_{y}\in\mathbb{R}^{T\times 4\times 4}" class="ltx_Math" display="inline" id="S3.SS1.p1.10.m10" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>y</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>4</mn><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>4</mn></mrow></msup></mrow><annotation encoding="application/x-tex">c_{y}\in\mathbb{R}^{T\times 4\times 4}</annotation></semantics></math> with intrinsics <math alttext="i_{y}\in\mathbb{R}^{T\times 3\times 3}" class="ltx_Math" display="inline" id="S3.SS1.p1.11.m11" intent=":literal"><semantics><mrow><msub><mi>i</mi><mi>y</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">i_{y}\in\mathbb{R}^{T\times 3\times 3}</annotation></semantics></math>.
Using a generative model <math alttext="f" class="ltx_Math" display="inline" id="S3.SS1.p1.12.m12" intent=":literal"><semantics><mi>f</mi><annotation encoding="application/x-tex">f</annotation></semantics></math>, we estimate <math alttext="\boldsymbol{V}_{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.13.m13" intent=":literal"><semantics><msub><mi>ùëΩ</mi><mi>y</mi></msub><annotation encoding="application/x-tex">\boldsymbol{V}_{y}</annotation></semantics></math> corresponding to the desired novel viewpoint <math alttext="c_{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.14.m14" intent=":literal"><semantics><msub><mi>c</mi><mi>y</mi></msub><annotation encoding="application/x-tex">c_{y}</annotation></semantics></math> by drawing from a conditional probability distribution:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A5.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\boldsymbol{V}_{y}\sim P_{f}\left(\boldsymbol{V}_{y}\mid\boldsymbol{V}_{x},c_{x},i_{x},c_{y},i_{y}\right)" class="ltx_Math" display="inline" id="S3.E1.m1" intent=":literal"><semantics><mrow><msub><mi>ùëΩ</mi><mi>y</mi></msub><mo>‚àº</mo><mrow><msub><mi>P</mi><mi>f</mi></msub><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo>(</mo><mrow><msub><mi>ùëΩ</mi><mi>y</mi></msub><mo>‚à£</mo><mrow><msub><mi>ùëΩ</mi><mi>x</mi></msub><mo>,</mo><msub><mi>c</mi><mi>x</mi></msub><mo>,</mo><msub><mi>i</mi><mi>x</mi></msub><mo>,</mo><msub><mi>c</mi><mi>y</mi></msub><mo>,</mo><msub><mi>i</mi><mi>y</mi></msub></mrow></mrow><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle\boldsymbol{V}_{y}\sim P_{f}\left(\boldsymbol{V}_{y}\mid\boldsymbol{V}_{x},c_{x},i_{x},c_{y},i_{y}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.18">The camera parameters <math alttext="c_{x},i_{x},c_{y},i_{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.15.m1" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>x</mi></msub><mo>,</mo><msub><mi>i</mi><mi>x</mi></msub><mo>,</mo><msub><mi>c</mi><mi>y</mi></msub><mo>,</mo><msub><mi>i</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">c_{x},i_{x},c_{y},i_{y}</annotation></semantics></math> represent two sequences of fully specified 6-DoF <math alttext="SE(3)" class="ltx_Math" display="inline" id="S3.SS1.p1.16.m2" intent=":literal"><semantics><mrow><mi>S</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi>E</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mrow><mo stretchy="false">(</mo><mn>3</mn><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">SE(3)</annotation></semantics></math> camera poses, ensuring that the task setting is both general and unambiguous.
Moreover, there should be some spatial overlap in content between the two perspectives <math alttext="c_{x}" class="ltx_Math" display="inline" id="S3.SS1.p1.17.m3" intent=":literal"><semantics><msub><mi>c</mi><mi>x</mi></msub><annotation encoding="application/x-tex">c_{x}</annotation></semantics></math> and <math alttext="c_{y}" class="ltx_Math" display="inline" id="S3.SS1.p1.18.m4" intent=":literal"><semantics><msub><mi>c</mi><mi>y</mi></msub><annotation encoding="application/x-tex">c_{y}</annotation></semantics></math> (even if this overlap is temporally asynchronous), otherwise the conditioning signal loses its relevance.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Architecture</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The task described above involves (1) <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.1">synthesis of high-dimensional data</em> in the form of multiple images, and (2) <em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.2">considerable uncertainty handling</em> mainly due to occlusion and ambiguous object motion.
These requirements are challenging, but naturally lend themselves to being implemented using the generative video paradigm.
Hence, we adopted Cosmos¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib33" title="World simulation with video foundation models for physical ai">35</a>]</cite>, a latent diffusion transformer, as our underlying base representation, due to its efficiency, high-quality pretrained checkpoints, and flexible conditioning mechanisms (<em class="ltx_emph ltx_font_italic" id="S3.SS2.p1.1.3">e.g</em>.<span class="ltx_text" id="S3.SS2.p1.1.4"></span> text, image, and video).</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Our proposed AnyView architecture, illustrated in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.F2" title="Figure 2 ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">2</span></a>, prioritizes simplicity and scalability.
Contrary to most state-of-the-art methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a>]</cite>, we elect to not use warped depth maps as explicit conditioning due to the risk of compounding errors due to depth estimation, and instead rely solely on a learned implicit representation as our rendering mechanism.
The reasoning behind this decision is so that we can achieve <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.1">unbounded</span> dynamic view synthesis, that does not require substantial overlap between target and generated videos, thus allowing for more extreme camera motion.
We explore this property in our proposed AnyViewBench, outperforming baselines that rely on explicit reprojection mechanisms.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.5">In order to make AnyView 4D-aware and controllable, we feed information about both viewpoints into the network in a structured yet straightforward way. To account for the possible lack of an absolute frame of reference, all camera poses are expected to exist relative to the target viewpoint <math alttext="c_{y,0}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1" intent=":literal"><semantics><msub><mi>c</mi><mrow><mi>y</mi><mo>,</mo><mn>0</mn></mrow></msub><annotation encoding="application/x-tex">c_{y,0}</annotation></semantics></math> at time <math alttext="t=0" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2" intent=":literal"><semantics><mrow><mi>t</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">t=0</annotation></semantics></math>.
In other words, <math alttext="c_{y}" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3" intent=":literal"><semantics><msub><mi>c</mi><mi>y</mi></msub><annotation encoding="application/x-tex">c_{y}</annotation></semantics></math> always starts at the ‚Äúorigin‚Äù, with <math alttext="c_{y,0}=I_{4\times 4}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4" intent=":literal"><semantics><mrow><msub><mi>c</mi><mrow><mi>y</mi><mo>,</mo><mn>0</mn></mrow></msub><mo>=</mo><msub><mi>I</mi><mrow><mn>4</mn><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>4</mn></mrow></msub></mrow><annotation encoding="application/x-tex">c_{y,0}=I_{4\times 4}</annotation></semantics></math> mapping to the identity matrix.
If this is not the case, a simple change of coordinate system can be done by applying <math alttext="\tilde{c}=c\cdot c_{y,0}^{-1}" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5" intent=":literal"><semantics><mrow><mover accent="true"><mi>c</mi><mo>~</mo></mover><mo>=</mo><mrow><mi>c</mi><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><msubsup><mi>c</mi><mrow><mi>y</mi><mo>,</mo><mn>0</mn></mrow><mrow><mo>‚àí</mo><mn>1</mn></mrow></msubsup></mrow></mrow><annotation encoding="application/x-tex">\tilde{c}=c\cdot c_{y,0}^{-1}</annotation></semantics></math>, assuming the camera-to-world extrinsics convention.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.13">First, the given video <math alttext="\boldsymbol{V}_{x}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1" intent=":literal"><semantics><msub><mi>ùëΩ</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\boldsymbol{V}_{x}</annotation></semantics></math> is compressed into a latent space by a video tokenizer to become <math alttext="\boldsymbol{v}_{x}\in\mathbb{R}^{t\times h\times w\times d}" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2" intent=":literal"><semantics><mrow><msub><mi>ùíó</mi><mi>x</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>t</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_{x}\in\mathbb{R}^{t\times h\times w\times d}</annotation></semantics></math>, with spatiotemporal downsampling ratios <math alttext="T/t=4" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3" intent=":literal"><semantics><mrow><mrow><mi>T</mi><mo>/</mo><mi>t</mi></mrow><mo>=</mo><mn>4</mn></mrow><annotation encoding="application/x-tex">T/t=4</annotation></semantics></math> and <math alttext="H/h=W/w=8" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4" intent=":literal"><semantics><mrow><mrow><mi>H</mi><mo>/</mo><mi>h</mi></mrow><mo>=</mo><mrow><mi>W</mi><mo>/</mo><mi>w</mi></mrow><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">H/h=W/w=8</annotation></semantics></math>, and embedding size <math alttext="d=16" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m5" intent=":literal"><semantics><mrow><mi>d</mi><mo>=</mo><mn>16</mn></mrow><annotation encoding="application/x-tex">d=16</annotation></semantics></math>. We then encode all camera parameters <math alttext="c_{x},i_{x},c_{y},i_{y}" class="ltx_Math" display="inline" id="S3.SS2.p4.6.m6" intent=":literal"><semantics><mrow><msub><mi>c</mi><mi>x</mi></msub><mo>,</mo><msub><mi>i</mi><mi>x</mi></msub><mo>,</mo><msub><mi>c</mi><mi>y</mi></msub><mo>,</mo><msub><mi>i</mi><mi>y</mi></msub></mrow><annotation encoding="application/x-tex">c_{x},i_{x},c_{y},i_{y}</annotation></semantics></math> into a unified <em class="ltx_emph ltx_font_italic" id="S3.SS2.p4.13.1">Pl√ºcker representation</em> <math alttext="\boldsymbol{P}=(\boldsymbol{r},\boldsymbol{m})" class="ltx_Math" display="inline" id="S3.SS2.p4.7.m7" intent=":literal"><semantics><mrow><mi>ùë∑</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><mi>ùíì</mi><mo>,</mo><mi>ùíé</mi><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{P}=(\boldsymbol{r},\boldsymbol{m})</annotation></semantics></math>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib42" title="Methods of algebraic geometry, volume 1">21</a>]</cite>, which combines extrinsics and intrinsics into a dense map containing per-pixel <em class="ltx_emph ltx_font_italic" id="S3.SS2.p4.13.2">ray</em> vectors <math alttext="\boldsymbol{r}" class="ltx_Math" display="inline" id="S3.SS2.p4.8.m8" intent=":literal"><semantics><mi>ùíì</mi><annotation encoding="application/x-tex">\boldsymbol{r}</annotation></semantics></math> and <em class="ltx_emph ltx_font_italic" id="S3.SS2.p4.13.3">moment</em> vectors <math alttext="\boldsymbol{m}=\boldsymbol{r}\times\boldsymbol{o}" class="ltx_Math" display="inline" id="S3.SS2.p4.9.m9" intent=":literal"><semantics><mrow><mi>ùíé</mi><mo>=</mo><mrow><mi>ùíì</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>ùíê</mi></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{m}=\boldsymbol{r}\times\boldsymbol{o}</annotation></semantics></math>.
This results in two quantities <math alttext="\boldsymbol{P}_{x},\boldsymbol{P}_{y}\in\mathbb{R}^{T\times H\times W\times 6}" class="ltx_Math" display="inline" id="S3.SS2.p4.10.m10" intent=":literal"><semantics><mrow><mrow><msub><mi>ùë∑</mi><mi>x</mi></msub><mo>,</mo><msub><mi>ùë∑</mi><mi>y</mi></msub></mrow><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>W</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>6</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{P}_{x},\boldsymbol{P}_{y}\in\mathbb{R}^{T\times H\times W\times 6}</annotation></semantics></math>, which are tensors with the same dimensionality as a 6-channel video, or two 3-channel videos. We can therefore separately tokenize the rays <math alttext="\boldsymbol{r}\in\mathbb{R}^{T\times H\times W\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p4.11.m11" intent=":literal"><semantics><mrow><mi>ùíì</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>W</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{r}\in\mathbb{R}^{T\times H\times W\times 3}</annotation></semantics></math> and moments <math alttext="\boldsymbol{m}\in\mathbb{R}^{T\times H\times W\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p4.12.m12" intent=":literal"><semantics><mrow><mi>ùíé</mi><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>W</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{m}\in\mathbb{R}^{T\times H\times W\times 3}</annotation></semantics></math> (shown as alternating columns in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.F2" title="Figure 2 ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">2</span></a>) the same way as before into <math alttext="\boldsymbol{p}_{x},\boldsymbol{p}_{y}\in\mathbb{R}^{t\times h\times w\times 2d}" class="ltx_Math" display="inline" id="S3.SS2.p4.13.m13" intent=":literal"><semantics><mrow><mrow><msub><mi>ùíë</mi><mi>x</mi></msub><mo>,</mo><msub><mi>ùíë</mi><mi>y</mi></msub></mrow><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mrow><mi>t</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>2</mn></mrow><mo lspace="0em" rspace="0em">‚Äã</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{p}_{x},\boldsymbol{p}_{y}\in\mathbb{R}^{t\times h\times w\times 2d}</annotation></semantics></math>. An interesting property of using Pl√ºcker maps instead of direct camera conditioning¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite> is the natural handling of non-pinhole camera models, since the dense 3D ray vectors directly capture camera intrinsics in a general, non-parametric way.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.5">Because latent RGB and Pl√ºcker tokens from each viewpoint contain information pertaining to the same spatiotemporal region, we merge them via concatenation along the channel dimension, while keeping tokens from separate viewpoints separate.
Since there are two viewpoints in total, this results in a sequence of <math alttext="2\cdot t\cdot h\cdot w" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1" intent=":literal"><semantics><mrow><mn>2</mn><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>t</mi><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>h</mi><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">2\cdot t\cdot h\cdot w</annotation></semantics></math> tokens, each of length <math alttext="3\cdot d" class="ltx_Math" display="inline" id="S3.SS2.p5.2.m2" intent=":literal"><semantics><mrow><mn>3</mn><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">3\cdot d</annotation></semantics></math>.
All tokens are tagged with rotary positional embeddings¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib47" title="RoFormer: enhanced transformer with rotary position embedding">47</a>]</cite>, as well as a unique per-view embedding.
After completing all self-attention and cross-attention blocks, the output sequence is the latent RGB video <math alttext="\boldsymbol{v}_{y}\in\mathbb{R}^{t\times h\times w\times d}" class="ltx_Math" display="inline" id="S3.SS2.p5.3.m3" intent=":literal"><semantics><mrow><msub><mi>ùíó</mi><mi>y</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>t</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>h</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>w</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{v}_{y}\in\mathbb{R}^{t\times h\times w\times d}</annotation></semantics></math>. During training, these latent tokens are supervised with an <math alttext="\mathcal{L}_{2}" class="ltx_Math" display="inline" id="S3.SS2.p5.4.m4" intent=":literal"><semantics><msub><mi class="ltx_font_mathcaligraphic">‚Ñí</mi><mn>2</mn></msub><annotation encoding="application/x-tex">\mathcal{L}_{2}</annotation></semantics></math> loss, and during inference, they are iteratively denoised before finally being decoded into a generated video <math alttext="\boldsymbol{V}_{y}\in\mathbb{R}^{T\times H\times W\times 3}" class="ltx_Math" display="inline" id="S3.SS2.p5.5.m5" intent=":literal"><semantics><mrow><msub><mi>ùëΩ</mi><mi>y</mi></msub><mo>‚àà</mo><msup><mi>‚Ñù</mi><mrow><mi>T</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>H</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mi>W</mi><mo lspace="0.222em" rspace="0.222em">√ó</mo><mn>3</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\boldsymbol{V}_{y}\in\mathbb{R}^{T\times H\times W\times 3}</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.16">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.16.17.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T1.16.17.1.1" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.1.1" style="font-size:67%;">Benchmark</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.2" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.2.1" style="font-size:67%;">S/R</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.3.1" style="font-size:67%;">Domain</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.4.1" style="font-size:67%;">Type</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.5.1" style="font-size:67%;"># Cameras</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.6.1" style="font-size:67%;"># Episodes</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.7.1" style="font-size:67%;">Resolution</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.8.1" style="font-size:67%;">Input Cam.</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.9.1" style="font-size:67%;">Align Start</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.16.17.1.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.17.1.10.1" style="font-size:67%;">Gen. Type</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.16.18.2">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="10" id="S3.T1.16.18.2.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text ltx_font_bold" id="S3.T1.16.18.2.1.1" style="font-size:67%;">Narrow</span><span class="ltx_text" id="S3.T1.16.18.2.1.2" style="font-size:67%;"> dynamic view synthesis</span>
</th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T1.1.1.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.1.1.2.1" style="font-size:67%;">DyCheck iPhone¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.1.1.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib26" title="Dynamic novel-view synthesis: a reality check">13</a><span class="ltx_text" id="S3.T1.1.1.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.1.1.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.1.1.4.1" style="font-size:67%;">Hand-Object</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.1.1.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.1.1.6.1" style="font-size:67%;">2 ‚Äì 3</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.1.1.7.1" style="font-size:67%;">5 / 7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.1.1.1.1" style="font-size:67%;">Variable @ </span><math alttext="288\times 384" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">288</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">384</mn></mrow><annotation encoding="application/x-tex">288\times 384</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.1.1.8.1" style="font-size:67%;">Moving</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.1.1.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.1.1.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.1.1.10.1" style="font-size:67%;">0-shot overall</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.2.2.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.2.2.2.1" style="font-size:67%;">Kubric-4D (gradual)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.2.2.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a><span class="ltx_text" id="S3.T1.2.2.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.2.2.3.1" style="font-size:67%;">Sim</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.2.2.4.1" style="font-size:67%;">Multi-Object</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.2.2.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.2.2.6.1" style="font-size:67%;">16 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.2.2.7.1" style="font-size:67%;">20 / 100</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.2.2.1.1" style="font-size:67%;">13 frames @ </span><math alttext="384\times 256" class="ltx_Math" display="inline" id="S3.T1.2.2.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">256</mn></mrow><annotation encoding="application/x-tex">384\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.2.2.8.1" style="font-size:67%;">Fixed</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.2.2.9.1" style="font-size:67%;">Yes</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.2.2.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.2.2.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.3.3.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.3.3.2.1" style="font-size:67%;">ParDom-4D (gradual)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.3.3.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib25" title="Parallel domain">39</a><span class="ltx_text" id="S3.T1.3.3.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.3.3.3.1" style="font-size:67%;">Sim</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.3.3.4.1" style="font-size:67%;">Driving</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.3.3.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.3.3.6.1" style="font-size:67%;">19 (16 exo + 3 ego)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.3.3.7.1" style="font-size:67%;">20 / 61</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.3.3.1.1" style="font-size:67%;">13 frames @ </span><math alttext="384\times 256" class="ltx_Math" display="inline" id="S3.T1.3.3.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">256</mn></mrow><annotation encoding="application/x-tex">384\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.3.3.8.1" style="font-size:67%;">Variable</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.3.3.9.1" style="font-size:67%;">Yes</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.3.3.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.3.3.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.16.19.3">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="10" id="S3.T1.16.19.3.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.16.19.3.1.1" style="font-size:67%;">AnyViewBench<span class="ltx_text ltx_font_upright" id="S3.T1.16.19.3.1.1.1">: In-distribution extreme</span></span><span class="ltx_text" id="S3.T1.16.19.3.1.2" style="font-size:67%;"> dynamic view synthesis</span>
</th>
</tr>
<tr class="ltx_tr" id="S3.T1.4.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T1.4.4.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.4.4.2.1" style="font-size:67%;">DROID (ID)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.4.4.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib16" title="DROID: a large-scale in-the-wild robot manipulation dataset">28</a><span class="ltx_text" id="S3.T1.4.4.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.4.4.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.4.4.4.1" style="font-size:67%;">Robotics</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.4.4.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.4.4.6.1" style="font-size:67%;">2 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.4.4.7.1" style="font-size:67%;">64 / 3,301</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.4.4.1.1" style="font-size:67%;">29 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="S3.T1.4.4.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.4.4.8.1" style="font-size:67%;">Fixed</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.4.4.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.4.4.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.4.4.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.5.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.5.5.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.5.5.2.1" style="font-size:67%;">Ego-Exo4D (ID)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.5.5.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib22" title="Ego-exo4d: understanding skilled human activity from first- and third-person perspectives">14</a><span class="ltx_text" id="S3.T1.5.5.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.5.5.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.5.5.4.1" style="font-size:67%;">Human Activity</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.5.5.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.5.5.6.1" style="font-size:67%;">4 ‚Äì 5 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.5.5.7.1" style="font-size:67%;">64 / 276</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.5.5.1.1" style="font-size:67%;">41 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="S3.T1.5.5.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.5.5.8.1" style="font-size:67%;">Fixed</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.5.5.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.5.5.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.5.5.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.6.6.2" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.2.1" style="font-size:67%;">LBM</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.3.1" style="font-size:67%;">Sim + Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.4.1" style="font-size:67%;">Robotics</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.6.1" style="font-size:67%;">2 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.7.1" style="font-size:67%;">64 / 5,988</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.6.6.1.1" style="font-size:67%;">41 frames @ </span><math alttext="336\times 256" class="ltx_Math" display="inline" id="S3.T1.6.6.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">336</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">256</mn></mrow><annotation encoding="application/x-tex">336\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.8.1" style="font-size:67%;">Fixed</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.6.6.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.6.6.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.7.7.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.7.7.2.1" style="font-size:67%;">Kubric-4D (direct)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.7.7.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a><span class="ltx_text" id="S3.T1.7.7.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.7.7.3.1" style="font-size:67%;">Sim</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.7.7.4.1" style="font-size:67%;">Multi-Object</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.7.7.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.7.7.6.1" style="font-size:67%;">16 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.7.7.7.1" style="font-size:67%;">20 / 100</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.7.7.1.1" style="font-size:67%;">13 frames @ </span><math alttext="384\times 256" class="ltx_Math" display="inline" id="S3.T1.7.7.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">256</mn></mrow><annotation encoding="application/x-tex">384\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.7.7.8.1" style="font-size:67%;">Fixed</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.7.7.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.7.7.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.7.7.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.8.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.8.8.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.8.8.2.1" style="font-size:67%;">Kubric-5D¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.8.8.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a><span class="ltx_text" id="S3.T1.8.8.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.8.8.3.1" style="font-size:67%;">Sim</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.8.8.4.1" style="font-size:67%;">Multi-Object</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.8.8.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.8.8.6.1" style="font-size:67%;">16 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.8.8.7.1" style="font-size:67%;">64 / 200</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.8.8.1.1" style="font-size:67%;">41 frames @ </span><math alttext="384\times 256" class="ltx_Math" display="inline" id="S3.T1.8.8.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">256</mn></mrow><annotation encoding="application/x-tex">384\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.8.8.8.1" style="font-size:67%;">Variable</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.8.8.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.8.8.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.8.8.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.9.9">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.9.9.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.9.9.2.1" style="font-size:67%;">Lyft¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.9.9.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib12" title="One thousand and one hours: self-driving motion prediction dataset.">22</a><span class="ltx_text" id="S3.T1.9.9.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.9.9.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.9.9.4.1" style="font-size:67%;">Driving</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.9.9.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.9.9.6.1" style="font-size:67%;">6 (ego only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.9.9.7.1" style="font-size:67%;">64 / 436</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.9.9.1.1" style="font-size:67%;">41 frames @ </span><math alttext="384\times 320" class="ltx_Math" display="inline" id="S3.T1.9.9.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">320</mn></mrow><annotation encoding="application/x-tex">384\times 320</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.9.9.8.1" style="font-size:67%;">Variable</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.9.9.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.9.9.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.9.9.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.10">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.10.10.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.10.10.2.1" style="font-size:67%;">ParDom-4D (direct)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.10.10.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib25" title="Parallel domain">39</a><span class="ltx_text" id="S3.T1.10.10.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.10.10.3.1" style="font-size:67%;">Sim</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.10.10.4.1" style="font-size:67%;">Driving</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.10.10.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.10.10.6.1" style="font-size:67%;">19 (16 exo + 3 ego)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.10.10.7.1" style="font-size:67%;">20 / 61</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.10.10.1.1" style="font-size:67%;">13 frames @ </span><math alttext="384\times 256" class="ltx_Math" display="inline" id="S3.T1.10.10.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">256</mn></mrow><annotation encoding="application/x-tex">384\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.10.10.8.1" style="font-size:67%;">Variable</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.10.10.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.10.10.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.10.10.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.11.11">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.11.11.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.11.11.2.1" style="font-size:67%;">Waymo¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.11.11.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib15" title="Scalability in perception for autonomous driving: waymo open dataset">48</a><span class="ltx_text" id="S3.T1.11.11.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.11.11.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.11.11.4.1" style="font-size:67%;">Driving</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.11.11.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.11.11.6.1" style="font-size:67%;">5 (ego only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.11.11.7.1" style="font-size:67%;">64 / 202</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.11.11.1.1" style="font-size:67%;">41 frames @ </span><math alttext="384\times\{176,256\}" class="ltx_Math" display="inline" id="S3.T1.11.11.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mrow><mo maxsize="0.670em" minsize="0.670em">{</mo><mn mathsize="0.670em">176</mn><mo mathsize="0.670em">,</mo><mn mathsize="0.670em">256</mn><mo maxsize="0.670em" minsize="0.670em">}</mo></mrow></mrow><annotation encoding="application/x-tex">384\times\{176,256\}</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.11.11.8.1" style="font-size:67%;">Variable</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.11.11.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.11.11.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.11.11.10.1" style="font-size:67%;">In-dist.</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.16.20.4">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="10" id="S3.T1.16.20.4.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.T1.16.20.4.1.1" style="font-size:67%;">AnyViewBench<span class="ltx_text ltx_font_upright" id="S3.T1.16.20.4.1.1.1">: Zero-shot extreme</span></span><span class="ltx_text" id="S3.T1.16.20.4.1.2" style="font-size:67%;"> dynamic view synthesis</span>
</th>
</tr>
<tr class="ltx_tr" id="S3.T1.12.12">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T1.12.12.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.12.12.2.1" style="font-size:67%;">Argoverse¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.12.12.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib13" title="Argoverse 2: next generation datasets for self-driving perception and forecasting">56</a><span class="ltx_text" id="S3.T1.12.12.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.12.12.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.12.12.4.1" style="font-size:67%;">Driving</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.12.12.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.12.12.6.1" style="font-size:67%;">7 (ego only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.12.12.7.1" style="font-size:67%;">64 / 1,042</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.12.12.1.1" style="font-size:67%;">41 frames @ </span><math alttext="\{288,384\}\times\{288,384\}" class="ltx_Math" display="inline" id="S3.T1.12.12.1.m1" intent=":literal"><semantics><mrow><mrow><mo maxsize="0.670em" minsize="0.670em">{</mo><mn mathsize="0.670em">288</mn><mo mathsize="0.670em">,</mo><mn mathsize="0.670em">384</mn><mo maxsize="0.670em" minsize="0.670em" rspace="0.055em">}</mo></mrow><mo mathsize="0.670em" rspace="0.222em">√ó</mo><mrow><mo maxsize="0.670em" minsize="0.670em">{</mo><mn mathsize="0.670em">288</mn><mo mathsize="0.670em">,</mo><mn mathsize="0.670em">384</mn><mo maxsize="0.670em" minsize="0.670em">}</mo></mrow></mrow><annotation encoding="application/x-tex">\{288,384\}\times\{288,384\}</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.12.12.8.1" style="font-size:67%;">Variable</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.12.12.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.T1.12.12.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.12.12.10.1" style="font-size:67%;">0-shot dataset</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.13.13">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.13.13.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.13.13.2.1" style="font-size:67%;">AssemblyHands¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.13.13.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib44" title="AssemblyHands: towards egocentric activity understanding via 3d hand pose estimation">37</a><span class="ltx_text" id="S3.T1.13.13.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.13.13.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.13.13.4.1" style="font-size:67%;">Hand-Object</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.13.13.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.13.13.6.1" style="font-size:67%;">8 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.13.13.7.1" style="font-size:67%;">20 / 20</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.13.13.1.1" style="font-size:67%;">41 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="S3.T1.13.13.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.13.13.8.1" style="font-size:67%;">Fixed</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.13.13.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.13.13.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.13.13.10.1" style="font-size:67%;">0-shot domain</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.14.14">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.14.14.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.14.14.2.1" style="font-size:67%;">DDAD¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.14.14.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib14" title="3D packing for self-supervised monocular depth estimation">16</a><span class="ltx_text" id="S3.T1.14.14.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.14.14.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.14.14.4.1" style="font-size:67%;">Driving</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.14.14.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.14.14.6.1" style="font-size:67%;">6 (ego only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.14.14.7.1" style="font-size:67%;">64 / 200</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.14.14.1.1" style="font-size:67%;">41 frames @ </span><math alttext="384\times 240" class="ltx_Math" display="inline" id="S3.T1.14.14.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">240</mn></mrow><annotation encoding="application/x-tex">384\times 240</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.14.14.8.1" style="font-size:67%;">Variable</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.14.14.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.14.14.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.14.14.10.1" style="font-size:67%;">0-shot dataset</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.15.15">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row" id="S3.T1.15.15.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.15.15.2.1" style="font-size:67%;">DROID (OOD)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.15.15.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib16" title="DROID: a large-scale in-the-wild robot manipulation dataset">28</a><span class="ltx_text" id="S3.T1.15.15.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.15.15.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.15.15.4.1" style="font-size:67%;">Robotics</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.15.15.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.15.15.6.1" style="font-size:67%;">2 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.15.15.7.1" style="font-size:67%;">64 / 252</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.15.15.1.1" style="font-size:67%;">29 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="S3.T1.15.15.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.15.15.8.1" style="font-size:67%;">Fixed</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.15.15.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.T1.15.15.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.15.15.10.1" style="font-size:67%;">0-shot station</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.16.16">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.16.16.2" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.16.16.2.1" style="font-size:67%;">Ego-Exo4D (OOD)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T1.16.16.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib22" title="Ego-exo4d: understanding skilled human activity from first- and third-person perspectives">14</a><span class="ltx_text" id="S3.T1.16.16.2.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.3" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.16.3.1" style="font-size:67%;">Real</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.4" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.16.4.1" style="font-size:67%;">Human Activity</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.5" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.16.5.1" style="font-size:67%;">4D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.6" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.16.6.1" style="font-size:67%;">4 ‚Äì 5 (exo only)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.7" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.16.7.1" style="font-size:67%;">64 / 408</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.1" style="padding:-0.4pt 1.0pt;">
<span class="ltx_text" id="S3.T1.16.16.1.1" style="font-size:67%;">41 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="S3.T1.16.16.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.8" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.16.8.1" style="font-size:67%;">Fixed</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.9" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.16.9.1" style="font-size:67%;">No</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T1.16.16.10" style="padding:-0.4pt 1.0pt;"><span class="ltx_text" id="S3.T1.16.16.10.1" style="font-size:67%;">0-shot activity/site</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:67%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.46.1.1" style="font-size:135%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.47.2" style="font-size:135%;">
<span class="ltx_text ltx_font_bold" id="S3.T1.47.2.1">Testing datasets.</span>
We evaluate on several benchmarks that cover both <em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.2">narrow</em> and <em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.3">extreme</em> settings.
We define <span class="ltx_text ltx_font_bold" id="S3.T1.47.2.4">AnyViewBench</span> as a multi-faceted benchmark focusing on the latter category, setting a new standard for consistent dynamic view synthesis in challenging settings.
Test splits are capped at 64 per dataset by means of uniform subsampling.
<em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.5">Exo(centric)</em> refers to inward-facing viewpoints from cameras outside the scene, whereas <em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.6">ego(centric)</em> refers to outward-facing viewpoints close to the subject of interest (<em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.7">e.g</em>.<span class="ltx_text" id="S3.T1.47.2.8"></span> a vehicle).
<em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.9">Input Cam.</em> refers to what the camera characteristic of the observed video (<em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.10">i.e</em>.<span class="ltx_text" id="S3.T1.47.2.11"></span> static vs dynamic).
<em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.12">Align Start</em> specifies whether the output trajectory starts at the same initial frame as the input.
The rightmost column (<em class="ltx_emph ltx_font_italic" id="S3.T1.47.2.13">Generalization Type</em>) qualitatively denotes how large the distribution shift is relative to the AnyView training mixture.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Datasets</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Because AnyView does not rely on any explicit conditioning mechanism (<em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.1">e.g</em>.<span class="ltx_text" id="S3.SS3.p1.1.2"></span> intermediate depth maps) to facilitate the rendering of novel viewpoints, it must learn implicit multi-view geometry as well as a wide range of appearance priors, to be able to inpaint and outpaint potentially large unobserved portions of the scene.
In order to train such a generalist spatiotemporal representation capable of handling multiple domains, we combined 12 different 4D datasets into our unified training pipeline.
Among them is <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.1.3">Kubric-5D</em>, our newly introduced variation of Kubric-4D¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a>]</cite> that vastly increases the diversity of camera trajectories. We classify our training datasets into four distinct quadrants: <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.4">Robotics</span>, <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.5">Driving</span>, <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.6">3D</span>, and <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.1.7">Other</span>.
A visual overview is illustrated in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.F3" title="Figure 3 ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3</span></a>, and more details are provided in the supplementary material. To the best of our knowledge, this data mixture covers a significant portion of publicly available multi-view video datasets.
We leave the inclusion of additional 4D datasets¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib39" title="PointOdyssey: a large-scale synthetic dataset for long-term point tracking">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib40" title="Infinite photorealistic worlds using procedural generation">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib41" title="Infinigen indoors: photorealistic indoor scenes using procedural generation">42</a>]</cite> to future work.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Implementation Details</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.6">We train AnyView for 40,000 iterations on 64 NVIDIA H200 GPUs at a global batch size of 512.
We apply curriculum learning with increasing resolution:
first we train at a largest image dimension of <math alttext="384" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1" intent=":literal"><semantics><mn>384</mn><annotation encoding="application/x-tex">384</annotation></semantics></math> for 30,000 steps, before finetuning at a largest image dimension of <math alttext="576" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2" intent=":literal"><semantics><mn>576</mn><annotation encoding="application/x-tex">576</annotation></semantics></math>.
The initial learning rate is <math alttext="5\cdot 10^{-5}" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3" intent=":literal"><semantics><mrow><mn>5</mn><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><msup><mn>10</mn><mrow><mo>‚àí</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5\cdot 10^{-5}</annotation></semantics></math>, and drops smoothly to <math alttext="1\cdot 10^{-5}" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4" intent=":literal"><semantics><mrow><mn>1</mn><mo lspace="0.222em" rspace="0.222em">‚ãÖ</mo><msup><mn>10</mn><mrow><mo>‚àí</mo><mn>5</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1\cdot 10^{-5}</annotation></semantics></math> according to a cosine schedule.
All experiments are performed with the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p1.6.1">Cosmos-Predict2-2B-Video2World</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib36" title="Cosmos-predict2: diffusion-based world foundation models for physics-aware image and video generation">36</a>]</cite> model,
starting from their pretrained network, which has around 2 billion parameters.
We disable language conditioning, since it is not relevant to our task setting.
Furthermore, in order to properly combine datasets with varying physical scales, we divide the translation vectors of all cameras <math alttext="\{c_{x},c_{y}\}" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5" intent=":literal"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>c</mi><mi>x</mi></msub><mo>,</mo><msub><mi>c</mi><mi>y</mi></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{c_{x},c_{y}\}</annotation></semantics></math> by a carefully chosen per-dataset normalization constant to ensure the resulting Pl√ºcker values always fall in the range <math alttext="[-1,1]" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6" intent=":literal"><semantics><mrow><mo stretchy="false">[</mo><mrow><mo>‚àí</mo><mn>1</mn></mrow><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-1,1]</annotation></semantics></math>, occasionally clipping pixels as needed.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Challenges</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">As the field is evolving, many existing DVS benchmarks are beginning to <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">lack difficulty</span>,
containing scenes with minimal object motion and modest camera transformations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib26" title="Dynamic novel-view synthesis: a reality check">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib64" title="Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib65" title="Deep 3d mask volume for view synthesis of dynamic scenes">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite>.
Qualitative results are often demonstrated on camera trajectories with rotational variations of only about <math alttext="10-30" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1" intent=":literal"><semantics><mrow><mn>10</mn><mo>‚àí</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">10-30</annotation></semantics></math> degrees relative to the center of the scene¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib4" title="Shape of motion: 4d reconstruction from a single video">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib11" title="ReCapture: generative video camera controls for user-provided videos using masked video fine-tuning">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib27" title="ReCamMaster: camera-controlled generative rendering from a single video">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib43" title="Dynamic view synthesis as an inverse problem">66</a>]</cite>.
Consequently, the heavy lifting of inpainting large occlusions is mostly avoided, making it unclear to which extent these models learn robust, multi-view consistent 4D representations.
These efforts are further complicated by a <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">lack of standardization</span>, which
can be partially attributed to the inherent complexity of DVS: merely describing the task is insufficient to define a path towards practical execution.
Design choices often left in the dark include but are not limited to: video resolution, number of frames, camera controllability and conventions, used frames of reference, the space of possible camera transformations, and so on.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Benchmarks</h3>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.3.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.3.4.1" style="font-size:80%;">Method</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.1.1.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.1.1.1.1" style="font-size:80%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1" intent=":literal"><semantics><mo mathsize="0.800em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.2.2.2" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.2.2.2.1" style="font-size:80%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1" intent=":literal"><semantics><mo mathsize="0.800em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.3.3" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.3.3.1" style="font-size:80%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.3.3.3.m1" intent=":literal"><semantics><mo mathsize="0.800em" stretchy="false">‚Üì</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.3.5" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.3.5.1" style="font-size:80%;">TTO</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.3.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.3.6.1" style="font-size:80%;">Aux.</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.3.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.3.7.1" style="font-size:80%;">0-shot</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.3.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.3.8.1" style="font-size:80%;">Time</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.4.1">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="5" id="S4.T2.3.4.1.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.3.4.1.1.1" style="font-size:80%;">DyCheck iPhone</span><span class="ltx_text" id="S4.T2.3.4.1.1.2" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.4.1.1.3.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib26" title="Dynamic novel-view synthesis: a reality check">13</a><span class="ltx_text" id="S4.T2.3.4.1.1.4.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.4.1.2" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.4.1.3" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.4.1.4" style="padding:-1.05pt 1.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.5.2">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.5.2.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.5.2.1.1" style="font-size:80%;">ShapeOfMotion</span><sup class="ltx_sup" id="S4.T2.3.5.2.1.2"><span class="ltx_text" id="S4.T2.3.5.2.1.2.1" style="font-size:80%;">‚Ä†</span></sup><span class="ltx_text" id="S4.T2.3.5.2.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.5.2.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib4" title="Shape of motion: 4d reconstruction from a single video">53</a><span class="ltx_text" id="S4.T2.3.5.2.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.5.2.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.5.2.2.1" style="font-size:80%;">16.72</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.5.2.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.5.2.3.1" style="font-size:80%;">0.630</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.5.2.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.5.2.4.1" style="font-size:80%;">0.450</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.5.2.5" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.5.2.5.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.5.2.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.5.2.6.1" style="font-size:80%;">D P T</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.5.2.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.5.2.7.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.5.2.8" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.5.2.8.1" style="font-size:80%;position:relative; bottom:1.7pt;--ltx-fg-color:#CC0000;">~</span><span class="ltx_text" id="S4.T2.3.5.2.8.2" style="font-size:80%;--ltx-fg-color:#CC0000;">1h</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.6.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.6.3.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.6.3.1.1" style="font-size:80%;">CogNVS</span><sup class="ltx_sup" id="S4.T2.3.6.3.1.2"><span class="ltx_text" id="S4.T2.3.6.3.1.2.1" style="font-size:80%;">‚Ä†</span></sup><span class="ltx_text" id="S4.T2.3.6.3.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.6.3.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a><span class="ltx_text" id="S4.T2.3.6.3.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.3.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.6.3.2.1" style="font-size:80%;">16.94</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.3.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.6.3.3.1" style="font-size:80%;">0.449</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.6.3.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.6.3.4.1" style="font-size:80%;">0.598</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.6.3.5" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.6.3.5.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.6.3.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.6.3.6.1" style="font-size:80%;">D(GT)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.6.3.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.6.3.7.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.6.3.8" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.6.3.8.1" style="font-size:80%;position:relative; bottom:1.7pt;--ltx-fg-color:#CC0000;">~</span><span class="ltx_text" id="S4.T2.3.6.3.8.2" style="font-size:80%;--ltx-fg-color:#CC0000;">1h</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.7.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.7.4.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.7.4.1.1" style="font-size:80%;">GEN3C</span><sup class="ltx_sup" id="S4.T2.3.7.4.1.2"><span class="ltx_text" id="S4.T2.3.7.4.1.2.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S4.T2.3.7.4.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.7.4.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a><span class="ltx_text" id="S4.T2.3.7.4.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.7.4.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.7.4.2.1" style="font-size:80%;">10.13</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.7.4.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.7.4.3.1" style="font-size:80%;">0.175</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.7.4.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.7.4.4.1" style="font-size:80%;">0.695</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.7.4.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.7.4.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.7.4.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.7.4.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.7.4.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.7.4.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.7.4.8.1" style="font-size:80%;--ltx-fg-color:#CC4D00;"><span class="ltx_text" id="S4.T2.3.7.4.8.1.1" style="position:relative; bottom:1.7pt;">~</span>15m</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.8.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.8.5.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.8.5.1.1" style="font-size:80%;">TrajAttn</span><sup class="ltx_sup" id="S4.T2.3.8.5.1.2"><span class="ltx_text" id="S4.T2.3.8.5.1.2.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S4.T2.3.8.5.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.8.5.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a><span class="ltx_text" id="S4.T2.3.8.5.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.5.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.8.5.2.1" style="font-size:80%;">10.30</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.5.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.8.5.3.1" style="font-size:80%;">0.181</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.8.5.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.8.5.4.1" style="font-size:80%;">0.682</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" id="S4.T2.3.8.5.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.8.5.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.8.5.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.8.5.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.8.5.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.8.5.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.8.5.8.1" style="font-size:80%;--ltx-fg-color:#CC4D00;"><span class="ltx_text" id="S4.T2.3.8.5.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10m</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.9.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.9.6.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.9.6.1.1" style="font-size:80%;">TrajCrafter</span><sup class="ltx_sup" id="S4.T2.3.9.6.1.2"><span class="ltx_text" id="S4.T2.3.9.6.1.2.1" style="font-size:80%;">‚Ä†</span></sup><span class="ltx_text" id="S4.T2.3.9.6.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.9.6.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a><span class="ltx_text" id="S4.T2.3.9.6.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.9.6.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.9.6.2.1" style="font-size:80%;">14.24</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.9.6.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.9.6.3.1" style="font-size:80%;">0.417</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.9.6.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.9.6.4.1" style="font-size:80%;">0.519</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.9.6.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.9.6.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.9.6.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.9.6.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.9.6.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.9.6.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.9.6.8.1" style="font-size:80%;--ltx-fg-color:#008000;"><span class="ltx_text" id="S4.T2.3.9.6.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.10.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.10.7.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.10.7.1.1" style="font-size:80%;">GCD</span><sup class="ltx_sup" id="S4.T2.3.10.7.1.2"><span class="ltx_text" id="S4.T2.3.10.7.1.2.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S4.T2.3.10.7.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.10.7.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a><span class="ltx_text" id="S4.T2.3.10.7.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.10.7.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.10.7.2.1" style="font-size:80%;">11.43</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.10.7.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.10.7.3.1" style="font-size:80%;">0.247</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.10.7.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.10.7.4.1" style="font-size:80%;">0.728</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.10.7.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.10.7.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.10.7.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.10.7.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.10.7.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.10.7.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.10.7.8.1" style="font-size:80%;--ltx-fg-color:#008000;"><span class="ltx_text" id="S4.T2.3.10.7.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.11.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.11.8.1" style="padding:-1.05pt 1.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.11.8.1.1" style="font-size:80%;">AnyView</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.8.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.11.8.2.1" style="font-size:80%;">13.47</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.8.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.11.8.3.1" style="font-size:80%;">0.295</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.11.8.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.11.8.4.1" style="font-size:80%;">0.550</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" id="S4.T2.3.11.8.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.11.8.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.11.8.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.11.8.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.11.8.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.11.8.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.11.8.8.1" style="font-size:80%;--ltx-fg-color:#008000;"><span class="ltx_text" id="S4.T2.3.11.8.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.12.9">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="5" id="S4.T2.3.12.9.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.3.12.9.1.1" style="font-size:80%;">Kubric-4D</span><span class="ltx_text" id="S4.T2.3.12.9.1.2" style="font-size:80%;"> (gradual)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.12.9.1.3.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a><span class="ltx_text" id="S4.T2.3.12.9.1.4.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.12.9.2" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.12.9.3" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.12.9.4" style="padding:-1.05pt 1.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.13.10">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.13.10.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.13.10.1.1" style="font-size:80%;">CogNVS</span><sup class="ltx_sup" id="S4.T2.3.13.10.1.2"><span class="ltx_text" id="S4.T2.3.13.10.1.2.1" style="font-size:80%;">‚Ä†</span></sup><span class="ltx_text" id="S4.T2.3.13.10.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.13.10.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a><span class="ltx_text" id="S4.T2.3.13.10.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.13.10.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.13.10.2.1" style="font-size:80%;">22.63</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.13.10.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.13.10.3.1" style="font-size:80%;">0.760</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.13.10.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.13.10.4.1" style="font-size:80%;">0.232</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.13.10.5" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.13.10.5.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.13.10.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.13.10.6.1" style="font-size:80%;">D(GT)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.13.10.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.13.10.7.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.13.10.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.13.10.8.1" style="font-size:80%;--ltx-fg-color:#CC0000;"> <span class="ltx_text" id="S4.T2.3.13.10.8.1.1" style="position:relative; bottom:1.7pt;">~</span>1h</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.14.11">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.14.11.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.14.11.1.1" style="font-size:80%;">ReCapture</span><sup class="ltx_sup" id="S4.T2.3.14.11.1.2"><span class="ltx_text" id="S4.T2.3.14.11.1.2.1" style="font-size:80%;">‚Ä†</span></sup><span class="ltx_text" id="S4.T2.3.14.11.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.14.11.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib11" title="ReCapture: generative video camera controls for user-provided videos using masked video fine-tuning">69</a><span class="ltx_text" id="S4.T2.3.14.11.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.11.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.14.11.2.1" style="font-size:80%;">20.92</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.11.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.14.11.3.1" style="font-size:80%;">0.596</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.14.11.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.14.11.4.1" style="font-size:80%;">0.402</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.14.11.5" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.14.11.5.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.14.11.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.14.11.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.14.11.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.14.11.7.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.14.11.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.14.11.8.1" style="font-size:80%;--ltx-fg-color:#CC4D00;"><span class="ltx_text" id="S4.T2.3.14.11.8.1.1" style="position:relative; bottom:1.7pt;">~</span>15m</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.15.12">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.15.12.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.15.12.1.1" style="font-size:80%;">GEN3C</span><sup class="ltx_sup" id="S4.T2.3.15.12.1.2"><span class="ltx_text" id="S4.T2.3.15.12.1.2.1" style="font-size:80%;">‚Ä†</span></sup><span class="ltx_text" id="S4.T2.3.15.12.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.15.12.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a><span class="ltx_text" id="S4.T2.3.15.12.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.15.12.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.15.12.2.1" style="font-size:80%;">19.41</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.15.12.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.15.12.3.1" style="font-size:80%;">0.630</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.15.12.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.15.12.4.1" style="font-size:80%;">0.290</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.15.12.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.15.12.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.15.12.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.15.12.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.15.12.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.15.12.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.15.12.8.1" style="font-size:80%;--ltx-fg-color:#CC4D00;"><span class="ltx_text" id="S4.T2.3.15.12.8.1.1" style="position:relative; bottom:1.7pt;">~</span>15m</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.16.13">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.16.13.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.16.13.1.1" style="font-size:80%;">TrajAttn</span><sup class="ltx_sup" id="S4.T2.3.16.13.1.2"><span class="ltx_text" id="S4.T2.3.16.13.1.2.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S4.T2.3.16.13.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.16.13.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a><span class="ltx_text" id="S4.T2.3.16.13.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.13.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.16.13.2.1" style="font-size:80%;">15.73</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.13.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.16.13.3.1" style="font-size:80%;">0.404</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.16.13.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.16.13.4.1" style="font-size:80%;">0.530</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" id="S4.T2.3.16.13.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.16.13.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.16.13.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.16.13.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.16.13.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.16.13.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.16.13.8.1" style="font-size:80%;--ltx-fg-color:#808000;"><span class="ltx_text" id="S4.T2.3.16.13.8.1.1" style="position:relative; bottom:1.7pt;">~</span>5m</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.17.14">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.17.14.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.17.14.1.1" style="font-size:80%;">TrajCrafter</span><sup class="ltx_sup" id="S4.T2.3.17.14.1.2"><span class="ltx_text" id="S4.T2.3.17.14.1.2.1" style="font-size:80%;">‚Ä°</span></sup><span class="ltx_text" id="S4.T2.3.17.14.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.17.14.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a><span class="ltx_text" id="S4.T2.3.17.14.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.14.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.17.14.2.1" style="font-size:80%;">20.93</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.14.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.17.14.3.1" style="font-size:80%;">0.730</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.17.14.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.17.14.4.1" style="font-size:80%;">0.257</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.17.14.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.17.14.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.17.14.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.17.14.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.17.14.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.17.14.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.17.14.8.1" style="font-size:80%;--ltx-fg-color:#008000;"><span class="ltx_text" id="S4.T2.3.17.14.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.18.15">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.18.15.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.18.15.1.1" style="font-size:80%;">GCD</span><sup class="ltx_sup" id="S4.T2.3.18.15.1.2"><span class="ltx_text" id="S4.T2.3.18.15.1.2.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S4.T2.3.18.15.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.18.15.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a><span class="ltx_text" id="S4.T2.3.18.15.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.18.15.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.18.15.2.1" style="font-size:80%;">20.42</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.18.15.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.18.15.3.1" style="font-size:80%;">0.581</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.18.15.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.18.15.4.1" style="font-size:80%;">0.405</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.18.15.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.18.15.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.18.15.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.18.15.7" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.18.15.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.18.15.8.1" style="font-size:80%;--ltx-fg-color:#008000;"> <span class="ltx_text" id="S4.T2.3.18.15.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.19.16">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.19.16.1" style="padding:-1.05pt 1.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.19.16.1.1" style="font-size:80%;">AnyView</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.19.16.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.19.16.2.1" style="font-size:80%;">21.21</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.19.16.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.19.16.3.1" style="font-size:80%;">0.644</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.19.16.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.19.16.4.1" style="font-size:80%;">0.358</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" id="S4.T2.3.19.16.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.19.16.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.19.16.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" id="S4.T2.3.19.16.7" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.19.16.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.19.16.8.1" style="font-size:80%;--ltx-fg-color:#008000;"><span class="ltx_text" id="S4.T2.3.19.16.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.20.17">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="5" id="S4.T2.3.20.17.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.3.20.17.1.1" style="font-size:80%;">ParDom-4D</span><span class="ltx_text" id="S4.T2.3.20.17.1.2" style="font-size:80%;"> (gradual)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.20.17.1.3.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib25" title="Parallel domain">39</a><span class="ltx_text" id="S4.T2.3.20.17.1.4.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.20.17.2" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.20.17.3" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T2.3.20.17.4" style="padding:-1.05pt 1.0pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.21.18">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.3.21.18.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.21.18.1.1" style="font-size:80%;">CogNVS</span><sup class="ltx_sup" id="S4.T2.3.21.18.1.2"><span class="ltx_text" id="S4.T2.3.21.18.1.2.1" style="font-size:80%;">‚Ä†</span></sup><span class="ltx_text" id="S4.T2.3.21.18.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.21.18.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a><span class="ltx_text" id="S4.T2.3.21.18.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.21.18.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.21.18.2.1" style="font-size:80%;">24.34</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.21.18.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.21.18.3.1" style="font-size:80%;">0.797</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.21.18.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.21.18.4.1" style="font-size:80%;">0.302</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.21.18.5" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.21.18.5.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.21.18.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.21.18.6.1" style="font-size:80%;">D(GT)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.3.21.18.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.21.18.7.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.3.21.18.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.21.18.8.1" style="font-size:80%;--ltx-fg-color:#CC0000;"> <span class="ltx_text" id="S4.T2.3.21.18.8.1.1" style="position:relative; bottom:1.7pt;">~</span>1h</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.22.19">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.22.19.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.22.19.1.1" style="font-size:80%;">GEN3C</span><sup class="ltx_sup" id="S4.T2.3.22.19.1.2"><span class="ltx_text" id="S4.T2.3.22.19.1.2.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S4.T2.3.22.19.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.22.19.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a><span class="ltx_text" id="S4.T2.3.22.19.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.22.19.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.22.19.2.1" style="font-size:80%;">18.40</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.22.19.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.22.19.3.1" style="font-size:80%;">0.528</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.22.19.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.22.19.4.1" style="font-size:80%;">0.542</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.22.19.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.22.19.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.22.19.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.22.19.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.22.19.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.22.19.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.22.19.8.1" style="font-size:80%;--ltx-fg-color:#CC4D00;"><span class="ltx_text" id="S4.T2.3.22.19.8.1.1" style="position:relative; bottom:1.7pt;">~</span>15m</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.23.20">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.3.23.20.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.23.20.1.1" style="font-size:80%;">TrajAttn</span><sup class="ltx_sup" id="S4.T2.3.23.20.1.2"><span class="ltx_text" id="S4.T2.3.23.20.1.2.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S4.T2.3.23.20.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.23.20.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a><span class="ltx_text" id="S4.T2.3.23.20.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.23.20.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.23.20.2.1" style="font-size:80%;">20.03</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.23.20.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.23.20.3.1" style="font-size:80%;">0.566</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.23.20.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.23.20.4.1" style="font-size:80%;">0.518</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" id="S4.T2.3.23.20.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.23.20.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.23.20.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T2.3.23.20.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.23.20.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.3.23.20.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.23.20.8.1" style="font-size:80%;--ltx-fg-color:#808000;"><span class="ltx_text" id="S4.T2.3.23.20.8.1.1" style="position:relative; bottom:1.7pt;">~</span>5m</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.24.21">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.24.21.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.24.21.1.1" style="font-size:80%;">TrajCrafter</span><sup class="ltx_sup" id="S4.T2.3.24.21.1.2"><span class="ltx_text" id="S4.T2.3.24.21.1.2.1" style="font-size:80%;">‚Ä°</span></sup><span class="ltx_text" id="S4.T2.3.24.21.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.24.21.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a><span class="ltx_text" id="S4.T2.3.24.21.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.24.21.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.24.21.2.1" style="font-size:80%;">21.46</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.24.21.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.24.21.3.1" style="font-size:80%;">0.719</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.24.21.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.24.21.4.1" style="font-size:80%;">0.342</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.24.21.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.24.21.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.24.21.6.1" style="font-size:80%;">D</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.24.21.7" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.24.21.7.1" style="font-size:80%;">‚úì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.24.21.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.24.21.8.1" style="font-size:80%;--ltx-fg-color:#008000;"> <span class="ltx_text" id="S4.T2.3.24.21.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.25.22">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.3.25.22.1" style="padding:-1.05pt 1.0pt;">
<span class="ltx_text" id="S4.T2.3.25.22.1.1" style="font-size:80%;">GCD</span><sup class="ltx_sup" id="S4.T2.3.25.22.1.2"><span class="ltx_text" id="S4.T2.3.25.22.1.2.1" style="font-size:80%;">*</span></sup><span class="ltx_text" id="S4.T2.3.25.22.1.3" style="font-size:80%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.3.25.22.1.4.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a><span class="ltx_text" id="S4.T2.3.25.22.1.5.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.25.22.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.25.22.2.1" style="font-size:80%;">24.75</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.25.22.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.25.22.3.1" style="font-size:80%;">0.724</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.25.22.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.25.22.4.1" style="font-size:80%;">0.355</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.25.22.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.25.22.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.25.22.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r ltx_border_t" id="S4.T2.3.25.22.7" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.3.25.22.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.25.22.8.1" style="font-size:80%;--ltx-fg-color:#008000;"><span class="ltx_text" id="S4.T2.3.25.22.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.3.26.23">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.3.26.23.1" style="padding:-1.05pt 1.0pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.3.26.23.1.1" style="font-size:80%;">AnyView</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.26.23.2" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.26.23.2.1" style="font-size:80%;">26.29</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.26.23.3" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.26.23.3.1" style="font-size:80%;">0.758</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.26.23.4" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.26.23.4.1" style="font-size:80%;">0.320</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_r" id="S4.T2.3.26.23.5" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.3.26.23.6" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.26.23.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_r" id="S4.T2.3.26.23.7" style="padding:-1.05pt 1.0pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.3.26.23.8" style="padding:-1.05pt 1.0pt;"><span class="ltx_text" id="S4.T2.3.26.23.8.1" style="font-size:80%;--ltx-fg-color:#008000;"><span class="ltx_text" id="S4.T2.3.26.23.8.1.1" style="position:relative; bottom:1.7pt;">~</span>10s</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.11.1.1" style="font-size:113%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.12.2" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="S4.T2.12.2.1">Narrow DVS results.</span>
We compare against several state-of-the-art baselines, including those using test-time optimization (TTO) and auxiliary networks (Aux.) for depth (D), poses (P), and/or 2D point tracks (T).
<span class="ltx_text" id="S4.T2.12.2.2" style="font-size:83%;">The inference runtime assumes that a video was not observed before, and thus includes a test-time optimization stage if present.
Results reported by:
<sup class="ltx_sup" id="S4.T2.12.2.2.1">‚Ä†</sup>original paper;
<sup class="ltx_sup" id="S4.T2.12.2.2.2">‚Ä°</sup>another paper (cited);
<sup class="ltx_sup" id="S4.T2.12.2.2.3">*</sup>computed by us.
</span>
</span></figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.18">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.18.19.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.18.19.1.1" rowspan="2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.19.1.1.1" style="font-size:67%;">Dataset</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.18.19.1.2" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.19.1.2.1" style="font-size:67%;">GCD¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.19.1.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a><span class="ltx_text" id="S4.T3.18.19.1.2.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.18.19.1.3" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.19.1.3.1" style="font-size:67%;">TrajAttn¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.19.1.3.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a><span class="ltx_text" id="S4.T3.18.19.1.3.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.18.19.1.4" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.19.1.4.1" style="font-size:67%;">GEN3C¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.19.1.4.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a><span class="ltx_text" id="S4.T3.18.19.1.4.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.18.19.1.5" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.19.1.5.1" style="font-size:67%;">TrajCrafter¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.19.1.5.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a><span class="ltx_text" id="S4.T3.18.19.1.5.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" colspan="3" id="S4.T3.18.19.1.6" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.19.1.6.1" style="font-size:67%;">CogNVS¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.19.1.6.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a><span class="ltx_text" id="S4.T3.18.19.1.6.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" colspan="3" id="S4.T3.18.19.1.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.19.1.7.1" style="font-size:67%;">Ours (AnyView)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.18">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.1.1.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.1.1.1.1" style="font-size:67%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.2.2.2" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.2.2.2.1" style="font-size:67%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.3.3.3" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.3.3.3.1" style="font-size:67%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üì</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.4.4.4" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.4.4.4.1" style="font-size:67%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.5.5" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.5.5.5.1" style="font-size:67%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.5.5.5.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.6.6.6" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.6.6.6.1" style="font-size:67%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.6.6.6.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üì</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.7.7.7" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.7.7.7.1" style="font-size:67%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.7.7.7.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.8.8.8" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.8.8.8.1" style="font-size:67%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.8.8.8.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.9.9.9" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.9.9.9.1" style="font-size:67%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.9.9.9.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üì</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.10.10.10" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.10.10.10.1" style="font-size:67%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.10.10.10.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.11.11.11" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.11.11.11.1" style="font-size:67%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.11.11.11.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.12.12.12" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.12.12.12.1" style="font-size:67%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.12.12.12.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üì</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.13.13.13" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.13.13.13.1" style="font-size:67%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.13.13.13.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.14.14.14" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.14.14.14.1" style="font-size:67%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.14.14.14.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.15.15.15" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.15.15.15.1" style="font-size:67%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.15.15.15.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üì</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.16.16.16" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.16.16.16.1" style="font-size:67%;">PSNR</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.16.16.16.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.17.17.17" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.17.17.17.1" style="font-size:67%;">SSIM</span><math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.17.17.17.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üë</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.18.18" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.18.18.1" style="font-size:67%;">LPIPS</span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.18.18.18.m1" intent=":literal"><semantics><mo mathsize="0.670em" stretchy="false">‚Üì</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.20.2">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="12" id="S4.T3.18.20.2.1" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.18.20.2.1.1" style="font-size:67%;">In-distribution</span></th>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.20.2.2" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.20.2.3" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.20.2.4" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.20.2.5" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.20.2.6" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.20.2.7" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.20.2.8" style="padding:-0.05pt 0.8pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.21.3">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.18.21.3.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.21.3.1.1" style="font-size:67%;">DROID (ID)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.21.3.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib16" title="DROID: a large-scale in-the-wild robot manipulation dataset">28</a><span class="ltx_text" id="S4.T3.18.21.3.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.2.1" style="font-size:67%;">10.18</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.3.1" style="font-size:67%;">0.255</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.21.3.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.4.1" style="font-size:67%;">0.688</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.5.1" style="font-size:67%;">9.93</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.6.1" style="font-size:67%;">0.247</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.21.3.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.7.1" style="font-size:67%;">0.671</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.8.1" style="font-size:67%;">9.62</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.9.1" style="font-size:67%;">0.211</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.21.3.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.10.1" style="font-size:67%;">0.666</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.11.1" style="font-size:67%;">10.45</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.12.1" style="font-size:67%;">0.257</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.21.3.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.13.1" style="font-size:67%;">0.620</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.14.1" style="font-size:67%;">9.44</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.15.1" style="font-size:67%;">0.281</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.21.3.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.16.1" style="font-size:67%;">0.634</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.17.1" style="font-size:67%;">14.47</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.18.1" style="font-size:67%;">0.445</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.21.3.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.21.3.19.1" style="font-size:67%;">0.472</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.22.4">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.22.4.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.22.4.1.1" style="font-size:67%;">EgoExo4D (ID)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.22.4.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib22" title="Ego-exo4d: understanding skilled human activity from first- and third-person perspectives">14</a><span class="ltx_text" id="S4.T3.18.22.4.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.2.1" style="font-size:67%;">12.10</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.3.1" style="font-size:67%;">0.255</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.22.4.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.4.1" style="font-size:67%;">0.670</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.5.1" style="font-size:67%;">11.64</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.6.1" style="font-size:67%;">0.234</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.22.4.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.7.1" style="font-size:67%;">0.646</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.8.1" style="font-size:67%;">11.69</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.9.1" style="font-size:67%;">0.222</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.22.4.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.10.1" style="font-size:67%;">0.643</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.11.1" style="font-size:67%;">11.33</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.12.1" style="font-size:67%;">0.195</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.22.4.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.13.1" style="font-size:67%;">0.642</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.14.1" style="font-size:67%;">10.80</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.15.1" style="font-size:67%;">0.241</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.22.4.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.16.1" style="font-size:67%;">0.670</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.17.1" style="font-size:67%;">18.14</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.18.1" style="font-size:67%;">0.531</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.22.4.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.22.4.19.1" style="font-size:67%;">0.379</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.23.5">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.23.5.1" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.1.1" style="font-size:67%;">LBM</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.2.1" style="font-size:67%;">12.59</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.3.1" style="font-size:67%;">0.398</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.23.5.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.4.1" style="font-size:67%;">0.694</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.5.1" style="font-size:67%;">13.47</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.6.1" style="font-size:67%;">0.421</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.23.5.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.7.1" style="font-size:67%;">0.614</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.8.1" style="font-size:67%;">13.48</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.9.1" style="font-size:67%;">0.449</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.23.5.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.10.1" style="font-size:67%;">0.581</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.11.1" style="font-size:67%;">13.68</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.12.1" style="font-size:67%;">0.447</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.23.5.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.13.1" style="font-size:67%;">0.537</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.14.1" style="font-size:67%;">13.30</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.15.1" style="font-size:67%;">0.453</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.23.5.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.16.1" style="font-size:67%;">0.548</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.17.1" style="font-size:67%;">17.94</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.18.1" style="font-size:67%;">0.649</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.23.5.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.23.5.19.1" style="font-size:67%;">0.348</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.24.6">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.24.6.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.24.6.1.1" style="font-size:67%;">Kubric-4D (direct)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.24.6.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a><span class="ltx_text" id="S4.T3.18.24.6.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.2.1" style="font-size:67%;">17.57</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.3.1" style="font-size:67%;">0.477</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.24.6.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.4.1" style="font-size:67%;">0.512</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.5.1" style="font-size:67%;">13.47</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.6.1" style="font-size:67%;">0.320</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.24.6.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.7.1" style="font-size:67%;">0.607</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.8.1" style="font-size:67%;">13.59</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.9.1" style="font-size:67%;">0.341</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.24.6.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.10.1" style="font-size:67%;">0.599</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.11.1" style="font-size:67%;">14.14</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.12.1" style="font-size:67%;">0.294</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.24.6.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.13.1" style="font-size:67%;">0.592</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.14.1" style="font-size:67%;">12.55</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.15.1" style="font-size:67%;">0.329</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.24.6.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.16.1" style="font-size:67%;">0.601</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.17.1" style="font-size:67%;">18.38</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.18.1" style="font-size:67%;">0.441</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.24.6.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.24.6.19.1" style="font-size:67%;">0.362</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.25.7">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.25.7.1" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.1.1" style="font-size:67%;">Kubric-5D</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.2.1" style="font-size:67%;">13.83</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.3.1" style="font-size:67%;">0.391</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.25.7.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.4.1" style="font-size:67%;">0.644</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.5.1" style="font-size:67%;">13.25</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.6.1" style="font-size:67%;">0.360</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.25.7.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.7.1" style="font-size:67%;">0.628</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.8.1" style="font-size:67%;">13.10</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.9.1" style="font-size:67%;">0.327</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.25.7.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.10.1" style="font-size:67%;">0.627</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.11.1" style="font-size:67%;">13.30</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.12.1" style="font-size:67%;">0.287</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.25.7.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.13.1" style="font-size:67%;">0.625</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.14.1" style="font-size:67%;">12.18</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.15.1" style="font-size:67%;">0.318</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.25.7.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.16.1" style="font-size:67%;">0.643</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.17.1" style="font-size:67%;">17.18</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.18.1" style="font-size:67%;">0.468</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.25.7.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.25.7.19.1" style="font-size:67%;">0.428</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.26.8">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.26.8.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.26.8.1.1" style="font-size:67%;">Lyft¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.26.8.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib12" title="One thousand and one hours: self-driving motion prediction dataset.">22</a><span class="ltx_text" id="S4.T3.18.26.8.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.2.1" style="font-size:67%;">8.72</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.3.1" style="font-size:67%;">0.335</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.26.8.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.4.1" style="font-size:67%;">0.697</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.5.1" style="font-size:67%;">8.33</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.6.1" style="font-size:67%;">0.273</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.26.8.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.7.1" style="font-size:67%;">0.621</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.8.1" style="font-size:67%;">8.43</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.9.1" style="font-size:67%;">0.286</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.26.8.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.10.1" style="font-size:67%;">0.634</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.11.1" style="font-size:67%;">8.73</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.12.1" style="font-size:67%;">0.251</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.26.8.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.13.1" style="font-size:67%;">0.621</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.14.1" style="font-size:67%;">8.34</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.15.1" style="font-size:67%;">0.319</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.26.8.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.16.1" style="font-size:67%;">0.628</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.17.1" style="font-size:67%;">15.37</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.18.1" style="font-size:67%;">0.564</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.26.8.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.26.8.19.1" style="font-size:67%;">0.371</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.27.9">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.27.9.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.27.9.1.1" style="font-size:67%;">ParDom-4D (direct)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.27.9.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib25" title="Parallel domain">39</a><span class="ltx_text" id="S4.T3.18.27.9.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.2.1" style="font-size:67%;">22.67</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.3.1" style="font-size:67%;">0.656</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.27.9.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.4.1" style="font-size:67%;">0.457</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.5.1" style="font-size:67%;">16.91</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.6.1" style="font-size:67%;">0.445</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.27.9.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.7.1" style="font-size:67%;">0.610</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.8.1" style="font-size:67%;">16.64</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.9.1" style="font-size:67%;">0.478</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.27.9.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.10.1" style="font-size:67%;">0.590</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.11.1" style="font-size:67%;">18.23</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.12.1" style="font-size:67%;">0.475</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.27.9.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.13.1" style="font-size:67%;">0.586</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.14.1" style="font-size:67%;">18.36</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.15.1" style="font-size:67%;">0.499</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.27.9.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.16.1" style="font-size:67%;">0.564</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.17.1" style="font-size:67%;">24.26</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.18.1" style="font-size:67%;">0.688</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.27.9.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.27.9.19.1" style="font-size:67%;">0.351</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.28.10">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.28.10.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.28.10.1.1" style="font-size:67%;">Waymo¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.28.10.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib15" title="Scalability in perception for autonomous driving: waymo open dataset">48</a><span class="ltx_text" id="S4.T3.18.28.10.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.2.1" style="font-size:67%;">12.93</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.3.1" style="font-size:67%;">0.393</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.28.10.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.4.1" style="font-size:67%;">0.647</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.5.1" style="font-size:67%;">12.55</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.6.1" style="font-size:67%;">0.350</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.28.10.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.7.1" style="font-size:67%;">0.613</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.8.1" style="font-size:67%;">12.98</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.9.1" style="font-size:67%;">0.350</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.28.10.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.10.1" style="font-size:67%;">0.600</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.11.1" style="font-size:67%;">12.66</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.12.1" style="font-size:67%;">0.312</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.28.10.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.13.1" style="font-size:67%;">0.593</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.14.1" style="font-size:67%;">13.27</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.15.1" style="font-size:67%;">0.377</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.28.10.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.16.1" style="font-size:67%;">0.594</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.17.1" style="font-size:67%;">16.52</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.18.1" style="font-size:67%;">0.477</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.28.10.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.28.10.19.1" style="font-size:67%;">0.480</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.29.11">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.18.29.11.1" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.1.1" style="font-size:67%;">Average</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.18.29.11.2.1" style="font-size:67%;">13.95</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.18.29.11.3.1" style="font-size:67%;">0.400</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.18.29.11.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.4.1" style="font-size:67%;">0.623</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.5.1" style="font-size:67%;">12.44</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.6.1" style="font-size:67%;">0.331</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.18.29.11.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.7.1" style="font-size:67%;">0.626</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.8.1" style="font-size:67%;">12.44</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.9.1" style="font-size:67%;">0.333</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.18.29.11.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.10.1" style="font-size:67%;">0.617</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.11.1" style="font-size:67%;">12.82</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.12.1" style="font-size:67%;">0.315</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.18.29.11.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.18.29.11.13.1" style="font-size:67%;">0.602</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.14.1" style="font-size:67%;">12.28</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.15.1" style="font-size:67%;">0.352</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.18.29.11.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.29.11.16.1" style="font-size:67%;">0.610</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.18.29.11.17.1" style="font-size:67%;">17.78</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.18.29.11.18.1" style="font-size:67%;">0.533</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.18.29.11.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.18.29.11.19.1" style="font-size:67%;">0.399</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.30.12">
<th class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="12" id="S4.T3.18.30.12.1" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.18.30.12.1.1" style="font-size:67%;">Zero-shot</span></th>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.30.12.2" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.30.12.3" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.30.12.4" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.30.12.5" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.30.12.6" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.30.12.7" style="padding:-0.05pt 0.8pt;"></td>
<td class="ltx_td ltx_border_tt" id="S4.T3.18.30.12.8" style="padding:-0.05pt 0.8pt;"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.31.13">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.18.31.13.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.31.13.1.1" style="font-size:67%;">Argoverse¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.31.13.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib13" title="Argoverse 2: next generation datasets for self-driving perception and forecasting">56</a><span class="ltx_text" id="S4.T3.18.31.13.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.2.1" style="font-size:67%;">11.45</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.3.1" style="font-size:67%;">0.403</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.31.13.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.4.1" style="font-size:67%;">0.682</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.5.1" style="font-size:67%;">10.62</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.6.1" style="font-size:67%;">0.317</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.31.13.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.7.1" style="font-size:67%;">0.665</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.8.1" style="font-size:67%;">10.52</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.9.1" style="font-size:67%;">0.319</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.31.13.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.10.1" style="font-size:67%;">0.680</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.11.1" style="font-size:67%;">10.67</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.12.1" style="font-size:67%;">0.325</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.31.13.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.13.1" style="font-size:67%;">0.621</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.14.1" style="font-size:67%;">10.76</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.15.1" style="font-size:67%;">0.360</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.18.31.13.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.16.1" style="font-size:67%;">0.610</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.17.1" style="font-size:67%;">12.38</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.18.1" style="font-size:67%;">0.399</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.18.31.13.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.31.13.19.1" style="font-size:67%;">0.587</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.32.14">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.32.14.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.32.14.1.1" style="font-size:67%;">AssemblyHands¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.32.14.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib44" title="AssemblyHands: towards egocentric activity understanding via 3d hand pose estimation">37</a><span class="ltx_text" id="S4.T3.18.32.14.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.2.1" style="font-size:67%;">9.77</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.3.1" style="font-size:67%;">0.262</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.32.14.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.4.1" style="font-size:67%;">0.759</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.5.1" style="font-size:67%;">9.97</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.6.1" style="font-size:67%;">0.266</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.32.14.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.7.1" style="font-size:67%;">0.736</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.8.1" style="font-size:67%;">9.86</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.9.1" style="font-size:67%;">0.237</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.32.14.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.10.1" style="font-size:67%;">0.732</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.11.1" style="font-size:67%;">11.45</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.12.1" style="font-size:67%;">0.248</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.32.14.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.13.1" style="font-size:67%;">0.701</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.14.1" style="font-size:67%;">9.93</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.15.1" style="font-size:67%;">0.281</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.32.14.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.16.1" style="font-size:67%;">0.701</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.17.1" style="font-size:67%;">11.21</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.18.1" style="font-size:67%;">0.291</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.32.14.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.32.14.19.1" style="font-size:67%;">0.688</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.33.15">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.33.15.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.33.15.1.1" style="font-size:67%;">DDAD¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.33.15.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib14" title="3D packing for self-supervised monocular depth estimation">16</a><span class="ltx_text" id="S4.T3.18.33.15.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.2.1" style="font-size:67%;">9.81</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.3.1" style="font-size:67%;">0.278</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.33.15.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.4.1" style="font-size:67%;">0.660</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.5.1" style="font-size:67%;">9.16</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.6.1" style="font-size:67%;">0.244</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.33.15.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.7.1" style="font-size:67%;">0.620</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.8.1" style="font-size:67%;">9.35</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.9.1" style="font-size:67%;">0.259</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.33.15.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.10.1" style="font-size:67%;">0.600</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.11.1" style="font-size:67%;">10.73</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.12.1" style="font-size:67%;">0.300</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.33.15.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.13.1" style="font-size:67%;">0.558</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.14.1" style="font-size:67%;">10.81</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.15.1" style="font-size:67%;">0.355</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.33.15.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.16.1" style="font-size:67%;">0.572</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.17.1" style="font-size:67%;">11.44</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.18.1" style="font-size:67%;">0.341</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.33.15.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.33.15.19.1" style="font-size:67%;">0.519</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.34.16">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.34.16.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.34.16.1.1" style="font-size:67%;">DROID (OOD)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.34.16.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib16" title="DROID: a large-scale in-the-wild robot manipulation dataset">28</a><span class="ltx_text" id="S4.T3.18.34.16.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.2.1" style="font-size:67%;">11.81</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.3.1" style="font-size:67%;">0.315</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.34.16.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.4.1" style="font-size:67%;">0.690</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.5.1" style="font-size:67%;">10.83</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.6.1" style="font-size:67%;">0.320</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.34.16.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.7.1" style="font-size:67%;">0.678</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.8.1" style="font-size:67%;">10.56</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.9.1" style="font-size:67%;">0.276</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.34.16.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.10.1" style="font-size:67%;">0.674</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.11.1" style="font-size:67%;">11.37</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.12.1" style="font-size:67%;">0.339</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.34.16.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.13.1" style="font-size:67%;">0.614</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.14.1" style="font-size:67%;">10.48</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.15.1" style="font-size:67%;">0.358</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.34.16.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.16.1" style="font-size:67%;">0.632</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.17.1" style="font-size:67%;">12.34</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.18.1" style="font-size:67%;">0.422</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.34.16.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.34.16.19.1" style="font-size:67%;">0.601</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.35.17">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.18.35.17.1" style="padding:-0.05pt 0.8pt;">
<span class="ltx_text" id="S4.T3.18.35.17.1.1" style="font-size:67%;">EgoExo4D (OOD)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.18.35.17.1.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib22" title="Ego-exo4d: understanding skilled human activity from first- and third-person perspectives">14</a><span class="ltx_text" id="S4.T3.18.35.17.1.3.2" style="font-size:67%;">]</span></cite>
</th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.2.1" style="font-size:67%;">11.98</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.3.1" style="font-size:67%;">0.239</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.35.17.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.4.1" style="font-size:67%;">0.668</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.5.1" style="font-size:67%;">11.31</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.6.1" style="font-size:67%;">0.203</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.35.17.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.7.1" style="font-size:67%;">0.653</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.8.1" style="font-size:67%;">11.40</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.9.1" style="font-size:67%;">0.193</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.35.17.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.10.1" style="font-size:67%;">0.651</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.11.1" style="font-size:67%;">11.27</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.12.1" style="font-size:67%;">0.180</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.35.17.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.13.1" style="font-size:67%;">0.647</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.14.1" style="font-size:67%;">10.52</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.15.1" style="font-size:67%;">0.227</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S4.T3.18.35.17.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.16.1" style="font-size:67%;">0.683</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.17.1" style="font-size:67%;">13.30</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.18.1" style="font-size:67%;">0.297</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.18.35.17.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.35.17.19.1" style="font-size:67%;">0.562</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.18.36.18">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.18.36.18.1" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.1.1" style="font-size:67%;">Average</span></th>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.2" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.2.1" style="font-size:67%;">10.96</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.3" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.3.1" style="font-size:67%;">0.299</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.18.36.18.4" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.4.1" style="font-size:67%;">0.692</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.5" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.5.1" style="font-size:67%;">10.38</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.6" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.6.1" style="font-size:67%;">0.270</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.18.36.18.7" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.7.1" style="font-size:67%;">0.670</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.8" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.8.1" style="font-size:67%;">10.34</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.9" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.9.1" style="font-size:67%;">0.257</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.18.36.18.10" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.10.1" style="font-size:67%;">0.667</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.11" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.18.36.18.11.1" style="font-size:67%;">11.10</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.12" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.12.1" style="font-size:67%;">0.279</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.18.36.18.13" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.18.36.18.13.1" style="font-size:67%;">0.628</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.14" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.14.1" style="font-size:67%;">10.50</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.15" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.18.36.18.15.1" style="font-size:67%;">0.316</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T3.18.36.18.16" style="padding:-0.05pt 0.8pt;"><span class="ltx_text" id="S4.T3.18.36.18.16.1" style="font-size:67%;">0.640</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.17" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.18.36.18.17.1" style="font-size:67%;">12.03</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.18" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.18.36.18.18.1" style="font-size:67%;">0.350</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T3.18.36.18.19" style="padding:-0.05pt 0.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.18.36.18.19.1" style="font-size:67%;">0.591</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:67%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.27.1.1" style="font-size:135%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.28.2" style="font-size:135%;">
<span class="ltx_text ltx_font_bold" id="S4.T3.28.2.1">Extreme DVS results (AnyViewBench).</span>
Note that <em class="ltx_emph ltx_font_italic" id="S4.T3.28.2.2">in-distribution</em> datasets are part of AnyView‚Äôs training mixture, but might be zero-shot for some of the baselines, hence we provide these results for completeness.
For qualitative comparison, please refer to Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S0.F1" title="Figure 1 ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1</span></a>.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="257" id="S4.F4.g1" src="x4.png" width="373"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="256" id="S4.F4.g2" src="x5.png" width="373"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.5.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.6.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S4.F4.6.2.1">AnyView in-domain DVS results</span>
on <span class="ltx_text ltx_font_bold" id="S4.F4.6.2.2">Kubric-4D</span> (left) and <span class="ltx_text ltx_font_bold" id="S4.F4.6.2.3">Pardom-4D</span> (right).
We show the first and last frame of each video.
The scene layout is generally preserved very well, despite drastic viewpoint changes and/or heavy occlusion from the input vantage point.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="353" id="S4.F5.g1" src="x6.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.4.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S4.F5.4.2.1">Results on DyCheck iPhone (0-shot narrow DVS).</span>
While these scenes are not highly dynamic, they do contain subtle, intricate motions and hand-object interactions. </span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We first consider three popular DVS benchmarks that can be classified as falling into the ‚Äúnarrow‚Äù regime.
Then, to address the aforementioned concerns, we propose <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.1">AnyViewBench</em>, which substantially pushes models into the more challenging ‚Äúextreme‚Äù regime.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">DyCheck iPhone (narrow DVS).</span>
The iPhone dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib26" title="Dynamic novel-view synthesis: a reality check">13</a>]</cite> is a small collection of high-quality, real-world, multi-view videos of easy-to-moderate difficulty established to measure DVS fidelity.
Following previous works¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a>]</cite>, that have pointed out that the provided camera poses are not very accurate, we compute corrected extrinsics using MoSca¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib48" title="Mosca: dynamic gaussian fusion from casual videos via 4d motion scaffolds">30</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Kubric-4D and ParDom-4D (narrow + extreme DVS).</span>
The GCD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite> paper introduced two synthetic datasets for DVS training and evaluation, based on the Kubric¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a>]</cite> and ParallelDomain¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib25" title="Parallel domain">39</a>]</cite> simulation environments.</p>
</div>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="265" id="S4.F6.g1" src="x7.png" width="373"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="265" id="S4.F6.g2" src="x8.png" width="374"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.5.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.6.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S4.F6.6.2.1">
AnyView extreme DVS results</span>
on <span class="ltx_text ltx_font_bold" id="S4.F6.6.2.2">driving</span> (left) and <span class="ltx_text ltx_font_bold" id="S4.F6.6.2.3">robotics</span> (right) benchmarks.
We show both in-domain and zero-shot results.
For driving videos, we focus on the three frontal cameras, whereas for robotics, we focus on all scene cameras.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="491" id="S4.F7.g1" src="x9.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F7.5.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S4.F7.6.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S4.F7.6.2.1">AnyView extreme DVS results on Ego-Exo4D.</span>
We show both in-domain and zero-shot results.
Note that in the zero-shot case, the background often has to be ‚Äúguessed‚Äù from the other camera viewpoint, but the inpainted regions (see <em class="ltx_emph ltx_font_italic" id="S4.F7.6.2.2">e.g</em>.<span class="ltx_text" id="S4.F7.6.2.3"></span> basketball, soccer) integrate harmoniously with the rest of the scene.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="230" id="S4.F8.sf1.g1" src="x10.png" width="315"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="236" id="S4.F8.sf2.g1" src="x11.png" width="307"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F8.3.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="S4.F8.4.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S4.F8.4.2.1">Examples of advanced reasoning within AnyView</span>, as a way to indirectly guide generation in unobserved parts of the scene.
</span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">AnyViewBench (extreme DVS).</span>
We introduce AnyViewBench, a multi-faceted benchmark that covers datasets across multiple domains (driving, robotics, and human activities), as shown in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1" title="Table 1 ‚Ä£ 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1</span></a>.
The camera motion patterns range from simple (fixed or linear) to complex (<em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.2">e.g</em>.<span class="ltx_text" id="S4.SS2.p4.1.3"></span> highly non-linear trajectories, changing intrinsics, <em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.4">etc</em>.<span class="ltx_text" id="S4.SS2.p4.1.5"></span>).
To promote rigorous evaluation, we provide synchronized videos from at least two separate viewpoints for each episode, with well-defined details such as spatial resolution and number of frames, such that ground truth metrics can be calculated in a straightforward manner.
For all <em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.6">in-distribution</em> datasets, we separate roughly <math alttext="10\%" class="ltx_Math" display="inline" id="S4.SS2.p4.1.m1" intent=":literal"><semantics><mrow><mn>10</mn><mo>%</mo></mrow><annotation encoding="application/x-tex">10\%</annotation></semantics></math> to serve as validation, and for both <em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.7">in-distribution</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.8">zero-shot</em> datasets, we curate smaller subsets to serve as official test splits.
Moreover, two DROID stations (GuptaLab, ILIAD), as well as certain EgoExo4D institutions (FAIR, NUS) and activities (CPR, Guitar), are held out to serve as <em class="ltx_emph ltx_font_italic" id="S4.SS2.p4.1.9">zero-shot</em> evaluation.
More information about AnyViewBench can be found in the supplementary material, and we will release it upon publication.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Baselines</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Most current DVS methods face key limitations: the input video must be captured from a strictly <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.1">static</em> camera¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite>, or from a strictly <em class="ltx_emph ltx_font_italic" id="S4.SS3.p1.1.2">dynamic</em> camera¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a>]</cite>, or both input and output videos must start from the same position¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib27" title="ReCamMaster: camera-controlled generative rendering from a single video">3</a>]</cite>,
or the camera controlling mechanism has limited degrees of freedom¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib27" title="ReCamMaster: camera-controlled generative rendering from a single video">3</a>]</cite>.
As a result, methods that excel in certain conditions might be incompatible with slightly different evaluation settings, hindering standardized evaluation across multiple benchmarks.
More information detailing all prior works we considered as baselines can be found in the supplementary material.
Most of these models already evaluate on at least a subset of the ‚Äúnarrow‚Äù benchmarks, but we additionally evaluate them (doing our best effort to project down to and accommodate the space of supported camera transformations as needed)
on AnyViewBench, which embodies the ‚Äúextreme‚Äù benchmarks.
ReCamMaster¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib27" title="ReCamMaster: camera-controlled generative rendering from a single video">3</a>]</cite> was not evaluated because it does not support arbitrary camera trajectories, and InverseDVS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib43" title="Dynamic view synthesis as an inverse problem">66</a>]</cite> was not evaluated because there was no working released code at the time of submission.
When evaluating baseline methods that require depth estimation to render reprojected images, we use DepthAnythingV2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib1" title="Depth anything v2">63</a>]</cite> and tune the maximum depth parameter for each dataset to achieve the best alignment between reprojected and ground truth images.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Results</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Following standard convention, we report DVS results in terms of PSNR (dB), SSIM, and LPIPS (VGG), averaged over all frames in the generated video.
Note that these metrics can only attest to how similar generated predictions are to the ground truth, but not necessarily how realistic and plausible they are when the true underlying scene cannot be fully known due to lack of overlap between viewpoints.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Quantitative results on existing narrow DVS benchmarks are reported in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2" title="Table 2 ‚Ä£ 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">2</span></a>, with qualitative results in¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.F4" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figures</span>¬†<span class="ltx_text ltx_ref_tag">4</span></a> and¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.F5" title="Figure 5 ‚Ä£ 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">5</span></a>.
For completeness, we also include metrics as reported by other papers, as well as evaluate the baselines ourselves when possible.
AnyView outperforms GCD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite>, the only baseline that does not require explicit depth estimation or reprojection, by a large margin, and compares favorably with explicit depth reprojection methods ‚Äî and those that require expensive test-time-optimization ‚Äî in most metrics. This <em class="ltx_emph ltx_font_italic" id="S4.SS4.p2.1.1">narrow</em> setting (i.e., large overlapping regions with small viewpoint changes) is particularly well-suited for such methods, since a lot of information can be directly transferred across viewpoints, and the model is tasked solely with inpainting the missing regions.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Next, we report results in <em class="ltx_emph ltx_font_italic" id="S4.SS4.p3.1.1">extreme</em> DVS setting using AnyViewBench, with quantitative results in Table¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3" title="Table 3 ‚Ä£ 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3</span></a> and illustrations in¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.F6" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figures</span>¬†<span class="ltx_text ltx_ref_tag">6</span></a> and¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.F7" title="Figure 7 ‚Ä£ 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">7</span></a>.
These scenarios are much more challenging, since they require implicit 4D understanding to ensure spatiotemporal consistency.
For example, in real-world driving, the amount of spatial overlap between neighboring cameras is generally small, meaning that when the model is prompted with generating the front-left view based solely on the front view (or vice-versa), it has to plausibly infer the majority of the scene based on little information.
However, if the ego vehicle is moving, information is able to eventually ‚Äúleak‚Äù into other views and can be propagated across the entire sequence, further limiting the space of ‚Äúcorrect‚Äù generations.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1">In the upper left scenario in Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.F6" title="Figure 6 ‚Ä£ 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">6</span></a>, the red car arriving at the intersection is predicted on the left view <em class="ltx_emph ltx_font_italic" id="S4.SS4.p4.1.1">before</em> it is visible in the input front view, showing that AnyView has learned to maintain spatiotemporal consistency, leading to improved performance in areas that otherwise would be ill-defined.
A related behavior is also observed in the left examples of Figure¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.F7" title="Figure 7 ‚Ä£ 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">7</span></a>, where AnyView leverages its foundational knowledge to infer how a basketball court or soccer field should look like from different perspectives.
Moreover, in¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.F8" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">8</span></a> we show anecdotal examples of AnyView leveraging subtle visuals cues to improve generation accuracy in unobserved areas, as evidence of advanced common sense and spatiotemporal reasoning.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">Implicitly learning these useful spatiotemporal properties in a data-driven way enables AnyView to produce more realistic and physically plausible representations of real-world scenarios compared to all baselines. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S0.F1" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figure</span>¬†<span class="ltx_text ltx_ref_tag">1</span></a>, while methods that rely on potentially inaccurate depth reprojection (<em class="ltx_emph ltx_font_italic" id="S4.SS4.p5.1.1">e.g</em>.<span class="ltx_text" id="S4.SS4.p5.1.2"></span> TrajAttn and GEN3C) struggle when starting from target poses away from input poses, AnyView successfully generates smooth, consistent target scenes regardless of camera positioning. Similarly, AnyView is able to accurately outpaint much larger unobserved portions of the scene compared to methods trained mostly for limited inpainting (<em class="ltx_emph ltx_font_italic" id="S4.SS4.p5.1.3">e.g</em>.<span class="ltx_text" id="S4.SS4.p5.1.4"></span> TrajCrafter and CogNVS).
As a consequence of these useful properties, we achieve state of the art zero-shot DVS performance on AnyViewBench, outperforming all other baseline methods by a significant margin across all considered datasets.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we propose <em class="ltx_emph ltx_font_italic" id="S5.p1.1.1">AnyView</em>, a generalist dynamic view synthesis framework targeting extreme camera displacements.
We also contribute <em class="ltx_emph ltx_font_italic" id="S5.p1.1.2">AnyViewBench</em>, a well-rounded benchmark that focuses on highly challenging scenarios from various domains, showing that AnyView significantly outperforms baselines in such settings with large camera displacement and limited overlap between views.
We hope that this work provides a useful building block towards improving video foundation models and 4D representations, with potential applications in dynamic scene reconstruction, world models, robotics, self-driving, and more.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist" id="bib.L1">
<li class="ltx_bibitem ltx_bib_article" id="bib.bib28">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cosmos world foundation model platform for physical ai</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2501.03575</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS1.p1.1" title="2.1 Video Generative Models ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib68">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Bahmani, I. Skorokhodov, G. Qian, A. Siarohin, W. Menapace, A. Tagliasacchi, D. B. Lindell, and S. Tulyakov</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Ac3d: analyzing and improving 3d camera control in video diffusion transformers</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†22875‚Äì22889</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p2.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib27">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Bai, M. Xia, X. Fu, X. Wang, L. Mu, J. Cao, Z. Liu, H. Hu, X. Bai, P. Wan, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ReCamMaster: camera-controlled generative rendering from a single video</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2503.11647</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.8.8.3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p2.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1.p1.1" title="4.1 Evaluation Challenges ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3.p1.1" title="4.3 Baselines ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib60">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Baillargeon</span><span class="ltx_text ltx_bib_year"> (1987)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Object permanence in 3<math alttext="1/2" class="ltx_Math" display="inline" id="bib.bib60.1.m1a" intent=":literal"><semantics><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">1/2</annotation></semantics></math>-and 4<math alttext="1/2" class="ltx_Math" display="inline" id="bib.bib60.2.m2a" intent=":literal"><semantics><mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">1/2</annotation></semantics></math>-month-old infants.</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Developmental psychology</span> <span class="ltx_text ltx_bib_volume">23</span> (<span class="ltx_text ltx_bib_number">5</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†655</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p2.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib38">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, V. Jampani, and R. Rombach</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Stable video diffusion: scaling latent video diffusion models to large datasets</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2311.15127</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2311.15127" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.1.1.3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.3.3.4" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib30">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Stable video diffusion: scaling latent video diffusion models to large datasets</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2311.15127</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS1.p1.1" title="2.1 Video Generative Models ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib63">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">N. Burgess</span><span class="ltx_text ltx_bib_year"> (2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Spatial memory: how egocentric and allocentric combine</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Trends in cognitive sciences</span> <span class="ltx_text ltx_bib_volume">10</span> (<span class="ltx_text ltx_bib_number">12</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†551‚Äì557</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p2.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib3">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Chen, T. Khurana, and D. Ramanan</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2507.12646</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2507.12646" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.11.11.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.35.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5.I1.i5.p1.1.1" title="In Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">5th item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p3.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p3.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2.p2.1" title="3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS2.p2.1" title="4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3.p1.1" title="4.3 Baselines ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.13.10.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.17.14.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.21.18.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.24.21.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.6.3.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.19.1.6" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib49">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Y. Chen, C. Xu, K. Dharmarajan, Z. Irshad, R. Cheng, K. Keutzer, M. Tomizuka, Q. Vuong, and K. Goldberg</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">RoVi-aug: robot and viewpoint augmentation for cross-embodiment robot learning</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Conference on Robot Learning (CoRL)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_place">Munich, Germany</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p1.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib21">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie√üner</span><span class="ltx_text ltx_bib_year"> (2017)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ScanNet: richly-annotated 3d reconstructions of indoor scenes</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proc. Computer Vision and Pattern Recognition (CVPR), IEEE</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.9.9.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i3.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib59">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. J. DiCarlo and D. D. Cox</span><span class="ltx_text ltx_bib_year"> (2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Untangling invariant object recognition</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Trends in cognitive sciences</span> <span class="ltx_text ltx_bib_volume">11</span> (<span class="ltx_text ltx_bib_number">8</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†333‚Äì341</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p2.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib53">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">C. Gao, A. Saraf, J. Kopf, and J. Huang</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dynamic view synthesis from dynamic monocular video</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†5712‚Äì5721</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p3.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib26">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Gao, R. Li, S. Tulsiani, B. Russell, and A. Kanazawa</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dynamic novel-view synthesis: a reality check</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">NeurIPS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p3.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.1.1.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1.p1.1" title="4.1 Evaluation Challenges ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS2.p2.1" title="4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.4.1.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib22">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik, T. Afouras, K. Ashutosh, V. Baiyya, S. Bansal, B. Boote, E. Byrne, Z. Chavis, J. Chen, F. Cheng, F. Chu, S. Crane, A. Dasgupta, J. Dong, M. Escobar, C. Forigua, A. Gebreselasie, S. Haresh, J. Huang, M. M. Islam, S. Jain, R. Khirodkar, D. Kukreja, K. J. Liang, J. Liu, S. Majumder, Y. Mao, M. Martin, E. Mavroudi, T. Nagarajan, F. Ragusa, S. K. Ramakrishnan, L. Seminara, A. Somayazulu, Y. Song, S. Su, Z. Xue, E. Zhang, J. Zhang, A. Castillo, C. Chen, X. Fu, R. Furuta, C. Gonzalez, P. Gupta, J. Hu, Y. Huang, Y. Huang, W. Khoo, A. Kumar, R. Kuo, S. Lakhavani, M. Liu, M. Luo, Z. Luo, B. Meredith, A. Miller, O. Oguntola, X. Pan, P. Peng, S. Pramanick, M. Ramazanova, F. Ryan, W. Shan, K. Somasundaram, C. Song, A. Southerland, M. Tateno, H. Wang, Y. Wang, T. Yagi, M. Yan, X. Yang, Z. Yu, S. C. Zha, C. Zhao, Z. Zhao, Z. Zhu, J. Zhuo, P. Arbelaez, G. Bertasius, D. Crandall, D. Damen, J. Engel, G. M. Farinella, A. Furnari, B. Ghanem, J. Hoffman, C. V. Jawahar, R. Newcombe, H. S. Park, J. M. Rehg, Y. Sato, M. Savva, J. Shi, M. Z. Shou, and M. Wray</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Ego-exo4d: understanding skilled human activity from first- and third-person perspectives</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2311.18259</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2311.18259" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.3.3.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i4.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">4th item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4.I1.i3.p1.1" title="In Appendix D Evaluation Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.16.16.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.5.5.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.22.4.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.35.17.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib24">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Greff, F. Belletti, L. Beyer, C. Doersch, Y. Du, D. Duckworth, D. J. Fleet, D. Gnanapragasam, F. Golemo, C. Herrmann, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Kubric: a scalable dataset generator</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.5.5.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i4.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">4th item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.SS1.p1.4" title="C.1 Kubric-5D ‚Ä£ Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ßC.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS3.p1.1" title="3.3 Datasets ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.2.2.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.7.7.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.8.8.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS2.p3.1" title="4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.12.9.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.24.6.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib14">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Guizilini, R. Ambrus, S. Pillai, A. Raventos, and A. Gaidon</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">3D packing for self-supervised monocular depth estimation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F11" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F11.13.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4.I1.i1.p1.1" title="In Appendix D Evaluation Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.14.14.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.33.15.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib75">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Guizilini, K. Lee, R. Ambrus, and A. Gaidon</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning optical flow, depth, and scene flow without real-world labels</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">IEEE Robotics and Automation Letters</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i1.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib76">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Guizilini, J. Li, R. Ambrus, and A. Gaidon</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Geometric unsupervised domain adaptation for semantic segmentation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i1.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib37">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Guo, L. X. Shi, J. Chen, and C. Finn</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Ctrl-world: a controllable generative world model for robot manipulation</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2510.10125</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2510.10125" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p1.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib70">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. He, Y. Xu, Y. Guo, G. Wetzstein, B. Dai, H. Li, and C. Yang</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cameractrl: enabling camera control for text-to-video generation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2404.02101</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p1.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_book" id="bib.bib42">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. V. D. Hodge and D. Pedoe</span><span class="ltx_text ltx_bib_year"> (1947)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Methods of algebraic geometry, volume 1</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">Cambridge University Press</span>, <span class="ltx_text ltx_bib_place">London/New York</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note">Original Publication</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2.p4.13" title="3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib12">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, L. Chen, A. Jain, S. Omari, V. Iglovikov, and P. Ondruska</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">One thousand and one hours: self-driving motion prediction dataset.</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CoRL</span>,
</span>
<span class="ltx_bibblock">Vol. <span class="ltx_text ltx_bib_volume">155</span>, <span class="ltx_text ltx_bib_pages"> pp.¬†409‚Äì418</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.6.6.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i1.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4.I1.i1.p1.1" title="In Appendix D Evaluation Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.9.9.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.26.8.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib6">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Hu, X. Gao, X. Li, S. Zhao, X. Cun, Y. Zhang, L. Quan, and Y. Shan</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">DepthCrafter: generating consistent long depth sequences for open-world videos</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">CVPR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5.I1.i4.p1.6" title="In Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">4th item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib2">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Huang, Q. Zhou, H. Rabeti, A. Korovko, H. Ling, X. Ren, T. Shen, J. Gao, D. Slepichev, C. Lin, J. Ren, K. Xie, J. Biswas, L. Leal-Taixe, and S. Fidler</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ViPE: video pose engine for 3d geometric perception</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">NVIDIA Research Whitepapers</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5.I1.i3.p1.2" title="In Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib55">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Huang, S. Miao, B. Yang, Y. Ma, and Y. Liao</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Vivid4D: improving 4d reconstruction from monocular video by video inpainting</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†12592‚Äì12604</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p3.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib74">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. Z. Irshad, V. Guizilini, A. Khazatsky, and K. Pertsch</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scaling-up automatic camera calibration for droid dataset: a study using foundation models and existing deep-learning tools</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a class="ltx_ref ltx_url ltx_font_typewriter" href="medium.com/p/4ddfc45361d3" title="">medium.com/p/4ddfc45361d3</a>Medium blog post</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i2.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">2nd item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib61">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. Kahneman, A. Treisman, and B. J. Gibbs</span><span class="ltx_text ltx_bib_year"> (1992)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The reviewing of object files: object-specific integration of information</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Cognitive psychology</span> <span class="ltx_text ltx_bib_volume">24</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†175‚Äì219</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p2.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib16">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, P. D. Fagan, J. Hejna, M. Itkina, M. Lepert, Y. J. Ma, P. T. Miller, J. Wu, S. Belkhale, S. Dass, H. Ha, A. Jain, A. Lee, Y. Lee, M. Memmel, S. Park, I. Radosavovic, K. Wang, A. Zhan, K. Black, C. Chi, K. B. Hatch, S. Lin, J. Lu, J. Mercat, A. Rehman, P. R. Sanketi, A. Sharma, C. Simpson, Q. Vuong, H. R. Walke, B. Wulfe, T. Xiao, J. H. Yang, A. Yavary, T. Z. Zhao, C. Agia, R. Baijal, M. G. Castro, D. Chen, Q. Chen, T. Chung, J. Drake, E. P. Foster, J. Gao, V. Guizilini, D. A. Herrera, M. Heo, K. Hsu, J. Hu, M. Z. Irshad, D. Jackson, C. Le, Y. Li, K. Lin, R. Lin, Z. Ma, A. Maddukuri, S. Mirchandani, D. Morton, T. Nguyen, A. O‚ÄôNeill, R. Scalise, D. Seale, V. Son, S. Tian, E. Tran, A. E. Wang, Y. Wu, A. Xie, J. Yang, P. Yin, Y. Zhang, O. Bastani, G. Berseth, J. Bohg, K. Goldberg, A. Gupta, A. Gupta, D. Jayaraman, J. J. Lim, J. Malik, R. Mart√≠n-Mart√≠n, S. Ramamoorthy, D. Sadigh, S. Song, J. Wu, M. C. Yip, Y. Zhu, T. Kollar, S. Levine, and C. Finn</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">DROID: a large-scale in-the-wild robot manipulation dataset</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.2.2.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i2.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">2nd item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4.I1.i2.p1.1" title="In Appendix D Evaluation Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">2nd item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.15.15.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.4.4.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.21.3.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.34.16.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib56">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Komatsu</span><span class="ltx_text ltx_bib_year"> (2006)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The neural mechanisms of perceptual filling-in</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Nature reviews neuroscience</span> <span class="ltx_text ltx_bib_volume">7</span> (<span class="ltx_text ltx_bib_number">3</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†220‚Äì231</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p2.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib48">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Lei, Y. Weng, A. W. Harley, L. Guibas, and K. Daniilidis</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mosca: dynamic gaussian fusion from casual videos via 4d motion scaffolds</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†6165‚Äì6177</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS2.p2.1" title="4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib66">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Liang, R. Liu, E. Ozguroglu, S. Sudhakar, A. Dave, P. Tokmakov, S. Song, and C. Vondrick</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dreamitate: real-world visuomotor policy learning via video generation</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2406.16862</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p1.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib65">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">K. Lin, L. Xiao, F. Liu, G. Yang, and R. Ramamoorthi</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Deep 3d mask volume for view synthesis of dynamic scenes</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†1749‚Äì1758</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1.p1.1" title="4.1 Evaluation Challenges ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib23">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Ling, Y. Sheng, Z. Tu, W. Zhao, C. Xin, K. Wan, L. Yu, Q. Guo, Z. Yu, Y. Lu, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dl3dv-10k: a large-scale scene dataset for deep learning-based 3d vision</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†22160‚Äì22169</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.1.1.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i3.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib57">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Nanay</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">The importance of amodal completion in everyday perception</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">i-Perception</span> <span class="ltx_text ltx_bib_volume">9</span> (<span class="ltx_text ltx_bib_number">4</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†2041669518788887</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p2.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib33">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">NVIDIA, A. Ali, J. Bai, M. Bala, Y. Balaji, A. Blakeman, T. Cai, J. Cao, T. Cao, E. Cha, Y. Chao, P. Chattopadhyay, M. Chen, Y. Chen, Y. Chen, S. Cheng, Y. Cui, J. Diamond, Y. Ding, J. Fan, L. Fan, L. Feng, F. Ferroni, S. Fidler, X. Fu, R. Gao, Y. Ge, J. Gu, A. Gupta, S. Gururani, I. El Hanafi, A. Hassani, Z. Hao, J. Huffman, J. Jang, P. Jannaty, J. Kautz, G. Lam, X. Li, Z. Li, M. Liao, C. Lin, T. Lin, Y. Lin, H. Ling, M. Liu, X. Liu, Y. Lu, A. Luo, Q. Ma, H. Mao, K. Mo, S. Nah, Y. Narang, A. Panaskar, L. Pavao, T. Pham, M. Ramezanali, F. Reda, S. Reed, X. Ren, H. Shao, Y. Shen, S. Shi, S. Song, B. Stefaniak, S. Sun, S. Tang, S. Tasmeen, L. Tchapmi, W. Tseng, J. Varghese, A. Z. Wang, H. Wang, H. Wang, H. Wang, T. Wang, F. Wei, J. Xu, D. Yang, X. Yang, H. Ye, S. Ye, X. Zeng, J. Zhang, Q. Zhang, K. Zheng, A. Zhu, and Y. Zhu</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">World simulation with video foundation models for physical ai</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2511.00062" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.14.14.5" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS1.p1.1" title="2.1 Video Generative Models ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2.p1.1" title="3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib36">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">NVIDIA</span><span class="ltx_text ltx_bib_year"> (2025-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cosmos-predict2: diffusion-based world foundation models for physics-aware image and video generation</span>.
</span>
<span class="ltx_bibblock"> <span class="ltx_text ltx_bib_publisher">GitHub</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nvidia-cosmos/cosmos-predict2" title="">https://github.com/nvidia-cosmos/cosmos-predict2</a>Accessed: 2025-11-06</span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS4.p1.6" title="3.4 Implementation Details ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.4</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib44">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Ohkawa, K. He, F. Sener, T. Hodan, L. Tran, and C. Keskin</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">AssemblyHands: towards egocentric activity understanding via 3d hand pose estimation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†12999‚Äì13008</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4.I1.i3.p1.1" title="In Appendix D Evaluation Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.13.13.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.32.14.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib51">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Pang, N. Tang, K. Li, Y. Tang, X. Cai, Z. Zhang, G. Niu, M. Sugiyama, and Y. Yu</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Learning view-invariant world models for visual robotic manipulation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">The Thirteenth International Conference on Learning Representations</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p1.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib25">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Parallel domain</span>.
</span>
<span class="ltx_bibblock">Note: <span class="ltx_text ltx_bib_note"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://paralleldomain.com/" title="">https://paralleldomain.com/</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.7.7.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.10.10.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.3.3.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS2.p3.1" title="4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.20.17.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.27.9.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib58">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Park, H. Intraub, D. Yi, D. Widders, and M. M. Chun</span><span class="ltx_text ltx_bib_year"> (2007)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Beyond the edges of a view: boundary extension in human scene-selective visual cortex</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Neuron</span> <span class="ltx_text ltx_bib_volume">54</span> (<span class="ltx_text ltx_bib_number">2</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†335‚Äì342</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p2.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib40">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Raistrick, L. Lipson, Z. Ma, L. Mei, M. Wang, Y. Zuo, K. Kayan, H. Wen, B. Han, Y. Wang, A. Newell, H. Law, A. Goyal, K. Yang, and J. Deng</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Infinite photorealistic worlds using procedural generation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†12630‚Äì12641</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS3.p1.1" title="3.3 Datasets ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib41">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">A. Raistrick, L. Mei, K. Kayan, D. Yan, Y. Zuo, B. Han, H. Wen, M. Parakh, S. Alexandropoulos, L. Lipson, Z. Ma, and J. Deng</span><span class="ltx_text ltx_bib_year"> (2024-06)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Infinigen indoors: photorealistic indoor scenes using procedural generation</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†21783‚Äì21794</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS3.p1.1" title="3.3 Datasets ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib5">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Ren, T. Shen, J. Huang, H. Ling, Y. Lu, M. Nimier-David, T. M√ºller, A. Keller, S. Fidler, and J. Gao</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">GEN3C: 3d-informed world-consistent video generation with precise camera control</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.5.5.3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i3.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5.I1.i3.p1.2.1" title="In Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p3.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2.p2.1" title="3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3.p1.1" title="4.3 Baselines ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.15.12.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.22.19.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.7.4.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.19.1.4" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib34">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer</span><span class="ltx_text ltx_bib_year"> (2022)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">High-resolution image synthesis with latent diffusion models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†10684‚Äì10695</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS1.p1.1" title="2.1 Video Generative Models ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib77">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and A. Yao</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Assembly101: a large-scale multi-view video dataset for understanding procedural activities</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">CVPR 2022</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4.I1.i3.p1.1" title="In Appendix D Evaluation Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib62">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. N. Shepard and J. Metzler</span><span class="ltx_text ltx_bib_year"> (1971)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mental rotation of three-dimensional objects</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">Science</span> <span class="ltx_text ltx_bib_volume">171</span> (<span class="ltx_text ltx_bib_number">3972</span>), <span class="ltx_text ltx_bib_pages"> pp.¬†701‚Äì703</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p2.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib47">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Su, Y. Lu, S. Pan, B. Wen, and Y. Liu</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">RoFormer: enhanced transformer with rotary position embedding</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2104.09864</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2.p5.5" title="3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib15">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Scalability in perception for autonomous driving: waymo open dataset</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†2446‚Äì2454</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.11.11.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i1.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4.I1.i1.p1.1" title="In Appendix D Evaluation Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.11.11.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.28.10.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib50">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">S. Tian, B. Wulfe, K. Sargent, K. Liu, S. Zakharov, V. Guizilini, and J. Wu</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">View-invariant policy learning via zero-shot novel view synthesis</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2409.03685</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p1.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib9">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Van Hoorick, R. Wu, E. Ozguroglu, K. Sargent, R. Liu, P. Tokmakov, A. Dave, C. Zheng, and C. Vondrick</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Generative camera dolly: extreme monocular dynamic novel view synthesis</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">ECCV</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F10" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F10.14.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F11" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F11.13.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.1.1.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A2.p2.1" title="Appendix B Additional Qualitative Results ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i1.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i4.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">4th item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.SS1.p1.4" title="C.1 Kubric-5D ‚Ä£ Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ßC.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5.I1.i1.p1.5.1" title="In Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p2.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2.p4.13" title="3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS3.p1.1" title="3.3 Datasets ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1.p1.1" title="4.1 Evaluation Challenges ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS2.p3.1" title="4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3.p1.1" title="4.3 Baselines ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS4.p2.1" title="4.4 Results ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.10.7.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.18.15.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.25.22.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.19.1.2" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib71">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">V. Voleti, C. Yao, M. Boss, A. Letts, D. Pankratz, D. Tochilkin, C. Laforte, R. Rombach, and V. Jampani</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Sv3d: novel multi-view synthesis and 3d generation from a single image using latent video diffusion</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European Conference on Computer Vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†439‚Äì457</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p1.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib29">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Wan, A. Wang, B. Ai, B. Wen, C. Mao, C. Xie, D. Chen, F. Yu, H. Zhao, J. Yang, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Wan: open and advanced large-scale video generative models</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2503.20314</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.8.8.4" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS1.p1.1" title="2.1 Video Generative Models ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib4">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Q. Wang, V. Ye, H. Gao, W. Zeng, J. Austin, Z. Li, and A. Kanazawa</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Shape of motion: 4d reconstruction from a single video</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">International Conference on Computer Vision (ICCV)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p3.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p1.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1.p1.1" title="4.1 Evaluation Challenges ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.5.2.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib20">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">W. Wang, D. Zhu, X. Wang, Y. Hu, Y. Qiu, C. Wang, Y. Hu, A. Kapoor, and S. Scherer</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Tartanair: a dataset to push the limits of visual slam</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">IROS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.10.10.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i3.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib67">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">X. Wang, Z. Zhu, G. Huang, X. Chen, J. Zhu, and J. Lu</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Drivedreamer: towards real-world-drive world models for autonomous driving</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">European conference on computer vision</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†55‚Äì72</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p1.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib13">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">B. Wilson, W. Qi, T. Agarwal, J. Lambert, J. Singh, S. Khandelwal, B. Pan, R. Kumar, A. Hartnett, J. K. Pontes, D. Ramanan, P. Carr, and J. Hays</span><span class="ltx_text ltx_bib_year"> (2021)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Argoverse 2: next generation datasets for self-driving perception and forecasting</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A4.I1.i1.p1.1" title="In Appendix D Evaluation Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">1st item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.T1.12.12.2" title="In 3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.31.13.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib54">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">4d gaussian splatting for real-time dynamic scene rendering</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†20310‚Äì20320</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p3.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib72">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">R. Wu, R. Gao, B. Poole, A. Trevithick, C. Zheng, J. T. Barron, and A. Holynski</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cat4d: create anything in 4d with multi-view video diffusion models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the Computer Vision and Pattern Recognition Conference</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†26057‚Äì26068</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p1.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib17">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Xia, Y. Fu, S. Liu, and X. Wang</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">RGBD objects in the wild: scaling real-world 3d object learning from rgb-d videos</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2401.12592</span></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.12.12.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i3.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib7">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Xiao, W. Ouyang, Y. Zhou, S. Yang, L. Yang, J. Si, and X. Pan</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Trajectory attention for fine-grained video motion control</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICLR</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.3.3.3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.35.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5.I1.i2.p1.4.1" title="In Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">2nd item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p2.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2.p2.1" title="3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3.p1.1" title="4.3 Baselines ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.16.13.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.23.20.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.8.5.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.19.1.3" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib35">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Yan, Z. Xu, H. Lin, H. Jin, H. Guo, Y. Wang, K. Zhan, X. Lang, H. Bao, X. Zhou, and S. Peng</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">StreetCrafter: street view synthesis with controllable video diffusion models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p3.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib52">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. Yang, I. Huang, B. Vu, M. Bajracharya, R. Antonova, and J. Bohg</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Mobi-pi: mobilizing your robot learning policy</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2505.23692</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p1.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib1">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Depth anything v2</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv:2406.09414</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5.p2.1" title="Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3.p1.1" title="4.3 Baselines ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib32">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, <span class="ltx_text ltx_bib_etal">et al.</span></span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">CogVideoX: text-to-video diffusion models with an expert transformer</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2408.06072</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.10.10.4" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.11.11.3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.6.6.3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS1.p1.1" title="2.1 Video Generative Models ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib8">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Yesiltepe and P. Yanardag</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dynamic view synthesis as an inverse problem</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">NeurIPS</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p3.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_misc" id="bib.bib43">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">H. Yesiltepe and P. Yanardag</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Dynamic view synthesis as an inverse problem</span>.
</span>
<span class="ltx_bibblock">External Links: <span class="ltx_text ltx_bib_links"><span class="ltx_text ltx_bib_external">2506.08004</span>,
<a class="ltx_ref ltx_bib_external" href="https://arxiv.org/abs/2506.08004" title="">Link</a></span>
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.10.10.3" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.35.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p3.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1.p1.1" title="4.1 Evaluation Challenges ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3.p1.1" title="4.3 Baselines ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib64">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">J. S. Yoon, K. Kim, O. Gallo, H. S. Park, and J. Kautz</span><span class="ltx_text ltx_bib_year"> (2020)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>,
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_pages"> pp.¬†5336‚Äì5345</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1.p1.1" title="4.1 Evaluation Challenges ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.1</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib10">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">M. YU, W. Hu, J. Xing, and Y. Shan</span><span class="ltx_text ltx_bib_year"> (2025)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.35.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5.6.6.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 5</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i3.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5.I1.i4.p1.6.1" title="In Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">4th item</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S1.p3.1" title="1 Introduction ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p3.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS2.p2.1" title="3.2 Architecture ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS3.p1.1" title="4.3 Baselines ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.3</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.9.6.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T3.18.19.1.5" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib11">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">D. J. Zhang, R. Paiss, S. Zada, N. Karnad, D. E. Jacobs, Y. Pritch, I. Mosseri, M. Z. Shou, N. Wadhwa, and N. Ruiz</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">ReCapture: generative video camera controls for user-provided videos using masked video fine-tuning</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2411.05003</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.SS1.p1.1" title="4.1 Evaluation Challenges ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß4.1</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S4.T2.3.14.11.1" title="In 4.2 Benchmarks ‚Ä£ 4 Experiments ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_article" id="bib.bib69">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">G. Zheng, T. Li, R. Jiang, Y. Lu, T. Wu, and X. Li</span><span class="ltx_text ltx_bib_year"> (2024)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Cami2v: camera-controlled image-to-video diffusion model</span>.
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_journal">arXiv preprint arXiv:2410.15957</span>.
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S2.SS2.p1.1" title="2.2 Dynamic View Synthesis ‚Ä£ 2 Related Work ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß2.2</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib39">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">Y. Zheng, A. W. Harley, B. Shen, G. Wetzstein, and L. J. Guibas</span><span class="ltx_text ltx_bib_year"> (2023)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">PointOdyssey: a large-scale synthetic dataset for long-term point tracking</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">ICCV</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#S3.SS3.p1.1" title="3.3 Datasets ‚Ä£ 3 Methodology ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">¬ß3.3</span></a>.
</span>
</li>
<li class="ltx_bibitem ltx_bib_inproceedings" id="bib.bib19">
<span class="ltx_tag ltx_bib_key ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_author">T. Zhou, R. Tucker, J. Flynn, G. Fyffe, and N. Snavely</span><span class="ltx_text ltx_bib_year"> (2018)</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_bib_title">Stereo magnification: learning view synthesis using multiplane images</span>.
</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_bib_inbook">SIGGRAPH</span>,
</span>
<span class="ltx_bibblock ltx_bib_cited">Cited by: <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4.8.8.2" title="In AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">Table 4</span></a>,
<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A3.I1.i3.p1.1" title="In Appendix C Training Datasets ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">3rd item</span></a>.
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para ltx_align_center" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\thetitle</span>
<br class="ltx_break"/>
</div>
<div class="ltx_para ltx_align_center" id="p3">
<p class="ltx_p" id="p3.1"><span class="ltx_text" id="p3.1.1" style="font-size:144%;">Supplementary Material 
<br class="ltx_break"/></span></p>
</div>
<figure class="ltx_figure ltx_align_center" id="A0.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="502" id="A0.F9.g1" src="x12.png" width="747"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A0.F9.24.1.1" style="font-size:63%;">Figure 9</span>: </span><span class="ltx_text" id="A0.F9.25.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="A0.F9.25.2.1">Uncertainty analysis.</span>
In <span class="ltx_text ltx_font_bold" id="A0.F9.25.2.2">(a)</span>, the model cannot see what is contained <span class="ltx_text" id="A0.F9.25.2.3" style="--ltx-fg-color:#999900;">inside the black bin</span> because the contents are occluded, and resorts to predicting fruit (since those objects are common in LBM), in addition to spawning <span class="ltx_text" id="A0.F9.25.2.4" style="--ltx-fg-color:#E63333;">spurious objects</span> out-of-frame on the left.
In <span class="ltx_text ltx_font_bold" id="A0.F9.25.2.5">(b)</span>, we mainly observe variations of object positions along the <span class="ltx_text" id="A0.F9.25.2.6" style="--ltx-fg-color:#CC1AB3;">input viewing direction</span> (overlayed with pink arrows for clarity), which presumably stems primarily from uncertainty in terms of implicit depth estimation that the model has to perform internally as part of the representation.
In <span class="ltx_text ltx_font_bold" id="A0.F9.25.2.7">(c)</span>, only the front-right view is seen, which passes by <span class="ltx_text" id="A0.F9.25.2.8" style="--ltx-fg-color:#009900;">several</span> <span class="ltx_text" id="A0.F9.25.2.9" style="--ltx-fg-color:#004DCC;">buildings</span> that are reconstructed correctly in all samples (= front view).
Meanwhile, the <span class="ltx_text" id="A0.F9.25.2.10" style="--ltx-fg-color:#999900;">left half</span> of these output videos has more diversity since it is never directly observed.
</span></figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="A0.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="A0.F10.g1" src="x13.png" width="747"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A0.F10.13.1.1" style="font-size:63%;">Figure 10</span>: </span><span class="ltx_text" id="A0.F10.14.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="A0.F10.14.2.1">Gradually increasing target azimuth.</span>
As we increase the difficulty of the task by rotating the virtual camera over larger and larger angles away from the observed camera in this Kubric scene, GCD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite> produces garbled outputs where objects become <span class="ltx_text" id="A0.F10.14.2.2" style="--ltx-fg-color:#E61A1A;">essentially unrecognizable</span>.
In contrast, AnyView maintains <span class="ltx_text" id="A0.F10.14.2.3" style="--ltx-fg-color:#009900;">clear spatiotemporal correspondence</span> across dramatic viewpoint changes, demonstrating significantly enhanced 4D understanding over previous methods.
</span></figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="A0.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="545" id="A0.F11.g1" src="x14.png" width="581"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A0.F11.12.1.1" style="font-size:63%;">Figure 11</span>: </span><span class="ltx_text" id="A0.F11.13.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="A0.F11.13.2.1">Upward view synthesis on real-world driving scenarios.</span>
We compare AnyView with GCD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite> on DDAD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib14" title="3D packing for self-supervised monocular depth estimation">16</a>]</cite>, which is a zero-shot dataset for both methods.
AnyView generates much clearer predictions: almost every car that the model can see is reconstructed with high fidelity and accurate dynamics, whereas GCD often suffers from blurry artefacts, which worsen the further away one looks from the ego vehicle.
</span></figcaption>
</figure>
<figure class="ltx_figure ltx_align_center" id="A0.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="386" id="A0.F12.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:144%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A0.F12.6.1.1" style="font-size:63%;">Figure 12</span>: </span><span class="ltx_text" id="A0.F12.7.2" style="font-size:63%;">
<span class="ltx_text ltx_font_bold" id="A0.F12.7.2.1">Diversity of camera trajectories.</span>
Samples of dataset camera trajectories illustrating the diversity of motion patterns used in our evaluation.
</span></figcaption>
</figure>
<figure class="ltx_table ltx_align_center" id="A0.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A0.T4.12">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A0.T4.12.13.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="A0.T4.12.13.1.1"><span class="ltx_text" id="A0.T4.12.13.1.1.1" style="font-size:80%;">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T4.12.13.1.2"><span class="ltx_text" id="A0.T4.12.13.1.2.1" style="font-size:80%;">S/R</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T4.12.13.1.3"><span class="ltx_text" id="A0.T4.12.13.1.3.1" style="font-size:80%;">Domain</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T4.12.13.1.4"><span class="ltx_text" id="A0.T4.12.13.1.4.1" style="font-size:80%;">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T4.12.13.1.5"><span class="ltx_text" id="A0.T4.12.13.1.5.1" style="font-size:80%;"># Cameras</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T4.12.13.1.6"><span class="ltx_text" id="A0.T4.12.13.1.6.1" style="font-size:80%;"># Episodes</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T4.12.13.1.7"><span class="ltx_text" id="A0.T4.12.13.1.7.1" style="font-size:80%;">Resolution</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T4.12.13.1.8"><span class="ltx_text" id="A0.T4.12.13.1.8.1" style="font-size:80%;">Weight</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A0.T4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A0.T4.1.1.2">
<span class="ltx_text" id="A0.T4.1.1.2.1" style="font-size:80%;">DL3DV-10K¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.1.1.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib23" title="Dl3dv-10k: a large-scale scene dataset for deep learning-based 3d vision">33</a><span class="ltx_text" id="A0.T4.1.1.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T4.1.1.3"><span class="ltx_text" id="A0.T4.1.1.3.1" style="font-size:80%;">Real</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T4.1.1.4"><span class="ltx_text" id="A0.T4.1.1.4.1" style="font-size:80%;">Indoor + Outdoor</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T4.1.1.5"><span class="ltx_text" id="A0.T4.1.1.5.1" style="font-size:80%;">3D</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T4.1.1.6"><span class="ltx_text" id="A0.T4.1.1.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T4.1.1.7"><span class="ltx_text" id="A0.T4.1.1.7.1" style="font-size:80%;">5,906</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T4.1.1.1">
<span class="ltx_text" id="A0.T4.1.1.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="A0.T4.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="A0.T4.1.1.8"><span class="ltx_text" id="A0.T4.1.1.8.1" style="font-size:80%;">6.3¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.2.2.2">
<span class="ltx_text" id="A0.T4.2.2.2.1" style="font-size:80%;">DROID¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.2.2.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib16" title="DROID: a large-scale in-the-wild robot manipulation dataset">28</a><span class="ltx_text" id="A0.T4.2.2.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.2.2.3"><span class="ltx_text" id="A0.T4.2.2.3.1" style="font-size:80%;">Real</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.2.2.4"><span class="ltx_text" id="A0.T4.2.2.4.1" style="font-size:80%;">Robotics</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.2.2.5"><span class="ltx_text" id="A0.T4.2.2.5.1" style="font-size:80%;">4D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.2.2.6"><span class="ltx_text" id="A0.T4.2.2.6.1" style="font-size:80%;">2 (exo only)</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.2.2.7"><span class="ltx_text" id="A0.T4.2.2.7.1" style="font-size:80%;">29,712</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.2.2.1">
<span class="ltx_text" id="A0.T4.2.2.1.1" style="font-size:80%;">29 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="A0.T4.2.2.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.2.2.8"><span class="ltx_text" id="A0.T4.2.2.8.1" style="font-size:80%;">12.5¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.3.3.2">
<span class="ltx_text" id="A0.T4.3.3.2.1" style="font-size:80%;">Ego-Exo4D¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.3.3.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib22" title="Ego-exo4d: understanding skilled human activity from first- and third-person perspectives">14</a><span class="ltx_text" id="A0.T4.3.3.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.3.3.3"><span class="ltx_text" id="A0.T4.3.3.3.1" style="font-size:80%;">Real</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.3.3.4"><span class="ltx_text" id="A0.T4.3.3.4.1" style="font-size:80%;">Human Activity</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.3.3.5"><span class="ltx_text" id="A0.T4.3.3.5.1" style="font-size:80%;">4D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.3.3.6"><span class="ltx_text" id="A0.T4.3.3.6.1" style="font-size:80%;">4 ‚Äì 5 (exo only)</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.3.3.7"><span class="ltx_text" id="A0.T4.3.3.7.1" style="font-size:80%;">2,489</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.3.3.1">
<span class="ltx_text" id="A0.T4.3.3.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="A0.T4.3.3.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.3.3.8"><span class="ltx_text" id="A0.T4.3.3.8.1" style="font-size:80%;">9.4¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.4.4.2"><span class="ltx_text" id="A0.T4.4.4.2.1" style="font-size:80%;">LBM</span></th>
<td class="ltx_td ltx_align_center" id="A0.T4.4.4.3"><span class="ltx_text" id="A0.T4.4.4.3.1" style="font-size:80%;">Sim + Real</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.4.4.4"><span class="ltx_text" id="A0.T4.4.4.4.1" style="font-size:80%;">Robotics</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.4.4.5"><span class="ltx_text" id="A0.T4.4.4.5.1" style="font-size:80%;">4D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.4.4.6"><span class="ltx_text" id="A0.T4.4.4.6.1" style="font-size:80%;">2 (exo only)</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.4.4.7"><span class="ltx_text" id="A0.T4.4.4.7.1" style="font-size:80%;">53,886</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.4.4.1">
<span class="ltx_text" id="A0.T4.4.4.1.1" style="font-size:80%;">41 frames @ </span><math alttext="336\times 256" class="ltx_Math" display="inline" id="A0.T4.4.4.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">336</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">256</mn></mrow><annotation encoding="application/x-tex">336\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.4.4.8"><span class="ltx_text" id="A0.T4.4.4.8.1" style="font-size:80%;">12.5¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.5.5.2">
<span class="ltx_text" id="A0.T4.5.5.2.1" style="font-size:80%;">Kubric¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.5.5.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a><span class="ltx_text" id="A0.T4.5.5.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.5.5.3"><span class="ltx_text" id="A0.T4.5.5.3.1" style="font-size:80%;">Sim</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.5.5.4"><span class="ltx_text" id="A0.T4.5.5.4.1" style="font-size:80%;">Multi-Object</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.5.5.5"><span class="ltx_text" id="A0.T4.5.5.5.1" style="font-size:80%;">4D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.5.5.6"><span class="ltx_text" id="A0.T4.5.5.6.1" style="font-size:80%;">16 (exo only)</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.5.5.7"><span class="ltx_text" id="A0.T4.5.5.7.1" style="font-size:80%;">12,400</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.5.5.1">
<span class="ltx_text" id="A0.T4.5.5.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 256" class="ltx_Math" display="inline" id="A0.T4.5.5.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">256</mn></mrow><annotation encoding="application/x-tex">384\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.5.5.8"><span class="ltx_text" id="A0.T4.5.5.8.1" style="font-size:80%;">15.6¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.6.6.2">
<span class="ltx_text" id="A0.T4.6.6.2.1" style="font-size:80%;">Lyft¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.6.6.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib12" title="One thousand and one hours: self-driving motion prediction dataset.">22</a><span class="ltx_text" id="A0.T4.6.6.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.6.6.3"><span class="ltx_text" id="A0.T4.6.6.3.1" style="font-size:80%;">Real</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.6.6.4"><span class="ltx_text" id="A0.T4.6.6.4.1" style="font-size:80%;">Driving</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.6.6.5"><span class="ltx_text" id="A0.T4.6.6.5.1" style="font-size:80%;">4D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.6.6.6"><span class="ltx_text" id="A0.T4.6.6.6.1" style="font-size:80%;">6 (ego only)</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.6.6.7"><span class="ltx_text" id="A0.T4.6.6.7.1" style="font-size:80%;">296</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.6.6.1">
<span class="ltx_text" id="A0.T4.6.6.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 320" class="ltx_Math" display="inline" id="A0.T4.6.6.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">320</mn></mrow><annotation encoding="application/x-tex">384\times 320</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.6.6.8"><span class="ltx_text" id="A0.T4.6.6.8.1" style="font-size:80%;">3.1¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.7.7.2">
<span class="ltx_text" id="A0.T4.7.7.2.1" style="font-size:80%;">ParallelDomain¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.7.7.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib25" title="Parallel domain">39</a><span class="ltx_text" id="A0.T4.7.7.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.7.7.3"><span class="ltx_text" id="A0.T4.7.7.3.1" style="font-size:80%;">Sim</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.7.7.4"><span class="ltx_text" id="A0.T4.7.7.4.1" style="font-size:80%;">Driving</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.7.7.5"><span class="ltx_text" id="A0.T4.7.7.5.1" style="font-size:80%;">4D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.7.7.6"><span class="ltx_text" id="A0.T4.7.7.6.1" style="font-size:80%;">19 (16 exo + 3 ego)</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.7.7.7"><span class="ltx_text" id="A0.T4.7.7.7.1" style="font-size:80%;">7,352</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.7.7.1">
<span class="ltx_text" id="A0.T4.7.7.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 256" class="ltx_Math" display="inline" id="A0.T4.7.7.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">256</mn></mrow><annotation encoding="application/x-tex">384\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.7.7.8"><span class="ltx_text" id="A0.T4.7.7.8.1" style="font-size:80%;">18.8¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.8.8.2">
<span class="ltx_text" id="A0.T4.8.8.2.1" style="font-size:80%;">RealEstate-10K¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.8.8.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib19" title="Stereo magnification: learning view synthesis using multiplane images">72</a><span class="ltx_text" id="A0.T4.8.8.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.8.8.3"><span class="ltx_text" id="A0.T4.8.8.3.1" style="font-size:80%;">Real</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.8.8.4"><span class="ltx_text" id="A0.T4.8.8.4.1" style="font-size:80%;">Indoor + Outdoor</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.8.8.5"><span class="ltx_text" id="A0.T4.8.8.5.1" style="font-size:80%;">3D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.8.8.6"><span class="ltx_text" id="A0.T4.8.8.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.8.8.7"><span class="ltx_text" id="A0.T4.8.8.7.1" style="font-size:80%;">34,968</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.8.8.1">
<span class="ltx_text" id="A0.T4.8.8.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 208" class="ltx_Math" display="inline" id="A0.T4.8.8.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">208</mn></mrow><annotation encoding="application/x-tex">384\times 208</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.8.8.8"><span class="ltx_text" id="A0.T4.8.8.8.1" style="font-size:80%;">6.3¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.9.9.2">
<span class="ltx_text" id="A0.T4.9.9.2.1" style="font-size:80%;">ScanNet¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.9.9.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib21" title="ScanNet: richly-annotated 3d reconstructions of indoor scenes">10</a><span class="ltx_text" id="A0.T4.9.9.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.9.9.3"><span class="ltx_text" id="A0.T4.9.9.3.1" style="font-size:80%;">Real</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.9.9.4"><span class="ltx_text" id="A0.T4.9.9.4.1" style="font-size:80%;">Indoor</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.9.9.5"><span class="ltx_text" id="A0.T4.9.9.5.1" style="font-size:80%;">3D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.9.9.6"><span class="ltx_text" id="A0.T4.9.9.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.9.9.7"><span class="ltx_text" id="A0.T4.9.9.7.1" style="font-size:80%;">1,357</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.9.9.1">
<span class="ltx_text" id="A0.T4.9.9.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 288" class="ltx_Math" display="inline" id="A0.T4.9.9.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">288</mn></mrow><annotation encoding="application/x-tex">384\times 288</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.9.9.8"><span class="ltx_text" id="A0.T4.9.9.8.1" style="font-size:80%;">3.1¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.10.10.2">
<span class="ltx_text" id="A0.T4.10.10.2.1" style="font-size:80%;">TartanAir¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.10.10.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib20" title="Tartanair: a dataset to push the limits of visual slam">54</a><span class="ltx_text" id="A0.T4.10.10.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.10.10.3"><span class="ltx_text" id="A0.T4.10.10.3.1" style="font-size:80%;">Sim</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.10.10.4"><span class="ltx_text" id="A0.T4.10.10.4.1" style="font-size:80%;">Indoor + Outdoor</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.10.10.5"><span class="ltx_text" id="A0.T4.10.10.5.1" style="font-size:80%;">3D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.10.10.6"><span class="ltx_text" id="A0.T4.10.10.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.10.10.7"><span class="ltx_text" id="A0.T4.10.10.7.1" style="font-size:80%;">369</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.10.10.1">
<span class="ltx_text" id="A0.T4.10.10.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 288" class="ltx_Math" display="inline" id="A0.T4.10.10.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">288</mn></mrow><annotation encoding="application/x-tex">384\times 288</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.10.10.8"><span class="ltx_text" id="A0.T4.10.10.8.1" style="font-size:80%;">3.1¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A0.T4.11.11.2">
<span class="ltx_text" id="A0.T4.11.11.2.1" style="font-size:80%;">Waymo¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.11.11.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib15" title="Scalability in perception for autonomous driving: waymo open dataset">48</a><span class="ltx_text" id="A0.T4.11.11.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="A0.T4.11.11.3"><span class="ltx_text" id="A0.T4.11.11.3.1" style="font-size:80%;">Real</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.11.11.4"><span class="ltx_text" id="A0.T4.11.11.4.1" style="font-size:80%;">Driving</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.11.11.5"><span class="ltx_text" id="A0.T4.11.11.5.1" style="font-size:80%;">4D</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.11.11.6"><span class="ltx_text" id="A0.T4.11.11.6.1" style="font-size:80%;">5 (ego only)</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.11.11.7"><span class="ltx_text" id="A0.T4.11.11.7.1" style="font-size:80%;">798</span></td>
<td class="ltx_td ltx_align_center" id="A0.T4.11.11.1">
<span class="ltx_text" id="A0.T4.11.11.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times\{176,256\}" class="ltx_Math" display="inline" id="A0.T4.11.11.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mrow><mo maxsize="0.800em" minsize="0.800em">{</mo><mn mathsize="0.800em">176</mn><mo mathsize="0.800em">,</mo><mn mathsize="0.800em">256</mn><mo maxsize="0.800em" minsize="0.800em">}</mo></mrow></mrow><annotation encoding="application/x-tex">384\times\{176,256\}</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="A0.T4.11.11.8"><span class="ltx_text" id="A0.T4.11.11.8.1" style="font-size:80%;">3.1¬†%</span></td>
</tr>
<tr class="ltx_tr" id="A0.T4.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A0.T4.12.12.2">
<span class="ltx_text" id="A0.T4.12.12.2.1" style="font-size:80%;">WildRGB-D¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T4.12.12.2.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib17" title="RGBD objects in the wild: scaling real-world 3d object learning from rgb-d videos">59</a><span class="ltx_text" id="A0.T4.12.12.2.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T4.12.12.3"><span class="ltx_text" id="A0.T4.12.12.3.1" style="font-size:80%;">Real</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T4.12.12.4"><span class="ltx_text" id="A0.T4.12.12.4.1" style="font-size:80%;">Single-Object</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T4.12.12.5"><span class="ltx_text" id="A0.T4.12.12.5.1" style="font-size:80%;">3D</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T4.12.12.6"><span class="ltx_text" id="A0.T4.12.12.6.1" style="font-size:80%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T4.12.12.7"><span class="ltx_text" id="A0.T4.12.12.7.1" style="font-size:80%;">23,002</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A0.T4.12.12.1">
<span class="ltx_text" id="A0.T4.12.12.1.1" style="font-size:80%;">41 frames @ </span><math alttext="384\times 288" class="ltx_Math" display="inline" id="A0.T4.12.12.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.800em">384</mn><mo lspace="0.222em" mathsize="0.800em" rspace="0.222em">√ó</mo><mn mathsize="0.800em">288</mn></mrow><annotation encoding="application/x-tex">384\times 288</annotation></semantics></math>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="A0.T4.12.12.8"><span class="ltx_text" id="A0.T4.12.12.8.1" style="font-size:80%;">6.3¬†%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A0.T4.18.1.1" style="font-size:113%;">Table 4</span>: </span><span class="ltx_text" id="A0.T4.19.2" style="font-size:113%;">
<span class="ltx_text ltx_font_bold" id="A0.T4.19.2.1">AnyView training datasets.</span>
We use a weighted mixture of both static and dynamic data sources that combines multiple domains of interest.
For multi-view video (4D) datasets, if there are more than two cameras, we randomly sample an input + ground truth pair for each training sample.
For static (3D) datasets, with videos typically consisting of only one moving camera, we randomly sample subclips and treat them as different cameras for the purposes of training and evaluation.
</span></figcaption>
</figure>
<figure class="ltx_table ltx_align_center" id="A0.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A0.T5.14">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A0.T5.14.15.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A0.T5.14.15.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.15.1.1.1" style="font-size:67%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T5.14.15.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.15.1.2.1" style="font-size:67%;">Base Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T5.14.15.1.3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.15.1.3.1" style="font-size:67%;">Training Datasets</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T5.14.15.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.15.1.4.1" style="font-size:67%;">Resolution</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T5.14.15.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.15.1.5.1" style="font-size:67%;">Input Cam.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T5.14.15.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.15.1.6.1" style="font-size:67%;">Align Start</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A0.T5.14.15.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.15.1.7.1" style="font-size:67%;"># DOF</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A0.T5.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="A0.T5.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.1.1.2.1" style="font-size:67%;">GCD¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.1.1.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a><span class="ltx_text" id="A0.T5.1.1.2.3.2" style="font-size:67%;">]</span></cite><span class="ltx_text" id="A0.T5.1.1.2.4" style="font-size:67%;"> (2024)</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.1.1.3.1" style="font-size:67%;">SVD-1B¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.1.1.3.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib38" title="Stable video diffusion: scaling latent video diffusion models to large datasets">5</a><span class="ltx_text" id="A0.T5.1.1.3.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.1.1.4.1" style="font-size:67%;">Kubric-4D, ParDom-4D</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.1.1.1.1" style="font-size:67%;">14 frames @ </span><math alttext="384\times 256" class="ltx_Math" display="inline" id="A0.T5.1.1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">384</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">256</mn></mrow><annotation encoding="application/x-tex">384\times 256</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.1.1.5.1" style="font-size:67%;">Either</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.1.1.6.1" style="font-size:67%;">Either</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A0.T5.1.1.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.1.1.7.1" style="font-size:67%;">3</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.3.3">
<td class="ltx_td ltx_align_left" id="A0.T5.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.3.3.3.1" style="font-size:67%;">TrajAttn¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.3.3.3.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a><span class="ltx_text" id="A0.T5.3.3.3.3.2" style="font-size:67%;">]</span></cite><span class="ltx_text" id="A0.T5.3.3.3.4" style="font-size:67%;"> (2024)</span>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.3.3.4.1" style="font-size:67%;">SVD-1B¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.3.3.4.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib38" title="Stable video diffusion: scaling latent video diffusion models to large datasets">5</a><span class="ltx_text" id="A0.T5.3.3.4.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.3.3.5.1" style="font-size:67%;">MiraData</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.2.2.1.1" style="font-size:67%;">25 frames @ </span><math alttext="1024\times 576" class="ltx_Math" display="inline" id="A0.T5.2.2.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">1024</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">576</mn></mrow><annotation encoding="application/x-tex">1024\times 576</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.3.3.6.1" style="font-size:67%;">Flexible</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.3.3.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.3.3.7.1" style="font-size:67%;">Yes</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="6\cdot T" class="ltx_Math" display="inline" id="A0.T5.3.3.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">6</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">‚ãÖ</mo><mi mathsize="0.670em">T</mi></mrow><annotation encoding="application/x-tex">6\cdot T</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A0.T5.5.5">
<td class="ltx_td ltx_align_left" id="A0.T5.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.5.5.3.1" style="font-size:67%;">GEN3C¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.5.5.3.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a><span class="ltx_text" id="A0.T5.5.5.3.3.2" style="font-size:67%;">]</span></cite><span class="ltx_text" id="A0.T5.5.5.3.4" style="font-size:67%;"> (2025)</span>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.5.5.4.1" style="font-size:67%;">GEN3C-Cosmos-7B</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.5.5.5.1" style="font-size:67%;">Kubric-4D, DL3DV, RE-10K, Waymo OD</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.4.4.1.1" style="font-size:67%;">121 frames @ </span><math alttext="1280\times 704" class="ltx_Math" display="inline" id="A0.T5.4.4.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">1280</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">704</mn></mrow><annotation encoding="application/x-tex">1280\times 704</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.5.5.6.1" style="font-size:67%;">Moving</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.5.5.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.5.5.7.1" style="font-size:67%;">Yes</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="6\cdot T" class="ltx_Math" display="inline" id="A0.T5.5.5.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">6</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">‚ãÖ</mo><mi mathsize="0.670em">T</mi></mrow><annotation encoding="application/x-tex">6\cdot T</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A0.T5.6.6">
<td class="ltx_td ltx_align_left" id="A0.T5.6.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.6.6.2.1" style="font-size:67%;">TrajCrafter¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.6.6.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a><span class="ltx_text" id="A0.T5.6.6.2.3.2" style="font-size:67%;">]</span></cite><span class="ltx_text" id="A0.T5.6.6.2.4" style="font-size:67%;"> (2025)</span>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.6.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.6.6.3.1" style="font-size:67%;">CogVideoX-Fun-5B¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.6.6.3.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib32" title="CogVideoX: text-to-video diffusion models with an expert transformer">64</a><span class="ltx_text" id="A0.T5.6.6.3.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.6.6.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.6.6.4.1" style="font-size:67%;">OpenVid-1M, DL3DV, RE-10K</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.6.6.1.1" style="font-size:67%;">49 frames @ </span><math alttext="672\times 384" class="ltx_Math" display="inline" id="A0.T5.6.6.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">672</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">384</mn></mrow><annotation encoding="application/x-tex">672\times 384</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.6.6.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.6.6.5.1" style="font-size:67%;">Flexible</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.6.6.6.1" style="font-size:67%;">Yes</span><sup class="ltx_sup" id="A0.T5.6.6.6.2"><span class="ltx_text" id="A0.T5.6.6.6.2.1" style="font-size:67%;">*</span></sup>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.6.6.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.6.6.7.1" style="font-size:67%;">5</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.8.8">
<td class="ltx_td ltx_align_left" id="A0.T5.8.8.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.8.8.3.1" style="font-size:67%;">ReCamMaster¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.8.8.3.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib27" title="ReCamMaster: camera-controlled generative rendering from a single video">3</a><span class="ltx_text" id="A0.T5.8.8.3.3.2" style="font-size:67%;">]</span></cite><span class="ltx_text" id="A0.T5.8.8.3.4" style="font-size:67%;"> (2025)</span>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.8.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.8.8.4.1" style="font-size:67%;">Wan2.1¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.8.8.4.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib29" title="Wan: open and advanced large-scale video generative models">52</a><span class="ltx_text" id="A0.T5.8.8.4.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.8.8.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.8.8.5.1" style="font-size:67%;">MultiCamVideo</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.7.7.1.1" style="font-size:67%;">81 frames @ </span><math alttext="672\times 384" class="ltx_Math" display="inline" id="A0.T5.7.7.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">672</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">384</mn></mrow><annotation encoding="application/x-tex">672\times 384</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.8.8.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.8.8.6.1" style="font-size:67%;">Flexible</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.8.8.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.8.8.7.1" style="font-size:67%;">Yes</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.8.8.2" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="&lt;1" class="ltx_Math" display="inline" id="A0.T5.8.8.2.m1" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="0.670em">&lt;</mo><mn mathsize="0.670em">1</mn></mrow><annotation encoding="application/x-tex">&lt;1</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A0.T5.10.10">
<td class="ltx_td ltx_align_left" id="A0.T5.10.10.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.10.10.3.1" style="font-size:67%;">InverseDVS¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.10.10.3.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib43" title="Dynamic view synthesis as an inverse problem">66</a><span class="ltx_text" id="A0.T5.10.10.3.3.2" style="font-size:67%;">]</span></cite><span class="ltx_text" id="A0.T5.10.10.3.4" style="font-size:67%;"> (2025)</span>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.10.10.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.10.10.4.1" style="font-size:67%;">CogVideoX-5B-I2V¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.10.10.4.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib32" title="CogVideoX: text-to-video diffusion models with an expert transformer">64</a><span class="ltx_text" id="A0.T5.10.10.4.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.10.10.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.10.10.5.1" style="font-size:67%;">‚Äì</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.9.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.9.9.1.1" style="font-size:67%;">49 frames @ </span><math alttext="720\times 480" class="ltx_Math" display="inline" id="A0.T5.9.9.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">720</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">480</mn></mrow><annotation encoding="application/x-tex">720\times 480</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.10.10.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.10.10.6.1" style="font-size:67%;">Flexible</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.10.10.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.10.10.7.1" style="font-size:67%;">Flexible</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.10.10.2" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="6\cdot T" class="ltx_Math" display="inline" id="A0.T5.10.10.2.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">6</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">‚ãÖ</mo><mi mathsize="0.670em">T</mi></mrow><annotation encoding="application/x-tex">6\cdot T</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A0.T5.11.11">
<td class="ltx_td ltx_align_left" id="A0.T5.11.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.11.11.2.1" style="font-size:67%;">CogNVS¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.11.11.2.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a><span class="ltx_text" id="A0.T5.11.11.2.3.2" style="font-size:67%;">]</span></cite><span class="ltx_text" id="A0.T5.11.11.2.4" style="font-size:67%;"> (2025)</span>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.11.11.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.11.11.3.1" style="font-size:67%;">CogVideoX-5B-I2V¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.11.11.3.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib32" title="CogVideoX: text-to-video diffusion models with an expert transformer">64</a><span class="ltx_text" id="A0.T5.11.11.3.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.11.11.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.11.11.4.1" style="font-size:67%;">SA-V, TAO, YT-VOS, DAVIS</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.11.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.11.11.1.1" style="font-size:67%;">49 frames @ </span><math alttext="720\times 480" class="ltx_Math" display="inline" id="A0.T5.11.11.1.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">720</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mn mathsize="0.670em">480</mn></mrow><annotation encoding="application/x-tex">720\times 480</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A0.T5.11.11.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.11.11.5.1" style="font-size:67%;">Moving</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.11.11.6" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.11.11.6.1" style="font-size:67%;">Flexible</span></td>
<td class="ltx_td ltx_align_center" id="A0.T5.11.11.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.11.11.7.1" style="font-size:67%;">6</span></td>
</tr>
<tr class="ltx_tr" id="A0.T5.14.14">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A0.T5.14.14.4" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.14.4.1" style="font-size:67%;">Ours (AnyView)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A0.T5.14.14.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.14.14.5.1" style="font-size:67%;">Cosmos-2B¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A0.T5.14.14.5.2.1" style="font-size:67%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib33" title="World simulation with video foundation models for physical ai">35</a><span class="ltx_text" id="A0.T5.14.14.5.3.2" style="font-size:67%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A0.T5.14.14.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_text" id="A0.T5.14.14.6.1" style="font-size:67%;">See Table¬†</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4" style="font-size:67%;" title="Table 4 ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">4</span></a>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A0.T5.13.13.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<math alttext="[9,41]" class="ltx_Math" display="inline" id="A0.T5.12.12.1.m1" intent=":literal"><semantics><mrow><mo maxsize="0.670em" minsize="0.670em">[</mo><mn mathsize="0.670em">9</mn><mo mathsize="0.670em">,</mo><mn mathsize="0.670em">41</mn><mo maxsize="0.670em" minsize="0.670em">]</mo></mrow><annotation encoding="application/x-tex">[9,41]</annotation></semantics></math><span class="ltx_text" id="A0.T5.13.13.2.1" style="font-size:67%;"> frames @ </span><math alttext="576\times[304,384]" class="ltx_Math" display="inline" id="A0.T5.13.13.2.m2" intent=":literal"><semantics><mrow><mn mathsize="0.670em">576</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">√ó</mo><mrow><mo maxsize="0.670em" minsize="0.670em">[</mo><mn mathsize="0.670em">304</mn><mo mathsize="0.670em">,</mo><mn mathsize="0.670em">384</mn><mo maxsize="0.670em" minsize="0.670em">]</mo></mrow></mrow><annotation encoding="application/x-tex">576\times[304,384]</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A0.T5.14.14.7" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.14.7.1" style="font-size:67%;">Flexible</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A0.T5.14.14.8" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A0.T5.14.14.8.1" style="font-size:67%;">Flexible</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A0.T5.14.14.3" style="padding-left:2.0pt;padding-right:2.0pt;"><math alttext="6\cdot T" class="ltx_Math" display="inline" id="A0.T5.14.14.3.m1" intent=":literal"><semantics><mrow><mn mathsize="0.670em">6</mn><mo lspace="0.222em" mathsize="0.670em" rspace="0.222em">‚ãÖ</mo><mi mathsize="0.670em">T</mi></mrow><annotation encoding="application/x-tex">6\cdot T</annotation></semantics></math></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:67%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A0.T5.34.1.1" style="font-size:135%;">Table 5</span>: </span><span class="ltx_text" id="A0.T5.35.2" style="font-size:135%;">
<span class="ltx_text ltx_font_bold" id="A0.T5.35.2.1">Description of baselines.</span>
Some methods are self-supervised¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a>]</cite> and/or training-free¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib43" title="Dynamic view synthesis as an inverse problem">66</a>]</cite>, and hence do not require multi-view video datasets for training.
<em class="ltx_emph ltx_font_italic" id="A0.T5.35.2.2">Input Cam.</em> refers to what kind of video a model can accept as input. <em class="ltx_emph ltx_font_italic" id="A0.T5.35.2.3">Align Start</em> specifies whether the output trajectory needs to start at the same initial frame, in which case we typically apply the smooth interpolation procedure.
See Section¬†<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A5" title="Appendix E Baselines ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">E</span></a> for more information.
<sup class="ltx_sup" id="A0.T5.35.2.4"><span class="ltx_text" id="A0.T5.35.2.4.1" style="font-size:83%;">*</span></sup><span class="ltx_text" id="A0.T5.35.2.5" style="font-size:83%;">TrajCrafter is trained with aligned start, but the official implementation does include limited support for non-aligned starting point inference.
</span>
</span></figcaption>
</figure>
<section class="ltx_appendix ltx_centering" id="A1">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Uncertainty Analysis</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1"><span class="ltx_text" id="A1.p1.1.1" style="font-size:144%;">Figure¬†</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F9" style="font-size:144%;" title="Figure 9 ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">9</span></a><span class="ltx_text" id="A1.p1.1.2" style="font-size:144%;"> showcases how AnyView represents and expresses uncertainty. We calculate this by running the diffusion model multiple times to collect independent samples from the conditional distribution, and plotting the per-pixel diversity between these predictions as a spatial heatmap.
Each generation is conditioned on the same input signals, and represents a possible version of what the other viewpoint </span><em class="ltx_emph ltx_font_italic" id="A1.p1.1.3" style="font-size:144%;">could</em><span class="ltx_text" id="A1.p1.1.4" style="font-size:144%;"> look like.
Even if these outputs are not technically correct, due to the inherent ambiguity of the task at hand, they are still reasonable, realistic, and self-consistent, demonstrating that AnyView learns a powerful probabilistic representation that encodes the natural multimodality of unobserved parts of the world.</span></p>
</div>
</section>
<section class="ltx_appendix ltx_centering" id="A2">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Qualitative Results</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text" id="A2.p1.1.1" style="font-size:144%;">We complement the qualitative results depicted in the main paper with the following:</span></p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1"><span class="ltx_text" id="A2.p2.1.1" style="font-size:144%;">Figure¬†</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F10" style="font-size:144%;" title="Figure 10 ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">10</span></a><span class="ltx_text" id="A2.p2.1.2" style="font-size:144%;"> compares the performance of AnyView against GCD¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A2.p2.1.3.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a><span class="ltx_text" id="A2.p2.1.4.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A2.p2.1.5" style="font-size:144%;"> over increasingly wide horizontal camera displacements, showing that AnyView maintains better spatio-temporal consistency over large viewpoint changes.</span></p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1"><span class="ltx_text" id="A2.p3.1.1" style="font-size:144%;">Figure¬†</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F11" style="font-size:144%;" title="Figure 11 ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">11</span></a><span class="ltx_text" id="A2.p3.1.2" style="font-size:144%;"> shows top-down view synthesis on real-world (</span><em class="ltx_emph ltx_font_italic" id="A2.p3.1.3" style="font-size:144%;">DDAD</em><span class="ltx_text" id="A2.p3.1.4" style="font-size:144%;">) driving scenes, where we also compare against the GCD baseline. This effectively tests each model‚Äôs sim-to-real trajectory generalization capability, since the only training videos corresponding to similar viewpoint configurations (albeit still not the same) come from synthetic data (</span><em class="ltx_emph ltx_font_italic" id="A2.p3.1.5" style="font-size:144%;">ParallelDomain</em><span class="ltx_text" id="A2.p3.1.6" style="font-size:144%;">).</span></p>
</div>
<div class="ltx_para" id="A2.p4">
<p class="ltx_p" id="A2.p4.1"><span class="ltx_text" id="A2.p4.1.1" style="font-size:144%;">Moreover, we include all figures present in the paper as videos in the project webpage: </span><a class="ltx_ref ltx_href" href="https://tri-ml.github.io/AnyView/" style="font-size:144%;" title="">tri-ml.github.io/AnyView</a><span class="ltx_text" id="A2.p4.1.2" style="font-size:144%;">.
The highly encourage the reader to browse these results, since it is difficult otherwise to communicate 4D results through 2D PDF files.</span></p>
</div>
</section>
<section class="ltx_appendix ltx_centering" id="A3">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Training Datasets</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1"><span class="ltx_text" id="A3.p1.1.1" style="font-size:144%;">Here, we provide additional details about the AnyView training mixture, also summarized in Table¬†</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T4" style="font-size:144%;" title="Table 4 ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="A3.p1.1.2" style="font-size:144%;">. For all training datasets, we randomly selected around 10% of sequences to serve as </span><em class="ltx_emph ltx_font_italic" id="A3.p1.1.3" style="font-size:144%;">in-distribution</em><span class="ltx_text" id="A3.p1.1.4" style="font-size:144%;"> validation, from which many of the official AnyViewBench test splits were curated.</span></p>
</div>
<div class="ltx_para" id="A3.p2">
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i1.p1.1.1" style="font-size:144%;">Driving:</span><span class="ltx_text" id="A3.I1.i1.p1.1.2" style="font-size:144%;">
Most autonomous driving rigs have a set of well-calibrated RGB cameras mounted around the vehicle, providing plenty of real-world, </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i1.p1.1.3" style="font-size:144%;">egocentric</em><span class="ltx_text" id="A3.I1.i1.p1.1.4" style="font-size:144%;"> (outward-facing), temporally synchronized video footage.
We additionally capitalize on synthetic data to provide </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i1.p1.1.5" style="font-size:144%;">exocentric</em><span class="ltx_text" id="A3.I1.i1.p1.1.6" style="font-size:144%;"> (inward-facing) viewpoints that otherwise do not naturally occur in such datasets.
For training, we use the </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i1.p1.1.7" style="font-size:144%;">Woven Planet (Lyft) Level 5</em><span class="ltx_text" id="A3.I1.i1.p1.1.8" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i1.p1.1.9.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib12" title="One thousand and one hours: self-driving motion prediction dataset.">22</a><span class="ltx_text" id="A3.I1.i1.p1.1.10.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i1.p1.1.11" style="font-size:144%;">, </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i1.p1.1.12" style="font-size:144%;">ParallelDomain</em><span class="ltx_text" id="A3.I1.i1.p1.1.13" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i1.p1.1.14.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib76" title="Geometric unsupervised domain adaptation for semantic segmentation">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib75" title="Learning optical flow, depth, and scene flow without real-world labels">17</a><span class="ltx_text" id="A3.I1.i1.p1.1.15.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i1.p1.1.16" style="font-size:144%;">, and </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i1.p1.1.17" style="font-size:144%;">Waymo Open</em><span class="ltx_text" id="A3.I1.i1.p1.1.18" style="font-size:144%;"> (Perception)¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i1.p1.1.19.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib15" title="Scalability in perception for autonomous driving: waymo open dataset">48</a><span class="ltx_text" id="A3.I1.i1.p1.1.20.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i1.p1.1.21" style="font-size:144%;"> datasets.</span></p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i2.p1.1.1" style="font-size:144%;">Robotics:</span><span class="ltx_text" id="A3.I1.i2.p1.1.2" style="font-size:144%;">
To enable our model to operate in embodied AI contexts, we use </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i2.p1.1.3" style="font-size:144%;">DROID</em><span class="ltx_text" id="A3.I1.i2.p1.1.4" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i2.p1.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib16" title="DROID: a large-scale in-the-wild robot manipulation dataset">28</a><span class="ltx_text" id="A3.I1.i2.p1.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i2.p1.1.7" style="font-size:144%;"> with the improved calibration parameters provided in¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i2.p1.1.8.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib74" title="Scaling-up automatic camera calibration for droid dataset: a study using foundation models and existing deep-learning tools">26</a><span class="ltx_text" id="A3.I1.i2.p1.1.9.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i2.p1.1.10" style="font-size:144%;">.
This dataset was captured at many locations around the world, and laboratories tend to have significantly different appearance, lighting, camera positions, and calibration quality.
We also include a large collection of internally recorded bimanual and single-arm tabletop robotics demonstrations, denoted </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i2.p1.1.11" style="font-size:144%;">LBM</em><span class="ltx_text" id="A3.I1.i2.p1.1.12" style="font-size:144%;">.</span></p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i3.p1.1.1" style="font-size:144%;">3D:</span><span class="ltx_text" id="A3.I1.i3.p1.1.2" style="font-size:144%;">
Because multi-view video is expensive to collect and therefore rather small in overall scale, we leverage single-view, posed videos of static scenes as an additional data source. Following¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i3.p1.1.3.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a><span class="ltx_text" id="A3.I1.i3.p1.1.4.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i3.p1.1.5" style="font-size:144%;">, we adopt </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i3.p1.1.6" style="font-size:144%;">DL3DV-10K</em><span class="ltx_text" id="A3.I1.i3.p1.1.7" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i3.p1.1.8.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib23" title="Dl3dv-10k: a large-scale scene dataset for deep learning-based 3d vision">33</a><span class="ltx_text" id="A3.I1.i3.p1.1.9.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i3.p1.1.10" style="font-size:144%;"> and </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i3.p1.1.11" style="font-size:144%;">RealEstate-10K</em><span class="ltx_text" id="A3.I1.i3.p1.1.12" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i3.p1.1.13.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib19" title="Stereo magnification: learning view synthesis using multiplane images">72</a><span class="ltx_text" id="A3.I1.i3.p1.1.14.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i3.p1.1.15" style="font-size:144%;">.
We also include </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i3.p1.1.16" style="font-size:144%;">ScanNet</em><span class="ltx_text" id="A3.I1.i3.p1.1.17" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i3.p1.1.18.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib21" title="ScanNet: richly-annotated 3d reconstructions of indoor scenes">10</a><span class="ltx_text" id="A3.I1.i3.p1.1.19.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i3.p1.1.20" style="font-size:144%;">, </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i3.p1.1.21" style="font-size:144%;">TartanAir</em><span class="ltx_text" id="A3.I1.i3.p1.1.22" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i3.p1.1.23.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib20" title="Tartanair: a dataset to push the limits of visual slam">54</a><span class="ltx_text" id="A3.I1.i3.p1.1.24.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i3.p1.1.25" style="font-size:144%;">, and </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i3.p1.1.26" style="font-size:144%;">WildRGB-D</em><span class="ltx_text" id="A3.I1.i3.p1.1.27" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i3.p1.1.28.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib17" title="RGBD objects in the wild: scaling real-world 3d object learning from rgb-d videos">59</a><span class="ltx_text" id="A3.I1.i3.p1.1.29.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i3.p1.1.30" style="font-size:144%;">.
Because these environments are not dynamic, each frame can essentially be handled as if it were an independent camera, without any inherent temporal ordering.
We randomly sample non-overlapping segments of 41 frames at training time, and treat them as two separate viewpoints.</span></p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A3.I1.i4.p1">
<p class="ltx_p" id="A3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A3.I1.i4.p1.1.1" style="font-size:144%;">Other:</span><span class="ltx_text" id="A3.I1.i4.p1.1.2" style="font-size:144%;">
This catch-all category covers all remaining multi-view video datasets, including </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i4.p1.1.3" style="font-size:144%;">Kubric-4D</em><span class="ltx_text" id="A3.I1.i4.p1.1.4" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i4.p1.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a><span class="ltx_text" id="A3.I1.i4.p1.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i4.p1.1.7" style="font-size:144%;"> and </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i4.p1.1.8" style="font-size:144%;">Kubric-5D</em><span class="ltx_text" id="A3.I1.i4.p1.1.9" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i4.p1.1.10.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a><span class="ltx_text" id="A3.I1.i4.p1.1.11.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i4.p1.1.12" style="font-size:144%;"> with synthetic multi-object interaction and physics, as well as </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i4.p1.1.13" style="font-size:144%;">i.e</em><span class="ltx_text" id="A3.I1.i4.p1.1.14" style="font-size:144%;">.</span><span class="ltx_text" id="A3.I1.i4.p1.1.15"></span><span class="ltx_text" id="A3.I1.i4.p1.1.16" style="font-size:144%;"> </span><em class="ltx_emph ltx_font_italic" id="A3.I1.i4.p1.1.17" style="font-size:144%;">Ego-Exo4D</em><span class="ltx_text" id="A3.I1.i4.p1.1.18" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.I1.i4.p1.1.19.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib22" title="Ego-exo4d: understanding skilled human activity from first- and third-person perspectives">14</a><span class="ltx_text" id="A3.I1.i4.p1.1.20.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.I1.i4.p1.1.21" style="font-size:144%;">, depicting complex human activities in cluttered scenes.</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A3.p3">
<p class="ltx_p" id="A3.p3.1"><span class="ltx_text" id="A3.p3.1.1" style="font-size:144%;">In Figure¬†</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.F12" style="font-size:144%;" title="Figure 12 ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">12</span></a><span class="ltx_text" id="A3.p3.1.2" style="font-size:144%;">, we provide additional examples of input and target camera poses of various episodes across training and evaluation sets to illustrate the diversity.</span></p>
</div>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:144%;">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Kubric-5D</h3>
<div class="ltx_para" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.4"><em class="ltx_emph ltx_font_italic" id="A3.SS1.p1.4.1" style="font-size:144%;">Kubric-5D</em><span class="ltx_text" id="A3.SS1.p1.4.2" style="font-size:144%;"> is our newly introduced extension of Kubric-4D, with a new set of clips rendered with significantly more complex camera configuration and object placement. Compared to Kubric-4D, in which cameras are static with constant focal length, facing a small cluster of free-falling objects, </span><em class="ltx_emph ltx_font_italic" id="A3.SS1.p1.4.3" style="font-size:144%;">Kubric-5D</em><span class="ltx_text" id="A3.SS1.p1.4.4" style="font-size:144%;"> introduces dynamic cameras with varying focal lengths as well as varying object placement density, with the intent to enrich the dynamic information captured in the videos for the model to learn from. Specifically, we renedered </span><math alttext="1000" class="ltx_Math" display="inline" id="A3.SS1.p1.1.m1" intent=":literal"><semantics><mn mathsize="1.440em">1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math><span class="ltx_text" id="A3.SS1.p1.4.5" style="font-size:144%;"> randomized scenes, each scene containing </span><math alttext="16" class="ltx_Math" display="inline" id="A3.SS1.p1.2.m2" intent=":literal"><semantics><mn mathsize="1.440em">16</mn><annotation encoding="application/x-tex">16</annotation></semantics></math><span class="ltx_text" id="A3.SS1.p1.4.6" style="font-size:144%;"> cameras spawn at locations evenly distributed around the world center, and each camera‚Äôs trajectory type independently sampled; as for the focal length, 1/3 chance all 16 cameras in a scene share a preset value, 1/3 chance share a randomly sampled value, and 1/3 chance each camera has an independently sampled value. Combining a geometry selection such as </span><span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.4.7" style="font-size:144%;">spiral</span><span class="ltx_text" id="A3.SS1.p1.4.8" style="font-size:144%;">, </span><span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.4.9" style="font-size:144%;">radial</span><span class="ltx_text" id="A3.SS1.p1.4.10" style="font-size:144%;">, </span><span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.4.11" style="font-size:144%;">line</span><span class="ltx_text" id="A3.SS1.p1.4.12" style="font-size:144%;">, </span><span class="ltx_text ltx_font_typewriter" id="A3.SS1.p1.4.13" style="font-size:144%;">lissajous</span><span class="ltx_text" id="A3.SS1.p1.4.14" style="font-size:144%;">, </span><em class="ltx_emph ltx_font_italic" id="A3.SS1.p1.4.15" style="font-size:144%;">etc</em><span class="ltx_text" id="A3.SS1.p1.4.16" style="font-size:144%;">.</span><span class="ltx_text" id="A3.SS1.p1.4.17"></span><span class="ltx_text" id="A3.SS1.p1.4.18" style="font-size:144%;">, with the camera‚Äôs viewing direction, there are </span><math alttext="16" class="ltx_Math" display="inline" id="A3.SS1.p1.3.m3" intent=":literal"><semantics><mn mathsize="1.440em">16</mn><annotation encoding="application/x-tex">16</annotation></semantics></math><span class="ltx_text" id="A3.SS1.p1.4.19" style="font-size:144%;"> different types of trajectories (including being static). The number of objects as well as spawn area are also randomly sampled for each scene, covering the possibilities of denser/sparser clustering/scattering. All videos are rendered at </span><math alttext="576\times 384" class="ltx_Math" display="inline" id="A3.SS1.p1.4.m4" intent=":literal"><semantics><mrow><mn mathsize="1.440em">576</mn><mo lspace="0.222em" mathsize="1.440em" rspace="0.222em">√ó</mo><mn mathsize="1.440em">384</mn></mrow><annotation encoding="application/x-tex">576\times 384</annotation></semantics></math><span class="ltx_text" id="A3.SS1.p1.4.20" style="font-size:144%;"> resolution with 24 FPS for 60 seconds, using the Kubric engine </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.SS1.p1.4.21.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib24" title="Kubric: a scalable dataset generator">15</a><span class="ltx_text" id="A3.SS1.p1.4.22.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.SS1.p1.4.23" style="font-size:144%;"> and code adapted from </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A3.SS1.p1.4.24.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a><span class="ltx_text" id="A3.SS1.p1.4.25.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A3.SS1.p1.4.26" style="font-size:144%;">.</span></p>
</div>
</section>
</section>
<section class="ltx_appendix ltx_centering" id="A4">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Evaluation Datasets</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1"><span class="ltx_text" id="A4.p1.1.1" style="font-size:144%;">Here, we describe the logic of which datasets and subsets are held out for evaluation purposes.</span></p>
</div>
<div class="ltx_para" id="A4.p2">
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A4.I1.i1.p1">
<p class="ltx_p" id="A4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i1.p1.1.1" style="font-size:144%;">Driving:</span><span class="ltx_text" id="A4.I1.i1.p1.1.2" style="font-size:144%;">
The training sets for </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i1.p1.1.3" style="font-size:144%;">Lyft</em><span class="ltx_text" id="A4.I1.i1.p1.1.4" style="font-size:144%;"> and </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i1.p1.1.5" style="font-size:144%;">Waymo</em><span class="ltx_text" id="A4.I1.i1.p1.1.6" style="font-size:144%;"> are both recorded exclusively in the United States¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A4.I1.i1.p1.1.7.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib12" title="One thousand and one hours: self-driving motion prediction dataset.">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib15" title="Scalability in perception for autonomous driving: waymo open dataset">48</a><span class="ltx_text" id="A4.I1.i1.p1.1.8.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A4.I1.i1.p1.1.9" style="font-size:144%;">.
We hold out </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i1.p1.1.10" style="font-size:144%;">Argoverse</em><span class="ltx_text" id="A4.I1.i1.p1.1.11" style="font-size:144%;">, also recorded in the USA¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A4.I1.i1.p1.1.12.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib13" title="Argoverse 2: next generation datasets for self-driving perception and forecasting">56</a><span class="ltx_text" id="A4.I1.i1.p1.1.13.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A4.I1.i1.p1.1.14" style="font-size:144%;"> (albeit in mostly non-overlapping cities), because it has portrait videos as the front camera, which do not exist during training.
We also hold out </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i1.p1.1.15" style="font-size:144%;">DDAD</em><span class="ltx_text" id="A4.I1.i1.p1.1.16" style="font-size:144%;">, because it contains videos recorded in Japan¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A4.I1.i1.p1.1.17.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib14" title="3D packing for self-supervised monocular depth estimation">16</a><span class="ltx_text" id="A4.I1.i1.p1.1.18.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A4.I1.i1.p1.1.19" style="font-size:144%;">.</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A4.I1.i2.p1">
<p class="ltx_p" id="A4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i2.p1.1.1" style="font-size:144%;">Robotics:</span><span class="ltx_text" id="A4.I1.i2.p1.1.2" style="font-size:144%;">
While episodes in LBM are recorded across multiple stations in both simulation and the real world, </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i2.p1.1.3" style="font-size:144%;">DROID</em><span class="ltx_text" id="A4.I1.i2.p1.1.4" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A4.I1.i2.p1.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib16" title="DROID: a large-scale in-the-wild robot manipulation dataset">28</a><span class="ltx_text" id="A4.I1.i2.p1.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A4.I1.i2.p1.1.7" style="font-size:144%;"> has more visual diversity. We decide to hold out all videos belonging to 2 out of 13 institutions (Gupta Lab, ILIAD) for zero-shot testing.</span></p>
</div>
</li>
<li class="ltx_item" id="A4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A4.I1.i3.p1">
<p class="ltx_p" id="A4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.i3.p1.1.1" style="font-size:144%;">Human Activity:</span><span class="ltx_text" id="A4.I1.i3.p1.1.2" style="font-size:144%;">
One natural choice for this category is </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i3.p1.1.3" style="font-size:144%;">Ego-Exo4D</em><span class="ltx_text" id="A4.I1.i3.p1.1.4" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A4.I1.i3.p1.1.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib22" title="Ego-exo4d: understanding skilled human activity from first- and third-person perspectives">14</a><span class="ltx_text" id="A4.I1.i3.p1.1.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A4.I1.i3.p1.1.7" style="font-size:144%;">, which has highly challenging, real-world scenes, often involving multiple humans, recorded by 4 to 5 inward-facing cameras. We hold out two </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i3.p1.1.8" style="font-size:144%;">institutions</em><span class="ltx_text" id="A4.I1.i3.p1.1.9" style="font-size:144%;"> (FAIR, NUS), two </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i3.p1.1.10" style="font-size:144%;">activities</em><span class="ltx_text" id="A4.I1.i3.p1.1.11" style="font-size:144%;"> (cpr, guitar), and three </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i3.p1.1.12" style="font-size:144%;">institution-activity pairs</em><span class="ltx_text" id="A4.I1.i3.p1.1.13" style="font-size:144%;"> (basketball at Uniandes, piano at Indiana, soccer at UTokyo). Notably, </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i3.p1.1.14" style="font-size:144%;">cpr at NUS</em><span class="ltx_text" id="A4.I1.i3.p1.1.15" style="font-size:144%;"> becomes the ‚Äúmost zero-shot‚Äù combination since both the activity and institution are entirely unseen. Since the cameras used to collect the dataset have noticeable distortion, we implement a non-pinhole camera model to generate the actual viewing rays when given a grid, based on the official code examples that undistort the frames using coefficients stored in each sample.
We further evaluate on videos from the eight exocentric cameras of the </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i3.p1.1.16" style="font-size:144%;">AssemblyHands</em><span class="ltx_text" id="A4.I1.i3.p1.1.17" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A4.I1.i3.p1.1.18.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib44" title="AssemblyHands: towards egocentric activity understanding via 3d hand pose estimation">37</a><span class="ltx_text" id="A4.I1.i3.p1.1.19.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A4.I1.i3.p1.1.20" style="font-size:144%;"> dataset, a subset of </span><em class="ltx_emph ltx_font_italic" id="A4.I1.i3.p1.1.21" style="font-size:144%;">Assembly101</em><span class="ltx_text" id="A4.I1.i3.p1.1.22" style="font-size:144%;">¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A4.I1.i3.p1.1.23.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib77" title="Assembly101: a large-scale multi-view video dataset for understanding procedural activities">45</a><span class="ltx_text" id="A4.I1.i3.p1.1.24.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A4.I1.i3.p1.1.25" style="font-size:144%;"> that has calibrated camera intrinsics and extrinsics.
The dataset records dexterous hand-object interactions during the assembly and disassembly of pull-apart toys, providing a challenging zero-shot test setting for AnyView.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix ltx_centering" id="A5">
<h2 class="ltx_title ltx_title_appendix" style="font-size:144%;">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Baselines</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1"><span class="ltx_text" id="A5.p1.1.1" style="font-size:144%;">Here, we outline how each baseline was adapted to AnyViewBench.
In each case, when a method predicts </span><em class="ltx_emph ltx_font_italic" id="A5.p1.1.2" style="font-size:144%;">fewer</em><span class="ltx_text" id="A5.p1.1.3" style="font-size:144%;"> frames than the evaluation episode, we run the model multiple times in a sliding window fashion until the full video is covered, and average metrics such that each frame is used exactly once.
In the opposite scenario, </span><em class="ltx_emph ltx_font_italic" id="A5.p1.1.4" style="font-size:144%;">i.e</em><span class="ltx_text" id="A5.p1.1.5" style="font-size:144%;">.</span><span class="ltx_text" id="A5.p1.1.6"></span><span class="ltx_text" id="A5.p1.1.7" style="font-size:144%;"> when a method predicts </span><em class="ltx_emph ltx_font_italic" id="A5.p1.1.8" style="font-size:144%;">more</em><span class="ltx_text" id="A5.p1.1.9" style="font-size:144%;"> frames than necessary, we simply discard the superfluous ones.</span></p>
</div>
<div class="ltx_para" id="A5.p2">
<p class="ltx_p" id="A5.p2.1"><span class="ltx_text" id="A5.p2.1.1" style="font-size:144%;">We provide the evaluated methods with ground truth camera pose and intrinsics, and when a method needs depth we use DepthAnythingV2 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A5.p2.1.2.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib1" title="Depth anything v2">63</a><span class="ltx_text" id="A5.p2.1.3.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A5.p2.1.4" style="font-size:144%;"> to calculate metric depths maps since the ground truth pose we use are in metric space.</span></p>
</div>
<div class="ltx_para" id="A5.p3">
<p class="ltx_p" id="A5.p3.1"><span class="ltx_text" id="A5.p3.1.1" style="font-size:144%;">Some methods are trained to operate with smooth camera trajectories,
and their performance degrades when there is minimal overlap between the target and input trajectories in the beginning of the videos.
However, many trajectories in AnyViewBench exhibit precisely such limited overlap.
To address this, we use the estimated depth to smoothly interpolate between the input view and the first target view, freezing the first frame for a short while until the target pose is reached, then concatenate these interpolated frames with the actual input sequence.</span></p>
</div>
<div class="ltx_para" id="A5.p4">
<ul class="ltx_itemize" id="A5.I1">
<li class="ltx_item" id="A5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I1.i1.p1">
<p class="ltx_p" id="A5.I1.i1.p1.5"><span class="ltx_text ltx_font_bold" id="A5.I1.i1.p1.5.1" style="font-size:144%;">Generative Camera Dolly (GCD)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib9" title="Generative camera dolly: extreme monocular dynamic novel view synthesis">50</a>]</cite>:</span><span class="ltx_text" id="A5.I1.i1.p1.5.2" style="font-size:144%;">
This model only supports inference with 14 frames at a time (both in terms of input and output video), and with 3 degrees of freedom.
It assumes a spherical coordinate system </span><math alttext="(\phi,\theta,r)" class="ltx_Math" display="inline" id="A5.I1.i1.p1.1.m1" intent=":literal"><semantics><mrow><mo maxsize="1.440em" minsize="1.440em">(</mo><mi mathsize="1.440em">œï</mi><mo mathsize="1.440em">,</mo><mi mathsize="1.440em">Œ∏</mi><mo mathsize="1.440em">,</mo><mi mathsize="1.440em">r</mi><mo maxsize="1.440em" minsize="1.440em">)</mo></mrow><annotation encoding="application/x-tex">(\phi,\theta,r)</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.p1.5.3" style="font-size:144%;">, where the camera controls provided to the network are the relative azimuth angle </span><math alttext="\Delta\phi" class="ltx_Math" display="inline" id="A5.I1.i1.p1.2.m2" intent=":literal"><semantics><mrow><mi mathsize="1.440em" mathvariant="normal">Œî</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi mathsize="1.440em">œï</mi></mrow><annotation encoding="application/x-tex">\Delta\phi</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.p1.5.4" style="font-size:144%;">, relative elevation angle </span><math alttext="\Delta\theta" class="ltx_Math" display="inline" id="A5.I1.i1.p1.3.m3" intent=":literal"><semantics><mrow><mi mathsize="1.440em" mathvariant="normal">Œî</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi mathsize="1.440em">Œ∏</mi></mrow><annotation encoding="application/x-tex">\Delta\theta</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.p1.5.5" style="font-size:144%;">, and relative radius </span><math alttext="\Delta r" class="ltx_Math" display="inline" id="A5.I1.i1.p1.4.m4" intent=":literal"><semantics><mrow><mi mathsize="1.440em" mathvariant="normal">Œî</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi mathsize="1.440em">r</mi></mrow><annotation encoding="application/x-tex">\Delta r</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.p1.5.6" style="font-size:144%;">.
The input and target viewpoints always aim at the center of the scene.
To reduce the </span><math alttext="6\cdot T" class="ltx_Math" display="inline" id="A5.I1.i1.p1.5.m5" intent=":literal"><semantics><mrow><mn mathsize="1.440em">6</mn><mo lspace="0.222em" mathsize="1.440em" rspace="0.222em">‚ãÖ</mo><mi mathsize="1.440em">T</mi></mrow><annotation encoding="application/x-tex">6\cdot T</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.p1.5.7" style="font-size:144%;">-DOF AnyViewBench camera trajectories into the 3-DOF conditioning space of GCD, information loss is unavoidable, so we apply the following approximate projection:</span></p>
</div>
<div class="ltx_para" id="A5.I1.i1.p2">
<ol class="ltx_enumerate" id="A5.I1.i1.I1">
<li class="ltx_item" id="A5.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="A5.I1.i1.I1.i1.p1">
<p class="ltx_p" id="A5.I1.i1.I1.i1.p1.2"><span class="ltx_text" id="A5.I1.i1.I1.i1.p1.2.1" style="font-size:144%;">Take the forward-looking vector </span><math alttext="f=(f_{x},f_{y},f_{z})" class="ltx_Math" display="inline" id="A5.I1.i1.I1.i1.p1.1.m1" intent=":literal"><semantics><mrow><mi mathsize="1.440em">f</mi><mo mathsize="1.440em">=</mo><mrow><mo maxsize="1.440em" minsize="1.440em">(</mo><msub><mi mathsize="1.440em">f</mi><mi mathsize="1.440em">x</mi></msub><mo mathsize="1.440em">,</mo><msub><mi mathsize="1.440em">f</mi><mi mathsize="1.440em">y</mi></msub><mo mathsize="1.440em">,</mo><msub><mi mathsize="1.440em">f</mi><mi mathsize="1.440em">z</mi></msub><mo maxsize="1.440em" minsize="1.440em">)</mo></mrow></mrow><annotation encoding="application/x-tex">f=(f_{x},f_{y},f_{z})</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.I1.i1.p1.2.2" style="font-size:144%;"> (= third column of the extrinsics matrix) and translation vector </span><math alttext="t=(t_{x},t_{y},t_{z})" class="ltx_Math" display="inline" id="A5.I1.i1.I1.i1.p1.2.m2" intent=":literal"><semantics><mrow><mi mathsize="1.440em">t</mi><mo mathsize="1.440em">=</mo><mrow><mo maxsize="1.440em" minsize="1.440em">(</mo><msub><mi mathsize="1.440em">t</mi><mi mathsize="1.440em">x</mi></msub><mo mathsize="1.440em">,</mo><msub><mi mathsize="1.440em">t</mi><mi mathsize="1.440em">y</mi></msub><mo mathsize="1.440em">,</mo><msub><mi mathsize="1.440em">t</mi><mi mathsize="1.440em">z</mi></msub><mo maxsize="1.440em" minsize="1.440em">)</mo></mrow></mrow><annotation encoding="application/x-tex">t=(t_{x},t_{y},t_{z})</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.I1.i1.p1.2.3" style="font-size:144%;"> (= last column of the extrinsics matrix) of the camera pose of each viewpoint of either the middle or last frame (depending on the dataset) of the video.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A5.I1.i1.I1.i2.p1">
<p class="ltx_p" id="A5.I1.i1.I1.i2.p1.2"><span class="ltx_text" id="A5.I1.i1.I1.i2.p1.2.1" style="font-size:144%;">Measure the azimuth angle of each vector: </span><math alttext="\phi=\arctan\left(\frac{f_{y}}{f_{x}}\right)" class="ltx_Math" display="inline" id="A5.I1.i1.I1.i2.p1.1.m1" intent=":literal"><semantics><mrow><mi mathsize="1.440em">œï</mi><mo mathsize="1.440em">=</mo><mrow><mi mathsize="1.440em">arctan</mi><mo>‚Å°</mo><mrow><mo>(</mo><mfrac><msub><mi mathsize="1.440em">f</mi><mi mathsize="1.440em">y</mi></msub><msub><mi mathsize="1.440em">f</mi><mi mathsize="1.440em">x</mi></msub></mfrac><mo>)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\phi=\arctan\left(\frac{f_{y}}{f_{x}}\right)</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.I1.i2.p1.2.2" style="font-size:144%;">; the difference between both values is then </span><math alttext="\Delta\phi" class="ltx_Math" display="inline" id="A5.I1.i1.I1.i2.p1.2.m2" intent=":literal"><semantics><mrow><mi mathsize="1.440em" mathvariant="normal">Œî</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi mathsize="1.440em">œï</mi></mrow><annotation encoding="application/x-tex">\Delta\phi</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.I1.i2.p1.2.3" style="font-size:144%;">.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A5.I1.i1.I1.i3.p1">
<p class="ltx_p" id="A5.I1.i1.I1.i3.p1.2"><span class="ltx_text" id="A5.I1.i1.I1.i3.p1.2.1" style="font-size:144%;">Measure the elevation angle of each vector: </span><math alttext="\theta=-\arctan\left(\frac{f_{z}}{\sqrt{f_{x}^{2}+f_{y}^{2}}}\right)" class="ltx_Math" display="inline" id="A5.I1.i1.I1.i3.p1.1.m1" intent=":literal"><semantics><mrow><mi mathsize="1.440em">Œ∏</mi><mo mathsize="1.440em">=</mo><mrow><mo mathsize="1.440em" rspace="0.167em">‚àí</mo><mrow><mi mathsize="1.440em">arctan</mi><mo>‚Å°</mo><mrow><mo>(</mo><mfrac><msub><mi mathsize="1.440em">f</mi><mi mathsize="1.440em">z</mi></msub><msqrt><mrow><msubsup><mi mathsize="1.440em">f</mi><mi mathsize="1.440em">x</mi><mn mathsize="1.440em">2</mn></msubsup><mo mathsize="1.440em">+</mo><msubsup><mi mathsize="1.440em">f</mi><mi mathsize="1.440em">y</mi><mn mathsize="1.440em">2</mn></msubsup></mrow></msqrt></mfrac><mo>)</mo></mrow></mrow></mrow></mrow><annotation encoding="application/x-tex">\theta=-\arctan\left(\frac{f_{z}}{\sqrt{f_{x}^{2}+f_{y}^{2}}}\right)</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.I1.i3.p1.2.2" style="font-size:144%;">; the difference between both values is then </span><math alttext="\Delta\theta" class="ltx_Math" display="inline" id="A5.I1.i1.I1.i3.p1.2.m2" intent=":literal"><semantics><mrow><mi mathsize="1.440em" mathvariant="normal">Œî</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi mathsize="1.440em">Œ∏</mi></mrow><annotation encoding="application/x-tex">\Delta\theta</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.I1.i3.p1.2.3" style="font-size:144%;">.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="A5.I1.i1.I1.i4.p1">
<p class="ltx_p" id="A5.I1.i1.I1.i4.p1.2"><span class="ltx_text" id="A5.I1.i1.I1.i4.p1.2.1" style="font-size:144%;">Measure the Euclidean distance from each camera origin to the scene origin: </span><math alttext="r=\sqrt{t_{x}^{2}+t_{y}^{2}+t_{z}^{2}}" class="ltx_Math" display="inline" id="A5.I1.i1.I1.i4.p1.1.m1" intent=":literal"><semantics><mrow><mi mathsize="1.440em">r</mi><mo mathsize="1.440em">=</mo><msqrt><mrow><msubsup><mi mathsize="1.440em">t</mi><mi mathsize="1.440em">x</mi><mn mathsize="1.440em">2</mn></msubsup><mo mathsize="1.440em">+</mo><msubsup><mi mathsize="1.440em">t</mi><mi mathsize="1.440em">y</mi><mn mathsize="1.440em">2</mn></msubsup><mo mathsize="1.440em">+</mo><msubsup><mi mathsize="1.440em">t</mi><mi mathsize="1.440em">z</mi><mn mathsize="1.440em">2</mn></msubsup></mrow></msqrt></mrow><annotation encoding="application/x-tex">r=\sqrt{t_{x}^{2}+t_{y}^{2}+t_{z}^{2}}</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.I1.i4.p1.2.2" style="font-size:144%;">; the difference between both values is then </span><math alttext="\Delta r" class="ltx_Math" display="inline" id="A5.I1.i1.I1.i4.p1.2.m2" intent=":literal"><semantics><mrow><mi mathsize="1.440em" mathvariant="normal">Œî</mi><mo lspace="0em" rspace="0em">‚Äã</mo><mi mathsize="1.440em">r</mi></mrow><annotation encoding="application/x-tex">\Delta r</annotation></semantics></math><span class="ltx_text" id="A5.I1.i1.I1.i4.p1.2.3" style="font-size:144%;">.</span></p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="A5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I1.i2.p1">
<p class="ltx_p" id="A5.I1.i2.p1.4"><span class="ltx_text ltx_font_bold" id="A5.I1.i2.p1.4.1" style="font-size:144%;">Trajectory Attention¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib7" title="Trajectory attention for fine-grained video motion control">60</a>]</cite>:</span><span class="ltx_text" id="A5.I1.i2.p1.4.2" style="font-size:144%;">
TrajectoryAttention takes a variable number of input image frames at a resolution of </span><math alttext="1024\times 576" class="ltx_Math" display="inline" id="A5.I1.i2.p1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="1.440em">1024</mn><mo lspace="0.222em" mathsize="1.440em" rspace="0.222em">√ó</mo><mn mathsize="1.440em">576</mn></mrow><annotation encoding="application/x-tex">1024\times 576</annotation></semantics></math><span class="ltx_text" id="A5.I1.i2.p1.4.3" style="font-size:144%;">. Given </span><math alttext="N" class="ltx_Math" display="inline" id="A5.I1.i2.p1.2.m2" intent=":literal"><semantics><mi mathsize="1.440em">N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math><span class="ltx_text" id="A5.I1.i2.p1.4.4" style="font-size:144%;"> input images, we provide the </span><math alttext="N" class="ltx_Math" display="inline" id="A5.I1.i2.p1.3.m3" intent=":literal"><semantics><mi mathsize="1.440em">N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math><span class="ltx_text" id="A5.I1.i2.p1.4.5" style="font-size:144%;"> warped images from the target views along with the first image from the source view (</span><math alttext="N+1" class="ltx_Math" display="inline" id="A5.I1.i2.p1.4.m4" intent=":literal"><semantics><mrow><mi mathsize="1.440em">N</mi><mo mathsize="1.440em">+</mo><mn mathsize="1.440em">1</mn></mrow><annotation encoding="application/x-tex">N+1</annotation></semantics></math><span class="ltx_text" id="A5.I1.i2.p1.4.6" style="font-size:144%;"> images in total). Since our trajectories are represented in metric space, we opted to use the metric version of DepthAnythingV2, unlike the non-metric model used in the original implementation. We also modified the original warping code, which only supported transformations around the source view, so that it can handle arbitrary trajectories.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I1.i3.p1">
<p class="ltx_p" id="A5.I1.i3.p1.2"><span class="ltx_text ltx_font_bold" id="A5.I1.i3.p1.2.1" style="font-size:144%;">GEN3C¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib5" title="GEN3C: 3d-informed world-consistent video generation with precise camera control">43</a>]</cite>:</span><span class="ltx_text" id="A5.I1.i3.p1.2.2" style="font-size:144%;">
GEN3C supports number of frames in </span><math alttext="120*N+1" class="ltx_Math" display="inline" id="A5.I1.i3.p1.1.m1" intent=":literal"><semantics><mrow><mrow><mn mathsize="1.440em">120</mn><mo lspace="0.222em" mathsize="1.440em" rspace="0.222em">‚àó</mo><mi mathsize="1.440em">N</mi></mrow><mo mathsize="1.440em">+</mo><mn mathsize="1.440em">1</mn></mrow><annotation encoding="application/x-tex">120*N+1</annotation></semantics></math><span class="ltx_text" id="A5.I1.i3.p1.2.3" style="font-size:144%;"> pattern; we choose 121 as it is enough to cover the length of clips in all evaluated datasets. To meet the length requirement, each input video is padded to 121 frames using the last frame, and metrics are only computed on the original leading frames from the output. Following the official inference code, the videos are first resized and predicted in </span><math alttext="1280\times 704" class="ltx_Math" display="inline" id="A5.I1.i3.p1.2.m2" intent=":literal"><semantics><mrow><mn mathsize="1.440em">1280</mn><mo lspace="0.222em" mathsize="1.440em" rspace="0.222em">√ó</mo><mn mathsize="1.440em">704</mn></mrow><annotation encoding="application/x-tex">1280\times 704</annotation></semantics></math><span class="ltx_text" id="A5.I1.i3.p1.2.4" style="font-size:144%;">, and we resize them back to the original resolution for metrics calculation. The original implementation requires per-frame camera pose, intrinsics, and depth map estimated by choice of SLAM packages (VIPE </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A5.I1.i3.p1.2.5.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib2" title="ViPE: video pose engine for 3d geometric perception">24</a><span class="ltx_text" id="A5.I1.i3.p1.2.6.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A5.I1.i3.p1.2.7" style="font-size:144%;"> recommended) for each video; while this is designed for arbitrary videos without 3D information, it prevents us from specifying desired camera poses and intrinsics for fair comparison with the ground truths. Therefore, we instead feed the pipeline ground truth camera poses, intrinsics, and depths maps estimated by DepthAnythingV2 as mentioned in the beginning of section. It is worth noting that VIPE‚Äôs estimated depth cannot be used alone in this case, as its scale is coupled with the estimated pose and intrinsics instead of ground truth ones.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I1.i4.p1">
<p class="ltx_p" id="A5.I1.i4.p1.6"><span class="ltx_text ltx_font_bold" id="A5.I1.i4.p1.6.1" style="font-size:144%;">TrajectoryCrafter¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib10" title="Trajectorycrafter: redirecting camera trajectory for monocular videos via diffusion models">68</a>]</cite>:</span><span class="ltx_text" id="A5.I1.i4.p1.6.2" style="font-size:144%;">
TrajCrafter supports 49-frame clips at </span><math alttext="672\times 384" class="ltx_Math" display="inline" id="A5.I1.i4.p1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="1.440em">672</mn><mo lspace="0.222em" mathsize="1.440em" rspace="0.222em">√ó</mo><mn mathsize="1.440em">384</mn></mrow><annotation encoding="application/x-tex">672\times 384</annotation></semantics></math><span class="ltx_text" id="A5.I1.i4.p1.6.3" style="font-size:144%;">.
The input camera is flexible.
The original implementation relies on a parameterized trajectory representation (</span><math alttext="\theta" class="ltx_Math" display="inline" id="A5.I1.i4.p1.2.m2" intent=":literal"><semantics><mi mathsize="1.440em">Œ∏</mi><annotation encoding="application/x-tex">\theta</annotation></semantics></math><span class="ltx_text" id="A5.I1.i4.p1.6.4" style="font-size:144%;">, </span><math alttext="\phi" class="ltx_Math" display="inline" id="A5.I1.i4.p1.3.m3" intent=":literal"><semantics><mi mathsize="1.440em">œï</mi><annotation encoding="application/x-tex">\phi</annotation></semantics></math><span class="ltx_text" id="A5.I1.i4.p1.6.5" style="font-size:144%;">, </span><math alttext="r" class="ltx_Math" display="inline" id="A5.I1.i4.p1.4.m4" intent=":literal"><semantics><mi mathsize="1.440em">r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math><span class="ltx_text" id="A5.I1.i4.p1.6.6" style="font-size:144%;">, </span><math alttext="x" class="ltx_Math" display="inline" id="A5.I1.i4.p1.5.m5" intent=":literal"><semantics><mi mathsize="1.440em">x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math><span class="ltx_text" id="A5.I1.i4.p1.6.7" style="font-size:144%;">, </span><math alttext="y" class="ltx_Math" display="inline" id="A5.I1.i4.p1.6.m6" intent=":literal"><semantics><mi mathsize="1.440em">y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math><span class="ltx_text" id="A5.I1.i4.p1.6.8" style="font-size:144%;">) for spherical camera motion and computes geometric warping using depth estimated by DepthCrafter¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="A5.I1.i4.p1.6.9.1" style="font-size:144%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib6" title="DepthCrafter: generating consistent long depth sequences for open-world videos">23</a><span class="ltx_text" id="A5.I1.i4.p1.6.10.2" style="font-size:144%;">]</span></cite><span class="ltx_text" id="A5.I1.i4.p1.6.11" style="font-size:144%;">.
While suitable for smooth parametric trajectories, this approach has limited support for arbitrary real-world camera transformations, such as those found in our benchmark.
To address this limitation, we modified the inference implementation to load pre-computed re-projected RGB frames, bypassing the original depth estimation and re-projection steps.
We apply the depth warping interpolation procedure as described above.
Binary masks are automatically computed by thresholding black pixels to identify invalid re-projection regions.
The rest of the implementation is left unchanged.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I1.i5.p1">
<p class="ltx_p" id="A5.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A5.I1.i5.p1.1.1" style="font-size:144%;">CogNVS¬†<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#bib.bib3" title="Reconstruct, inpaint, finetune: dynamic novel-view synthesis from monocular videos">8</a>]</cite>:</span><span class="ltx_text" id="A5.I1.i5.p1.1.2" style="font-size:144%;">
Similarly to TrajectoryCrafter, CogNVS supports 49-frame sequences at a resolution of </span><math alttext="720\times" class="ltx_math_unparsed" display="inline" id="A5.I1.i5.p1.1.m1" intent=":literal"><semantics><mrow><mn mathsize="1.440em">720</mn><mo lspace="0.222em" mathsize="1.440em">√ó</mo></mrow><annotation encoding="application/x-tex">720\times</annotation></semantics></math><span class="ltx_text" id="A5.I1.i5.p1.1.3" style="font-size:144%;">480. We do not perform test-time optimization and instead run the model in a zero-shot manner.
CogNVS can be combined with any depth reconstruction approach, allowing improved view synthesis through better geometric reconstruction.
To ensure consistency with other baselines that rely on off-the-shelf depth estimators, we use monocular depth estimated by DepthAnythingV2.
We apply the depth warping interpolation procedure as described above, matching the required 49-frame length.</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A5.p5">
<p class="ltx_p" id="A5.p5.1"><span class="ltx_text" id="A5.p5.1.1" style="font-size:144%;">We summarize the training sets and some properties of each baseline in¬†</span><a class="ltx_ref" href="https://arxiv.org/html/2601.16982v1#A0.T5" style="font-size:144%;" title="Table 5 ‚Ä£ AnyView: Synthesizing Any Novel View in Dynamic Scenes"><span class="ltx_text ltx_ref_tag">5</span></a><span class="ltx_text" id="A5.p5.1.2" style="font-size:144%;">.
Here,
‚Äú# DOF‚Äù stands for (continuous) degrees of freedom, denoting the dimensionality of the space of trajectories each model was trained with (ignoring intrinsics), and is thus linked to its </span><em class="ltx_emph ltx_font_italic" id="A5.p5.1.3" style="font-size:144%;">effective</em><span class="ltx_text" id="A5.p5.1.4" style="font-size:144%;"> camera pose controllability at inference time.
</span><math alttext="&lt;1" class="ltx_Math" display="inline" id="A5.p5.1.m1" intent=":literal"><semantics><mrow><mi></mi><mo mathsize="1.440em">&lt;</mo><mn mathsize="1.440em">1</mn></mrow><annotation encoding="application/x-tex">&lt;1</annotation></semantics></math><span class="ltx_text" id="A5.p5.1.5" style="font-size:144%;"> means that only a finite list of possible canonical trajectories are supported.
The ‚ÄúInput Cam.‚Äù options mean:</span></p>
<ul class="ltx_itemize" id="A5.I2">
<li class="ltx_item" id="A5.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I2.i1.p1">
<p class="ltx_p" id="A5.I2.i1.p1.1"><span class="ltx_text" id="A5.I2.i1.p1.1.1" style="font-size:144%;">Moving: The method expects the camera trajectory of the input video to move, </span><em class="ltx_emph ltx_font_italic" id="A5.I2.i1.p1.1.2" style="font-size:144%;">e.g</em><span class="ltx_text" id="A5.I2.i1.p1.1.3" style="font-size:144%;">.</span><span class="ltx_text" id="A5.I2.i1.p1.1.4"></span><span class="ltx_text" id="A5.I2.i1.p1.1.5" style="font-size:144%;"> for depth estimation to work well.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I2.i2.p1">
<p class="ltx_p" id="A5.I2.i2.p1.1"><span class="ltx_text" id="A5.I2.i2.p1.1.1" style="font-size:144%;">Flexible: The same model can support either static pose or dynamic pose input videos.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I2.i3.p1">
<p class="ltx_p" id="A5.I2.i3.p1.1"><span class="ltx_text" id="A5.I2.i3.p1.1.1" style="font-size:144%;">Either: Separate models exist for input videos with fixed or moving poses over time.</span></p>
</div>
</li>
</ul>
<p class="ltx_p" id="A5.p5.2"><span class="ltx_text" id="A5.p5.2.1" style="font-size:144%;">The ‚ÄúAlign Start‚Äù options mean:</span></p>
<ul class="ltx_itemize" id="A5.I3">
<li class="ltx_item" id="A5.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I3.i1.p1">
<p class="ltx_p" id="A5.I3.i1.p1.1"><span class="ltx_text" id="A5.I3.i1.p1.1.1" style="font-size:144%;">Yes: The first target camera pose must be spatially very close to the first input camera pose (typically linked to narrow DVS).</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I3.i2.p1">
<p class="ltx_p" id="A5.I3.i2.p1.1"><span class="ltx_text" id="A5.I3.i2.p1.1.1" style="font-size:144%;">Flexible: The same model can support both narrow and extreme DVS.</span></p>
</div>
</li>
<li class="ltx_item" id="A5.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span>
<div class="ltx_para" id="A5.I3.i3.p1">
<p class="ltx_p" id="A5.I3.i3.p1.1"><span class="ltx_text" id="A5.I3.i3.p1.1.1" style="font-size:144%;">Either: Separate models exist for both settings.</span></p>
</div>
</li>
</ul>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Jan 18 21:06:13 2026 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
